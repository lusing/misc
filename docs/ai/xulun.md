# 人工智能学习手册

## 前言

习近平总书记于2018年10月31日在十九届中央政治局第九次集体学习时曾指出，人工智能是新一轮科技革命和产业变革的重要驱动力量，加快发展新一代人工智能是事关我国能否抓住新一轮科技革命和产业变革机遇的战略问题。他还指出，加快发展新一代人工智能是我们赢得全球科技竞争主动权的重要战略抓手，是推动我国科技跨越发展、产业优化升级、生产力整体跃升的重要战略资源。他还强调，人工智能是引领这一轮科技革命和产业变革的战略性技术，具有溢出带动性很强的“头雁”效应。

2023年，习近平总书记在黑龙江考察时提出了新质生产力的概念。而人工智能技术是新质生产力的重要引擎之一。在新的历史时期，人工智能技术将会在各个领域发挥重要作用，推动经济社会的发展。这要求我们要加强人工智能技术的学习，提高人工智能技术的应用能力。

虽然道理大家都懂，但是人工智能技术的学习却不是一件容易的事情。尤其人工智能的发展日新月益，在去年还是热门的LangChain技术，今年就迅速被RAG技术所取代。在人工智能的发展道路上，争论也很多。我们时常也能听到图灵奖获得者或者某著名教授发表对于主流技术的不同看法。这就要求我们在掌握最新的主流技术的同时，对于最经典的机器学习技术的基础有更坚实的基础。如果您想成为算法工程师，在应聘相关岗位的时候，这些基础知识也是会被经常考察的。另外，在进行人工智能技术的研究时，也需要更好地站在巨人的肩膀上，更好地理解前人的研究成果。

我们举个例子，对于统计机器学习产生过巨大影响的支持向量机技术，在1995年被正式提出，1998年被广泛接受，但是其核函数用到的关键定理是1909年就发表的Mercer定理。而支持向量的概念是1963年就提出了，距离被广泛应用还需要35年。

深度学习也是如此，1943年就提出了人工神经网络的原型，1958年开发出了感知机，但是直到2012年，深度学习才被广泛应用。前后历经70年。

而深度学习时代，主流技术几年一变方向。2012是以全连接网络、卷积神经网络、循环神经网络为主。2016年，以围棋打败李世石为标志性事件，深度强化学习成为主流方向。2017年，《Attention is all you need》论文的发表，正式开启了预训练大模型的时代，以Transformer架构将循环神经网络赶出主流。2018年，第一代代表性大模型BERT开创新时代。但是，引领第二次大模型革命的ChatGPT并不是从BERT基础上发展而来的，而是从OpenAI的生成式大模型GPT上发展起来的。机器学习的发展史有点像哲学的发展史，因为不是线性发展的，每一代新增的技术都不能完全替代前一代技术，所以需要对于每一代技术都有一定的了解。

最后就是目前主流的预训练大模型技术还有巨大的技术缺陷，比如大模型的幻觉问题。传统机器学习的可解释性和可靠性，是幻觉问题的有益的补充。未来的人工智能技术，很可能是在老技术的基础上发展起来的。

最后，深度学习和大模型被诟病的一点还有就是理论基础不扎实，被戏称为 “黑盒子”和“炼金术”。而学习传统机器能让读者受到更多的理论训练，对于未来学习是很有帮助的。

由于时间和能力所限，尤其书中引用了大量公式和开发了大量新代码，书中难免有错误，敬请读者批评指正。

## 内容简介

本书是站在深度学习第四次浪潮的浪尖上，对于机器学习的基础知识以及前三次浪潮的核心知识点所撰写的一本机器学习原理与编程指南。

本书的特点可以总结为“二新”和“三化”。

第一新是视角新。由于站在的时间点不同，本书取材的视角与前深度学习时代以及前三次浪潮的视角自然有所不同。预训练大模型的推出，大大简化了图像处理与自然语言处理的工作，所以本书选材时略过了过时技术的应用技术。比如基于循环神经网络的机器翻译，现在已经完全被GPT4o等大模型所取代。为了保证完整性，书中还会介绍原理、实现与库的使用，但是就不讲过时的应用技术来浪费读者的精力。所以本书所处的年代优势是跟前辈们不同的。

第二新是工具新。同时，尽管是针对老技术，我们使用的工具并不保守。对于传统机器学习部分，我们不但介绍了经典的Sklearn, NumPy, XGBoost等库，还用最新技术的PyTorch以及JAX等可以发挥最新硬件性能的库进行基于原理的代码演示开发。随书附赠的代码也将跟着工具的升级而更新，可以以更好的性能应用在实际中。

第一化是"公式代码化"。为了让本书读者更深入理解机器学习原理，书中使用了一定量的数学公式。但是，重要的数学公式我们都配有代码进行讲解。不管数学概念本身多么抽象，翻译成代码后就很容易祛魅。建议读者在阅读数学公式时，可以先看看代码，再回过头来看数学公式。当然，更好的阅读方法是先试着根据公式自己写代码，然后跟书中的代码进行对比。

第二化是"算法可视化"。本书充分利用了成熟库中的可视化工具，并结合其它绘图工具，对机器学习的结果进行了可视化展示。希望读者在学习中也多画一些图来帮助理解。

第三化是"算法实战化"。本书并不仅仅关注机器学习原理和库的调用方法，并且关注读者将来工作中非常重要的调超参数的问题。凡有重要影响性能的超参数，本书中都会花一些笔墨进行讲解。

在编排上，为了突出机器学习主要处理分类和回归的本质，本书还是将算法拆分成分类预测和数值预测两部分。即使割裂算法的完整性也在所不惜。

## 第一章 什么是机器学习

### 1.1 机器学习的定义

关于机器学习的定义，按惯例我们要致敬前辈。

著名的机器学习专家 Tom M. Mitchell 在他的书《机器学习》中给出了一个广泛引用的定义：A computer program is said to learn from experience 𝐸 with respect to some class of tasks 𝑇 and performance measure 𝑃 , if its performance at tasks in 𝑇, as measured by 𝑃, improves with experience 𝐸.

翻译成中文，就是：“如果一个计算机程序在处理某类任务𝑇时，通过经验𝐸的积累，其性能𝑃得到了提升，那么我们说该程序实现了学习。”

这个定义出现在1997年出版的《机器学习》一书上。那个时候不但没有深度学习，就连支持向量机都还没有被广泛接受。所以《机器学习》一书里都没有支持向量机的内容，这在后面的书里基本是不可想象的。但是，这个经典定义却一直被人们引用。

|概念|解释|
|---|---|
|任务T|机器学习程序所要完成的具体任务。例如，电子邮件分类、图像识别、语音识别等。|
|经验E|程序从数据中获得的经验。例如，用于训练模型的数据集。|
|性能度量P|用于评估程序在特定任务上的表现的指标。例如，分类准确率、均方误差等。|

假设我们要训练一个垃圾邮件分类器，那么，T、E、P分别是什么呢？读者不妨停下来思考一下再跟书里的作下对比。

|项目|内容|
|---|---|
|任务T|将电子邮件分类为“垃圾邮件”或“非垃圾邮件”。|
|经验E|大量已标记的电子邮件数据集，包含垃圾邮件和非垃圾邮件的示例。|
|性能度量P|分类准确率，即模型正确分类的电子邮件的比例。|

下面我们再介绍几个常用的机器学习术语：

|概念|解释|
|---|---|
|数据驱动|机器学习依赖大量的数据，通过数据分析和模式识别，模型能够从中提取有价值的信息。|
|模型|一种数学表示，能够捕捉数据中的规律和模式，以便对新数据进行预测或决策。|
|训练|通过调整模型参数，使其在训练数据上表现良好。|
|泛化|模型不仅在训练数据上表现良好，还能在未见过的数据上保持良好的性能。|

定义介绍完，我们通俗介绍一下机器学习。机器学习（Machine Learning）是一种人工智能（Artificial Intelligence）技术，它使计算机能够在没有明确编程指令的情况下通过数据进行学习和改进。机器学习算法通过从数据中提取模式和规律，从而做出预测或决策。

机器学习主要分为以下三种类型：

| 类型  | 描述  | 主要应用  |
|------|------|------|
| 监督学习（Supervised Learning） | 通过学习已标注的数据来进行预测或分类 | 分类和回归 |
| 非监督学习（Unsupervised Learning） | 在没有标签的数据中发现隐藏的模式和结构 | 聚类和降维 |
| 强化学习（Reinforcement Learning） | 通过与环境的交互来学习策略，以最大化累积奖励 | 围棋战胜李世石 |

监督学习是一种在有监督（即有标签）的数据集上训练模型的机器学习方法。数据集中的每个样本都有一个输入和一个已知的正确输出（标签），模型通过学习输入与输出之间的映射关系来进行预测。

非监督学习通过分析和建模未标注的数据，来识别数据的潜在结构和模式。其主要目标是从数据中提取有用的信息，而无需事先知道数据的标签。

在监督学习（Supervised Learning）中，标签（Label）是指与输入数据对应的目标输出或结果。标签用于指导模型的学习过程，使模型能够准确地预测新的未见数据。

标签分为两种：预测值型和分类型。预测值型标签是连续的数值，用于回归问题；分类型标签是离散的类别，用于分类问题。

假设我们要构建一个模型来预测房屋的价格。我们有一个数据集，其中每条记录包含房屋的特征以及对应的房价。这些特征可能包括房屋面积、卧室数量、浴室数量、所在地区等。房价作为目标输出，是我们要预测的值。

| 房屋面积 (平方英尺) | 卧室数量 | 浴室数量 | 所在地区 | 房价（标签，美元） |
|------|------|------|------|------|
| 1500 | 3 | 2 | A | 300,000 |
| 2000 | 4 | 3 | B | 450,000 |
| 1700 | 3 | 2 | A | 350,000 |
| 2200 | 4 | 3 | C | 500,000 |
| 1300 | 2 | 1 | A | 250,000 |

这个房价信息就是所谓的标签。

假设我们要构建一个模型来对鸢尾花的种类进行分类。鸢尾花数据集（Iris Dataset）是机器学习中一个经典的数据集，包含三种不同类型的鸢尾花：Setosa、Versicolor 和 Virginica。每条记录包含四个特征：花萼长度、花萼宽度、花瓣长度和花瓣宽度。标签是花的种类。

| 花萼长度 (cm) | 花萼宽度 (cm) | 花瓣长度 (cm) | 花瓣宽度 (cm) | 标签（花的种类） |
|------|------|------|------|------|
| 5.1 | 3.5 | 1.4 | 0.2 | Setosa |
| 7.0 | 3.2 | 4.7 | 1.4 | Versicolor |
| 6.3 | 3.3 | 6.0 | 2.5 | Virginica |
| 4.9 | 3.0 | 1.4 | 0.2 | Setosa |
| 6.5 | 2.8 | 4.6 | 1.5 | Versicolor |
| 5.7 | 2.8 | 4.1 | 1.3 | Versicolor |
| 6.7 | 3.1 | 5.6 | 2.4 | Virginica |

强化学习（Reinforcement Learning，RL）通过智能体（Agent）在环境（Environment）中的探索和交互来学习策略，以最大化累积奖励。强化学习不同于监督学习和非监督学习，它不依赖于大量标注数据，而是通过试错法和反馈机制来优化行为。

### 1.2 “没有白吃的午餐”定律

“没有白吃的午餐”定律（No Free Lunch Theorem）是机器学习领域的一个重要定理，由 David Wolpert 和 William Macready 于1997年提出。这个定理的核心思想是：在所有问题上，所有算法的性能是相同的。

这个定理的意义在于，没有一个算法能够在所有问题上都表现最好。也就是说，没有一个算法是万能的。在实际应用中，我们需要根据具体问题的特点，选择合适的算法。

我们来简要证明一下这个定律。我们先定义几个概念：

1. 搜索空间：假设有一个有限的搜索空间$X$，其中包含所有可能的输入。
2. 目标函数集合：目标函数$f: X \rightarrow Y$从搜索空间$X$映射到输出空间$Y$。所有可能的目标函数集合为$\mathcal{F}$。
3. 算法集合：算法$A$是一种策略，它根据已观察到的输入输出对来选择下一个输入。所有可能的算法集合为$\mathcal{A}$。
4. 性能度量：性能度量$P(A, f)$衡量算法$A$在目标函数$f$上的表现。

对于一个给定的算法$A \in \mathcal{A}$和目标函数$f \in \mathcal{F}$，算法$A$在$f$上的性能度量为$P(A, f)$。我们要计算算法$A$在所有目标函数上的平均性能：$\bar{P}(A) = \frac{1}{|\mathcal{F}|} \sum_{f \in \mathcal{F}} P(A, f)$

我们需要展示对于任意两个算法$A, B \in \mathcal{A}$，它们的平均性能是相等的。也就是说：$\bar{P}(A) = \bar{P}(B)$

假设算法$A$和$B$在每个目标函数$f$上的表现是独立的，我们可以交换求和的次序：
$$ \sum_{f \in \mathcal{F}} P(A, f) = \sum_{x \in X} \sum_{y \in Y} P(A, f(x)=y) $$

由于所有目标函数$f$是从$\mathcal{F}$中均匀采样得到的，因此每个输入$x \in X$和输出$y \in Y$的组合在所有目标函数上出现的频率是相等的。

由于每个输入输出组合在所有目标函数上出现的频率是相等的，我们可以得到：
$$ \sum_{f \in \mathcal{F}} P(A, f) = \sum_{f \in \mathcal{F}} P(B, f) $$

因此，对于任意两个算法$A$和$B$，它们在所有目标函数上的总性能是相等的。除以目标函数的数量$|\mathcal{F}|$后，我们得到它们的平均性能也相等：
$\bar{P}(A) = \bar{P}(B)$

通过上述步骤，我们证明了在所有可能的目标函数上，任何两个算法的平均性能是相等的。这就是“没有白吃的午餐”定律的核心思想：没有一种算法能够在所有可能的问题上都表现得比其他算法更好。

因为这个定律，我们并没有针对所有问题的最优算法，我们需要根据具体问题的特点，选择合适的算法。所以本书中的算法大家都要认真学习，因为每个算法都有其适用的场景。

### 1.3 人工智能简史

人工智能的发展史是一部曲折的进步史，而且，我们现在仍然处于这个曲折的进步史中。

人工智能的最大特点就是难，挑战性特别大。所以，在人工智能的历史中，多次出现高潮与低潮。整个人工智能的进步，也是在螺旋上升的过程中，不断有新的方向诞生推动技术进步。而在这个过程中，有很多方向遇到了瓶颈，至今发展缓慢。

机器可以实现人的智能，自古以来就是人类的梦想。古希腊神话中，赫淮斯托斯制造了一个黄铜巨人塔罗斯，守护克里特岛。在古代中国，周朝就有偃师机器人的传说。

《列子.汤问》中记载，在西周时期,有一位名叫偃师的工匠向当时的周穆王献上了一个自制的人偶机器人。这个人偶外表栩栩如生,能够自如地站立行走、唱歌跳舞。周穆王见了大为吃惊，叫来自己的宠妃一起观看。就在周穆王和宠妃对着人偶细细端详、品头论足时，偃师一声令下，人偶竟开口唱起了歌来，歌声婉转悠扬，十分动人。唱到高潮处，人偶又舒展四肢跳起了舞。如此精彩的表演，逗得周穆王哈哈大笑。周穆王看得有趣过瘾，还让宠姬一起出来观看。

表演将毕，那偶人却向周穆王的宠姬抛了抛媚眼，让周穆王勃然大怒，一心认定这个灵活宛似活人的家伙本就是个不折不扣的真人，便要将偃师当场处决。偃师却将机器人立刻折开，发现它只是由皮革、木头、胶漆、黑白红蓝颜料组成的机械物。

周穆王趋前细看，机器人的内部器官俱全，外边则是筋骨、关节、皮毛、牙齿、头发一应俱全，但却都是机械物，一经组合，却又是一个活生生的机器人，将机器人的心拆走，机器人便无法说话，拆走肝则眼目皆盲，将它的肾拆走，就无法走路。最后，才让周穆王心悦诚服，大叹偃师技法的高超。

#### 1.3.1 第一次人工智能浪潮与寒冬

在现代，人工智能的概念最早由英国数学家阿兰·图灵提出。1950年，图灵在《计算机器与智能》一文中首次提出了“机器能思考吗？”这个问题，开创了人工智能的研究领域。在他的论文《计算机器与智能》（Computing Machinery and Intelligence）中，图灵提出了一个名为“模仿游戏”（Imitation Game）的实验，这个实验后来被称为图灵测试。

图灵测试的核心思想是评估一个机器是否能够表现出与人类相当的智能。具体来说，测试分为三方：测试者（通常是人类）、被测试的机器，以及另一个参与者（通常也是人类）。测试者通过文本界面与机器和另一名人类进行交流。所有交流都是通过键盘和屏幕进行的，测试者无法看到或听到对方。测试者的任务是通过对话来判断哪个是人类，哪个是机器。机器的任务是尽可能模仿人类的回答，而人类则正常回答问题。如果测试者在一定时间内无法可靠地区分机器和人类，那么机器就被认为通过了图灵测试。

1956年，达特茅斯会议（Dartmouth Conference）被视为人工智能的诞生标志，会议上首次提出了“人工智能”这个术语。达特茅斯会议由约翰·麦卡锡（John McCarthy）、马文·明斯基（Marvin Minsky）、纳撒尼尔·罗切斯特（Nathaniel Rochester）和克劳德·香农（Claude Shannon）联合发起。这些科学家希望通过这次会议来探讨如何使机器表现出智能行为，并提出了“人工智能”这一术语。

达特茅斯会议会，符号主义人工智能（Symbolic AI）成为人工智能的主流方向。他们主要关注使用符号和规则来表示知识和进行推理。符号学派的核心思想是使用符号（如单词、公式、图形）来表示知识，并通过逻辑规则和算法来处理这些符号，以实现智能行为。

知识和信息以符号的形式存储和表示，这些符号可以是逻辑公式、概念或其他抽象表示。

使用逻辑规则和算法对符号进行操作和推理。例如，使用一阶逻辑（First-Order Logic）来进行定理证明和问题求解。

知识以结构化的形式存储在知识库中，这些知识可以包括事实、规则和关系。

通过推理引擎应用规则来处理符号，从而得出结论或执行任务。

符号主义的主要应用领域包括专家系统、知识图谱、自动定理证明等。这一分支至今仍然在人工智能领域有着重要的地位。

符号主义有非常好的可解释性。由于符号和规则是显式的，符号学派的系统通常更容易解释和理解。在特定领域内，基于规则的系统可以提供高精度和可靠的结果。可以通过知识工程的方法来构建和维护知识库，使系统能够处理复杂的专业知识。

然而，符号主义也存在一些局限性。构建和维护知识库需要大量的人工干预，知识获取过程复杂且耗时。符号学派系统在处理未预见到的情况和模糊、不确定的信息时表现较差。符号推理过程可能非常复杂，尤其是在处理大量符号和规则时。

符号主义遇到的问题，在今天仍然是人工智能领域的重要挑战。比如大模型的幻觉问题，本质上的困难跟符号主义一脉相承。

在符号主义大行其事的同时，连接主义（Connectionism）也在发展。连接主义是一种基于神经网络的人工智能方法，它模拟人类大脑的神经元网络结构，通过神经元之间的连接来实现学习和推理。

早在二战时的1943年，沃伦·麦卡洛克（Warren McCulloch）和沃尔特·皮茨（Walter Pitts）发表了关于神经网络的奠基性论文《A Logical Calculus of Ideas Immanent in Nervous Activity》，提出了神经元的数学模型，即麦卡洛克-皮茨神经元模型。

1958年，弗兰克·罗森布拉特（Frank Rosenblatt）提出了感知器（Perceptron），这是一种早期的神经网络模型，能够进行简单的模式分类任务。罗森布拉特的工作激发了对神经网络的广泛兴趣。

然而，研究者和媒体对AI的潜力产生了过高的期望，认为AI能够在短时间内实现类似人类的智能。实际上，AI在处理复杂和实际问题时遇到了许多困难，许多预期的目标未能实现。这种预期与现实之间的巨大差距导致了失望和对AI的怀疑。

当时的计算机硬件和存储能力相对有限，无法支持大规模数据处理和复杂计算，这限制了AI算法的应用和性能。

1969年，达特茅斯会议主要人物之一的马文·明斯基（Marvin Minsky）和西摩·帕普特（Seymour Papert）在《感知器》一书中指出了单层感知器的局限性，特别是它无法解决异或（XOR）问题。这一发现导致了对神经网络的兴趣急剧下降，进入了所谓的“第一次AI寒冬”。

随着AI研究的进展受阻，政府和机构对AI项目的资金支持逐渐减少。例如，美国国防高级研究计划局（DARPA）在20世纪70年代初减少了对AI研究的资助。许多AI研究项目被迫缩减或终止，研究者们转向其他更为现实和应用导向的领域。

#### 1.3.2 第二次人工智能浪潮与寒冬

第一次人工智能寒冬促使研究人员反思和寻找新方向。符号主义的困难解决不掉，研究人员开辟了一个新的领域，统计学习（Statistical Learning）。统计学习是一种基于数据和概率统计的机器学习方法，通过分析数据中的模式和规律，来构建模型并进行预测。

换种说话，本书的主要内容，机器学习正式登场了。

在这个时期，决策树、支持向量机等机器学习算法开始受到关注。这些算法不再依赖于人工编写的规则和知识库，而是通过学习数据中的模式和规律，来进行预测和决策。

与其同时，连接主义也取得了重要突破。1986年，David Rumelhart、Geoffrey Hinton和Ronald Williams重新发现并推广了反向传播算法。这使得多层神经网络（Multilayer Neural Networks）成为可能，成功地解决了单层感知器无法解决的异或问题。

这一时期，符号主义思想的专家系统发展很迅速。

专家系统（Expert System）旨在模拟人类专家在特定领域内的决策能力和知识。它通过利用知识库和推理引擎来解决复杂问题，提供诊断、建议或决策支持。专家系统在20世纪70年代和80年代得到了广泛的发展和应用。

但是，专家系统也避免不了符号系统的困境。专家系统依赖于大量的规则和知识库，这些规则需要人工编码和维护。当规则数量增加时，系统变得难以管理和扩展。此外，专家系统在处理不确定性和模糊信息时表现不佳，导致其在实际应用中的效果有限。

许多公司和政府机构在早期阶段对AI技术投入了大量资金，但由于技术未能如预期般带来革命性的变化，资金支持逐渐减少。投资者和资助机构对AI的信心下降，导致研究经费的大幅削减。许多AI研究项目被取消，研究人员失去了资金支持。许多学术机构和公司缩减了AI相关的研究和开发部门。

其中最有代表性的就是日本的第五代计算机计划。第五代计算机的目标包括:问题求解和推理、知识库管理、智能接口以及智能程序设计。它们旨在实现计算机的人工智能化,让计算机能够自主学习、理解自然语言、图像等。为此,日本选择了Prolog作为编程语言,希望建立一个超大的知识库,并使用Prolog进行高速的逻辑推理。

然而,这个计划最终以失败告终。它的目标过于超前,超越了当时的技术水平,无法实现自然语言理解、程序自动生成等目标。该计划花费了4亿美元,但在1991年被迫中止。

#### 1.3.3 深度学习革命

2006年，时隔二十年之后，BP算法三位作者之一的杰弗里·辛顿（Geoffrey Hinton）提出了深度信念网络（Deep Belief Network, DBN），这是深度学习的早期模型之一。他们展示了如何使用无监督预训练来初始化深层神经网络，从而克服传统方法中的训练困难。

2012年，辛顿的学生Alex Krizhevsky和Ilya Sutskever用AlexNet在ImageNet图像识别竞赛中取得了显著突破。这一事件标志着深度学习进入主流。辛顿教授在神经网络坐了二十几年的冷板凳，终于开创了新的时代。AlexNet所使用的卷积网络，是1998年Yann LeCun提出的LeNet的进化版，但是由于当时的硬件和数据集的限制，LeNet并没有引起太多的关注。等了十几年后终于找到了用武之地。

然而，深度学习的发展也并不是线性的。就当人们以为深度全连接网络、卷积神经网络和循环神经网络这深度学习三件套是主流的时候。2016年，DeepMind的AlphaGo击败人类围棋冠军李世石，标志着深度强化学习成为人工智能的新热点。

同时，研究者发现，循环神经网络需要结合注意力机制才有更好的效果。

2017年：Vaswani等人提出了Transformer模型，只用注意力机制就可以大大提升自然语言处理的效果。

2018年，以Transformer为基本元件的BERT模型在自然语言处理领域取得了巨大成功。标志着预训练大模型时代的开始。然而，引发新一代大模型的ChatGPT模型并不是以BERT为基础的，而是同年OpenAI发布的GPT模型。

2022年，ChatGPT的横空出世引领了大模型的新时代。

不过，大模型仍然有自己的问题。比如对于算力的需求，比如幻觉问题。

大模型成功地解决了之前专家系统和知识图谱的很多问题，但是数据质量的问题一直没有解决。因为数据质量的问题本质上还是符号主义的问题。

而且随着机器学习的发展，新方法越来越黑盒化。这导致了可解释性成为一个新的问题。对于高安全可靠的场景，可解释性是一个非常重要的问题。

### 1.4 小结

人工智能是一门非常受欢迎的学科。在过去几十年中一直在过度乐观和过度悲观中前进。

这里特别要强调，一定要多习，尤其是数学和统计学。因为人类的直觉是缺乏统计学观念的。两次机器学习寒冬都跟人类喜欢用更符合人类直觉的符号主义和专家系统有关。

即使是在现在的各公司领导者中，喜欢用直觉制定人工智能研究计划的人仍然很多。他们对于符号主义的难度一无所知。

跟随目前最新的机器学习技术是最重要的选择。但是，我们也要加强对于传统机器学习技术的学习，在大模型等最新技术不适用的情况下，统计机器学习仍然是最好的选择。

从历史上看，无论是自然语言处理还是图像和语音，基于规则的人工智能都没有任何突破，直到统计方法的引入才进入实用阶段，而深度学习的引入才让这些领域有了质的飞跃。目前这些技术的最新应用又基本上都被大模型技术取代。

在符号主义遇到困难难以突破，而黑盒模型又难以解释的情况下，基于统计的机器学习是最好的学习材料。

本书的宗旨就是介绍机器学习和深度学习的基础应用的同时，让大家能够理解这些技术的原理，以便在实际应用中能够更好地选择和使用这些技术。

## 第二章 分析数据

在机器学习成为一门广泛开设的课程之前，承担介绍机器学习的方法的课程早已有之，这门课程就是数据挖掘。数据挖掘是一门研究如何从大量数据中提取有用信息的学科。数据挖掘的目标是发现数据中的模式、规律和知识，以帮助人们做出更好的决策。

下面我们就了解一些著名的数据集，请大家思考下如果由您来挖掘的话，可以从数据中发现什么规律。

### 2.1 一些常用的数据集

首先，我们来看一些常用的数据集。数据集是机器学习的基础，是机器学习算法的输入。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/datasets.png)

#### 2.1.1 鸢尾花数据集

鸢尾花数据集是机器学习领域中最著名的数据集之一，由英国统计学家和生物学家罗纳德·艾尔默·费雪在1936年收集整理。数据集包含了150个样本，分为3类，每类50个样本，每个样本包含4个特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度。数据集的目标是根据这4个特征对鸢尾花进行分类。

这三种鸢尾花分别是：
- Iris-Setosa: 山鸢尾
- Iris-Versicolour: 变色鸢尾
- Iris-Virginica: 维吉尼亚鸢尾

作为本书中第一个数据集，我们来看一下鸢尾花数据集是个什么样子。我们通过机器学习库sklearn来加载鸢尾花数据集。

```python
from sklearn.datasets import load_iris

iris = load_iris()

print(iris)
```

我们可以看到，鸢尾花数据集主要包含数据data和标签target两部分。数据data是一个二维数组，每一行代表一个样本，每一列代表一个特征。标签target是一个一维数组，每个元素代表一个样本的类别。

我们以数据的前几行为例，展示一下鸢尾花数据集的数据部分：

```
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 ...
```

前面我们说过，这4列分别代表花萼长度、花萼宽度、花瓣长度、花瓣宽度。
以第一行数据为例，花萼长度为5.1cm，花萼宽度为3.5cm，花瓣长度为1.4cm，花瓣宽度为0.2cm。
同理，第二行数据的花萼长度为4.9cm，花萼宽度为3.0cm，花瓣长度为1.4cm，花瓣宽度为0.2cm。

然后我们再看对应的target的标签，这表示这朵花是哪一种鸢尾花。

```
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
```

我们可以看到，前50个样本的标签是0，代表山鸢尾；接下来的50个样本的标签是1，代表变色鸢尾；最后的50个样本的标签是2，代表维吉尼亚鸢尾。

所以，我们前面看到的花萼长度为5.1cm，花萼宽度为3.5cm，花瓣长度为1.4cm，花瓣宽度为0.2cm的鸢尾花是山鸢尾。

#### 2.1.2 糖尿病数据集

糖尿病数据集是另一个常用的数据集，用于糖尿病患者的预测。数据集包含了442个患者的10个生理特征（如年龄、性别、体重、血压等）和1个响应变量（疾病进展的定量指标）。数据集的目标是根据这些生理特征预测疾病的进展。

我们将其转为DataFrame格式，方便查看：

```python
from sklearn.datasets import load_diabetes
import pandas as pd

# 加载数据集
data = load_diabetes()
X = data.data
feature_names = data.feature_names

# 创建 DataFrame
df = pd.DataFrame(X, columns=feature_names)

# 显示前几行数据
print("\nFirst few rows of the dataset:")
print(df.head())
```

输出的结果如下：

```
First few rows of the dataset:
        age       sex       bmi        bp        s1        s2        s3  \
0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   
1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   
2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   
3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   
4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   

         s4        s5        s6  
0 -0.002592  0.019907 -0.017646  
1 -0.039493 -0.068332 -0.092204  
2 -0.002592  0.002861 -0.025930  
3  0.034309  0.022688 -0.009362  
4 -0.002592 -0.031988 -0.046641  
```

其具体含义为：
- age: 年龄
- sex: 性别
- bmi(body mass index): 身体质量指数，是衡量是否肥胖和标准体重的重要指标，理想BMI(18.5~23.9) = 体重(单位Kg) ÷ 身高的平方 (单位m)
- bp(blood pressure): 血压
- s1,s2,s3,s4,s4,s6: 六种血清的化验数据，是血液中各种疾病级数指针的6的属性值。
    - s1——tc，总血清胆固醇
    - s2——ldl，低密度脂蛋白
    - s3——hdl，高密度脂蛋白
    - s4——tch，总胆固醇/高密度脂蛋白
    - s5——ltg，可能是血清甘油三酯水平的对数
    - s6——glu，血糖水平

看了字段含义，大家可能还是看不懂。比如性别。我们来看看性别字段的分布：

```python
# 查看性别字段的分布
print("Sex field distribution:")
print(df['sex'].value_counts())
```

结果如下：

```
Sex field distribution:
sex
-0.044642    235
 0.050680    207
```

我们可以看到，性别字段的取值为-0.044642和0.050680，分别对应235和207个样本。这里的取值是经过预处理的。
我们可以发现-0.044642 * 235 +  0.050680 * 207 约等于0，说明是进行了标准化处理。

其实原始数据是这样的：
```
AGE	SEX	BMI	BP	S1	S2	S3	S4	S5	S6	Y
59	2	32.1	101	157	93.2	38	4	4.8598	87	151
48	1	21.6	87	183	103.2	70	3	3.8918	69	75
72	2	30.5	93	156	93.6	41	4	4.6728	85	141
24	1	25.3	84	198	131.4	40	5	4.8903	89	206
50	1	23	101	192	125.4	52	4	4.2905	80	135
```

#### 2.1.3 威斯康星乳腺癌数据集

威斯康星乳腺癌(诊断)数据集是一个著名的机器学习数据集,用于乳腺癌诊断分类任务。基于您提供的信息,我可以总结一些关键点:

数据集特征:

- 多变量数据集
- 569个样本
- 30个特征
- 二分类任务(良性或恶性)

这个数据集特征是从乳房肿块的细针穿刺抽吸(FNA)的数字化图像中计算得出的。它们描述了图像中存在的细胞核的特征,包括:

- 半径（到周边点的距离的平均值）
- 纹理（灰度值的标准差）
- 周长
- 面积
- 光滑度（半径长度的局部变化）
- 紧凑度（$\frac{周长^2}{面积} - 1.0$）
- 凹度（轮廓的凹陷部分的严重程度）
- 凹点（轮廓的凹陷部分的数量）
- 对称性
- 分形维度（“海岸线近似”- 1）

这些特征的均值、标准误差和“最差”或最大值（三个最差/最大值的均值）是针对每个图像计算的，共计30个特征。例如，字段0是平均半径，字段10是半径的标准误差，字段20是最差半径。

我们来看下数据加载的代码：

```python
# 加载乳腺癌数据集
breast_cancer = datasets.load_breast_cancer()

# 选择特征和目标变量
X = breast_cancer.data[0]
y = breast_cancer.target

print(X)
print(y)
```

打印出来的结果如下：

```
[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01
 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02
 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01
 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01
 4.601e-01 1.189e-01]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0
 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1
 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1
 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0
 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1
 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1
 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1
 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0
 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1
 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1
 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1
 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0
 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 0 0 0 0 0 0 1]
```

#### 2.1.4 自己生成数据集

除了加载现成的数据集，我们还可以自己生成数据集。在机器学习中，我们经常需要生成一些模拟数据来测试算法的性能。比如，我们可以生成一些符合正态分布的数据，然后用机器学习算法来拟合这些数据。

```python
import matplotlib.pyplot as plt

from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles

plt.figure(figsize=(8, 8))
plt.subplots_adjust(bottom=0.05, top=0.9, left=0.05, right=0.95)

plt.subplot(321)
plt.title("One informative feature, one cluster per class", fontsize="small")
X1, Y1 = make_classification(
    n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1
)
plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

plt.subplot(322)
plt.title("Two informative features, one cluster per class", fontsize="small")
X1, Y1 = make_classification(
    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1
)
plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

plt.subplot(323)
plt.title("Two informative features, two clusters per class", fontsize="small")
X2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)
plt.scatter(X2[:, 0], X2[:, 1], marker="o", c=Y2, s=25, edgecolor="k")

plt.subplot(324)
plt.title("Multi-class, two informative features, one cluster", fontsize="small")
X1, Y1 = make_classification(
    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3
)
plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

plt.subplot(325)
plt.title("Three blobs", fontsize="small")
X1, Y1 = make_blobs(n_features=2, centers=3)
plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

plt.subplot(326)
plt.title("Gaussian divided into three quantiles", fontsize="small")
X1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)
plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/generated_data.png)

我们再看一个生成多标签的例子：

```python
import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import make_multilabel_classification as make_ml_clf

COLORS = np.array(
    [
        "!",
        "#FF3333",  # red
        "#0198E1",  # blue
        "#BF5FFF",  # purple
        "#FCD116",  # yellow
        "#FF7216",  # orange
        "#4DBD33",  # green
        "#87421F",  # brown
    ]
)

# Use same random seed for multiple calls to make_multilabel_classification to
# ensure same distributions
RANDOM_SEED = np.random.randint(2**10)


def plot_2d(ax, n_labels=1, n_classes=3, length=50):
    X, Y, p_c, p_w_c = make_ml_clf(
        n_samples=150,
        n_features=2,
        n_classes=n_classes,
        n_labels=n_labels,
        length=length,
        allow_unlabeled=False,
        return_distributions=True,
        random_state=RANDOM_SEED,
    )

    ax.scatter(
        X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]).sum(axis=1)), marker="."
    )
    ax.scatter(
        p_w_c[0] * length,
        p_w_c[1] * length,
        marker="*",
        linewidth=0.5,
        edgecolor="black",
        s=20 + 1500 * p_c**2,
        color=COLORS.take([1, 2, 4]),
    )
    ax.set_xlabel("Feature 0 count")
    return p_c, p_w_c


_, (ax1, ax2) = plt.subplots(1, 2, sharex="row", sharey="row", figsize=(8, 4))
plt.subplots_adjust(bottom=0.15)

p_c, p_w_c = plot_2d(ax1, n_labels=1)
ax1.set_title("n_labels=1, length=50")
ax1.set_ylabel("Feature 1 count")

plot_2d(ax2, n_labels=3)
ax2.set_title("n_labels=3, length=50")
ax2.set_xlim(left=0, auto=True)
ax2.set_ylim(bottom=0, auto=True)

plt.show()

print("The data was generated from (random_state=%d):" % RANDOM_SEED)
print("Class", "P(C)", "P(w0|C)", "P(w1|C)", sep="\t")
for k, p, p_w in zip(["red", "blue", "yellow"], p_c, p_w_c.T):
    print("%s\t%0.2f\t%0.2f\t%0.2f" % (k, p, p_w[0], p_w[1]))
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/n_labels.png)

### 2.2 训练集和测试集

在机器学习中，我们通常将数据集分为训练集和测试集。训练集用于训练模型，测试集用于评估模型的性能。

常用的划分方法有：

- 留出法（Holdout Method）‌：这种方法将数据集分为两个互斥的集合，一个用作训练集，另一个用作测试集。通常，训练集占大部分（例如70%-80%），而测试集占小部分（例如20%-30%）。
- 交叉验证（Cross-Validation）‌：这是一种更复杂的方法，它将数据集分成k个子集，每次使用k-1个子集作为训练数据，剩下的一个子集用作测试数据。这个过程重复k次，每个子集都有一次机会作为测试集。最后，模型的性能是所有k次测试结果的平均值。
- 自助法（Bootstrap）‌：在这种方法中，从原始数据集中随机抽样生成训练集，未被抽中的样本构成测试集。由于抽样是有放回的，所以某些样本可能在训练集中出现多次，而有些则可能一次都不出现。

我们可以通过sklearn库中的train_test_split函数来将数据集分为训练集和测试集。

```python
from sklearn.model_selection import train_test_split

# 示例数据
X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]
y = [0, 1, 0, 1, 0, 1]

# 将数据划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("训练集特征：", X_train)
print("测试集特征：", X_test)
print("训练集标签：", y_train)
print("测试集标签：", y_test)
```

参数解释

- X 和 y：分别是特征矩阵和标签向量。
- test_size：测试集的比例。比如 test_size=0.2 表示 20% 的数据将用作测试集，80% 的数据将用作训练集。你也可以传入一个整数，表示测试集的样本数量。
- random_state：随机种子，确保每次运行划分结果一致。设置相同的 random_state 值可以重现相同的划分结果。


我们再来个例子，拆分鸢尾花数据集：

```python
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn import svm

X, y = datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4)
```

### 2.3 模型评估的基本方法

前面我们介绍了，监督学习分为数值预测和分类预测两大类。针对这两种预测，我们有不同的评估方法。

#### 2.3.1 数值预测的评估方法

数值预测的评估方法主要有均方误差、平均绝对误差、R方等。

均方误差 (Mean Squared Error, MSE)：预测值与实际值之差的平方的平均值，用于衡量预测的准确性。

均方根误差 (Root Mean Squared Error, RMSE)：MSE的平方根，以与原数据相同的单位衡量误差。

平均绝对误差 (Mean Absolute Error, MAE)：预测值与实际值之差的绝对值的平均值，直观地表示了预测的平均误差量。

R方 (R-squared, R2)：用于衡量模型对数据的拟合程度，取值范围为[0,1]，越接近1表示拟合程度越好。

#### 2.3.2 分类预测的评估方法

分类预测的评估方法主要有准确率、精确率、召回率、F1值等。

首先我们讨论一下最简单的情况，二分类问题。在二分类问题中，通常将预测结果分为正例和负例两类。在这种情况下，我们可以定义四种情况：

- 真正例（True Positive, TP）：实际为正例的样本被预测为正例。
- 真负例（True Negative, TN）：实际为负例的样本被预测为负例。
- 假正例（False Positive, FP）：实际为负例的样本被预测为正例。
- 假负例（False Negative, FN）：实际为正例的样本被预测为负例。

假正例也假阳性，占比指标称为误报率（False Positive Rate, FPR），假负例也称为假阴性，占比指标称为漏报率（False Negative Rate, FNR）。

举个例子来说明，假设有一个二分类模型用于预测某种疾病的患病情况，正例表示患病，负例表示健康。在这种情况下：

假阴性：模型将一个实际上患病的人错误地预测为健康，即将病人错判为健康。
假阳性：模型将一个实际上健康的人错误地预测为患病，即将健康人错判为患病。
假阴性和假阳性是模型预测错误的两种情况，它们的出现可能会对模型的性能和可靠性产生影响。在不同的应用场景中，对于假阴性和假阳性的重视程度可能有所不同，需要根据具体情况进行权衡和调整。

下面我们就可以定义三个重要的评估指标：

- 准确率 (Accuracy)：分类正确的样本数占总样本数的比例，用于衡量分类器的整体性能。
- 精确率 (Precision)：精确率是指在所有被模型预测为正例的样本中，实际为正例的比例。它衡量了模型预测为正例的准确性。精确率的计算公式为：精确率 = 真正例数TP / (真正例数TP + 假正例数FP)。想要提升精确率，就要减少误报的数量。
- 召回率 (Recall)：召回率是指在所有实际为正例的样本中，被模型正确预测为正例的比例。它衡量了模型对正例的识别能力。召回率的计算公式为：召回率 = 真正例数 / (真正例数 + 假阴性数)。想要提升召回率，就要减少漏报的数量。

在这里，请大家合上书思考一下，精确率和召回率的关系是什么？

在实际应用中，精确率和召回率往往是相互矛盾的，提高精确率可能会降低召回率，提高召回率可能会降低精确率。也就是说，关注误报就可能会漏报，关注漏报就可能会误报。为了综合考虑精确率和召回率，我们引入了F1值。

- F1值 (F1-score)：它结合了模型的精确率（precision）和召回率（recall）两个指标，以综合评估模型的性能表现。F1值的计算公式为：F1 = 2 * 精确率 * 召回率 / (精确率 + 召回率)。F1值的取值范围为[0,1]，越接近1表示模型的性能越好。

在实际应用中，我们往往会绘制P-R曲线和ROC曲线来综合评估模型的性能。
- P-R曲线 (Precision-Recall curve)：以召回率为横轴，精确率为纵轴绘制的曲线，用于衡量分类器的性能。
- ROC曲线 (Receiver Operating Characteristic curve)：以假正例率为横轴，真正例率为纵轴绘制的曲线，用于衡量分类器的性能。


下面我们用一个例子来看下ROC曲线：

```python
import numpy as np

from sklearn.datasets import load_iris

iris = load_iris()
target_names = iris.target_names
X, y = iris.data, iris.target
X, y = X[y != 2], y[y != 2]
n_samples, n_features = X.shape


random_state = np.random.RandomState(0)
X = np.concatenate([X, random_state.randn(n_samples, 200 * n_features)], axis=1)

import matplotlib.pyplot as plt

from sklearn import svm
from sklearn.metrics import RocCurveDisplay, auc
from sklearn.model_selection import StratifiedKFold

n_splits = 6
cv = StratifiedKFold(n_splits=n_splits)
classifier = svm.SVC(kernel="linear", probability=True, random_state=random_state)

tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)

fig, ax = plt.subplots(figsize=(6, 6))
for fold, (train, test) in enumerate(cv.split(X, y)):
    classifier.fit(X[train], y[train])
    viz = RocCurveDisplay.from_estimator(
        classifier,
        X[test],
        y[test],
        name=f"ROC fold {fold}",
        alpha=0.3,
        lw=1,
        ax=ax,
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
ax.plot(
    mean_fpr,
    mean_tpr,
    color="b",
    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
    lw=2,
    alpha=0.8,
)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax.fill_between(
    mean_fpr,
    tprs_lower,
    tprs_upper,
    color="grey",
    alpha=0.2,
    label=r"$\pm$ 1 std. dev.",
)

ax.set(
    xlabel="False Positive Rate",
    ylabel="True Positive Rate",
    title=f"Mean ROC curve with variability\n(Positive label '{target_names[1]}')",
)
ax.legend(loc="lower right")
plt.show()
```

画出来的曲线如下图所示：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/roc.png)

#### 2.3.3 过拟合与欠拟合

过拟合（Overfitting）与欠拟合（Underfitting）是机器学习模型在训练和预测过程中常见的问题。它们分别指模型在训练数据和未见数据上的表现差异。

过拟合是指模型在训练数据上表现非常好，但在测试数据或新数据上表现较差。过拟合的模型过于复杂，以至于捕捉到了训练数据中的噪声和随机性，而不是数据的普遍规律。

过拟合的特征为：

- 训练误差低，测试误差高：模型在训练数据上有很高的准确度或很低的误差，但在测试数据上表现不佳。
- 模型复杂：模型可能有过多的参数，或选择了高复杂度的假设空间（例如，高阶多项式回归）。
- 高方差：模型对训练数据的变化非常敏感，对新数据的泛化能力差。

欠拟合是指模型在训练数据和测试数据上都表现不佳。欠拟合的模型过于简单，无法捕捉到数据中的复杂模式或规律。

欠拟合的特征为：

- 训练误差高，测试误差高**：模型在训练数据和测试数据上都表现不理想，误差较高。
- 模型简单：模型可能过于简单，无法充分利用数据中的信息（例如，线性回归应用于非线性数据）。
- 高偏差：模型对数据的规律性认识不足，无法准确描述数据的关系。

过拟合的解决方法为：

- 正则化：通过引入正则化项（如 L1 或 L2 正则化）来惩罚过大的模型参数，限制模型的复杂度。
- 交叉验证：使用交叉验证技术来选择模型参数，确保模型在不同的数据子集上表现一致。
- 简化模型：选择更简单的模型，减少模型的参数数量或降低假设空间的复杂度。
- 增加训练数据：更多的数据可以帮助模型学习更普遍的规律，而不是捕捉到训练数据中的噪声。

欠拟合的解决方法为：

- 增加模型复杂度：选择更复杂的模型，增加模型的参数数量或选择更丰富的假设空间（例如，从线性回归扩展到多项式回归）。
- 特征工程：引入更多的特征或进行特征变换，以提高模型的表达能力。
- 减少正则化：如果模型使用了正则化，适当减少正则化项的权重，使模型可以更充分地拟合数据。
- 提高训练时间：增加训练迭代次数或使用更高效的优化算法，以确保模型充分学习数据。


### 2.4 用NumPy处理数据

要处理数据，我们需要一个强大的工具。NumPy是Python中用于科学计算的核心库之一。它提供了一个强大的N维数组对象，以及许多用于操作这些数组的函数。NumPy是一个开源的Python库，主要用于矩阵上的科学计算。Python本身是一种解释型的语言，不适合于数学计算等对于性能要求比较高的情况，而NumPy就是用C++语言实现的高性能的矩阵计算库的集大成者。NumPy包的核心是多维矩阵的支持。

NumPy的核心是多维数据结构ndarray，它是一个N维数组对象，是一个快速而灵活的大数据集容器。我们在做数据处理时，通常将数据导入到NumPy的ndarray对象。

我们可以通过`pip install numpy`的方式来安装NumPy，然后在Python中通过`import numpy as np`的方式来引入NumPy库。

我们可以通过numpy.array()函数来创建一个ndarray对象，这个函数接受一个列表或者元组作为参数，返回一个新的ndarray对象。

例：
```python
a3 = np.array([[1,0],[0,1]])
```
会生成这样一个多维数组对象
```
array([[1, 0],
       [0, 1]])
```
如果直接使用数组麻烦的话，还可以使用一些内置的函数来生成ndarray对象。

可以通过arange函数来生成指定开始值，结束值和步长值的一维数组。请注意，结束值并不包含在序列中，也就是说结束值是开区间。

```python
In [25]: a4 = np.arange(1,10,1)

In [26]: a4
Out[26]: array([1, 2, 3, 4, 5, 6, 7, 8, 9])
```

与arange类似，linspace通过给定初值、终值和元素个数来生成序列。是否包含终值可以通过endpoint属性来设置。

例：
```python
In [37]: a8 = np.linspace(1,10,10,endpoint=True)

In [38]: a8
Out[38]: array([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.])
```

除了线性的等差数列，我们也可以通过等比数列的方式来生成一维数组。
默认是以10的n次方为参数，比如logspace(0,4,3)的意思是，初值为10的0次方，即1，终值是10的4次方，即100，一共生成3个值。

例，生成[1,100,10000]
```python
In [47]: a9 = np.logspace(0,4,3)

In [48]: a9
Out[48]: array([  1.00000000e+00,   1.00000000e+02,   1.00000000e+04])
```

我们当然也可以修改基数，比如改成3：
```python
In [53]: a10 = np.logspace(1,5,3,base=3)

In [54]: a10
Out[54]: array([   3.,   27.,  243.])
```

#### 2.4.1 多维数组的形状

多维数组中，维度是一个重要的概念。维度是指数组中轴的个数，也就是说，一维数组有一个维度，二维数组有两个维度，三维数组有三个维度，以此类推。一维数组我们通常称为向量，二维数组我们通常称为矩阵，三维以上的数组我们通常称为张量。

NumPy的多维数组的形状是通过shape属性来表示的。shape属性是一个元组，元组的长度表示数组的维度，元组的每个元素表示数组在该维度上的大小。

如果有一个一维数组要转为多维数组，可以通过修改shape属性来实现。

我们可以先将数据存在一维数组中，可以用列表或者元组来生成一维数组，它们是等价的：
例：
```
In [2]: a1 = np.array([1,2,3,4])

In [3]: a1
Out[3]: array([1, 2, 3, 4])

In [4]: a2 = np.array((1,0,0,1))

In [5]: a2
Out[5]: array([1, 0, 0, 1])
```
我们通过shape属性来查看一个数组的形状：
```python
In [14]: a1.shape
Out[14]: (4,)

In [15]: a2.shape
Out[15]: (4,)
```
shape属性是可以直接修改的，比如我们想把上面的a1改成2 x 2的矩阵，就直接改shape值就是了：
```python
In [16]: a1.shape = 2,2

In [17]: a1
Out[17]: 
array([[1, 2],
       [3, 4]])
```
如果能确定一个轴，另一个可以赋-1让系统自己去算。
例：
```python
In [18]: a2.shape= 2,-1

In [19]: a2
Out[19]: 
array([[1, 0],
       [0, 1]])
```

如果想保持这个数组不变，生成一个形状改变的新数组，可以调用reshape方法。
例：我们将一个25个元素的数组生成一个5x5的新数组
```python
In [59]: a11 = np.linspace(1,100,25)

In [60]: a11
Out[60]: 
array([   1.   ,    5.125,    9.25 ,   13.375,   17.5  ,   21.625,
         25.75 ,   29.875,   34.   ,   38.125,   42.25 ,   46.375,
         50.5  ,   54.625,   58.75 ,   62.875,   67.   ,   71.125,
         75.25 ,   79.375,   83.5  ,   87.625,   91.75 ,   95.875,  100.   ])

In [61]: a12 = a11.reshape(5,-1)

In [62]: a12
Out[62]: 
array([[   1.   ,    5.125,    9.25 ,   13.375,   17.5  ],
       [  21.625,   25.75 ,   29.875,   34.   ,   38.125],
       [  42.25 ,   46.375,   50.5  ,   54.625,   58.75 ],
       [  62.875,   67.   ,   71.125,   75.25 ,   79.375],
       [  83.5  ,   87.625,   91.75 ,   95.875,  100.   ]])
```

#### 2.4.2 生成带有数据的多维数组

我们在数据处理中，经常要生成一些特殊的数组，比如全是0的数组，全是1的数组，对角线是1的数组等。NumPy提供了一些函数来生成这些特殊的数组。

zeros函数用于生成全是0的数组，第一个参数是shape：

例：
```python
In [65]: np.zeros((10,10))
Out[65]: 
array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])
```

与zeros类似，ones函数用于生成全是1的数组。

例：
```python
In [66]: np.ones((5,5))
Out[66]: 
array([[ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.]])
```

如果要生成对角线矩阵，可以使用eye函数。

例：
```python
a4 = np.eye(10)
print(a4)
```

结果如下：
```
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
 ```

如果没有特殊要求，可以使用empty函数来生成一个数组，这个数组的值是随机的。

例：
```python
In [67]: np.empty((3,3))
Out[67]: 
array([[  1.   ,   2.125,   3.25 ],
       [  4.375,   5.5  ,   6.625],
       [  7.75 ,   8.875,  10.   ]])
```

通过fromfunction函数可以通过一个函数来生成想要的数组。

例，生成九九乘法表：
```python
In [125]: def mul2(x,y):
     ...:     return (x+1)*(y+1)
     ...: 

In [126]: np.fromfunction(mul2,(9,9))
Out[126]: 
array([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],
       [  2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.],
       [  3.,   6.,   9.,  12.,  15.,  18.,  21.,  24.,  27.],
       [  4.,   8.,  12.,  16.,  20.,  24.,  28.,  32.,  36.],
       [  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.],
       [  6.,  12.,  18.,  24.,  30.,  36.,  42.,  48.,  54.],
       [  7.,  14.,  21.,  28.,  35.,  42.,  49.,  56.,  63.],
       [  8.,  16.,  24.,  32.,  40.,  48.,  56.,  64.,  72.],
       [  9.,  18.,  27.,  36.,  45.,  54.,  63.,  72.,  81.]])
```

#### 2.4.3 访问元素

NumPy中使用[]方括号来访问元素。如果是一维数组，就用下标数字，例如a[1]，如果是多维数组，就在方括号中使用元组tuple，例如a[(2,3,4)]

例：
```python
In [1]: import numpy as np

In [2]: a20 = np.linspace(1,100,27)

In [3]: a20
Out[3]: 
array([   1.        ,    4.80769231,    8.61538462,   12.42307692,
         16.23076923,   20.03846154,   23.84615385,   27.65384615,
         31.46153846,   35.26923077,   39.07692308,   42.88461538,
         46.69230769,   50.5       ,   54.30769231,   58.11538462,
         61.92307692,   65.73076923,   69.53846154,   73.34615385,
         77.15384615,   80.96153846,   84.76923077,   88.57692308,
         92.38461538,   96.19230769,  100.        ])

In [4]: a21 = a20.reshape(3,3,3)

In [5]: a21
Out[5]: 
array([[[   1.        ,    4.80769231,    8.61538462],
        [  12.42307692,   16.23076923,   20.03846154],
        [  23.84615385,   27.65384615,   31.46153846]],

       [[  35.26923077,   39.07692308,   42.88461538],
        [  46.69230769,   50.5       ,   54.30769231],
        [  58.11538462,   61.92307692,   65.73076923]],

       [[  69.53846154,   73.34615385,   77.15384615],
        [  80.96153846,   84.76923077,   88.57692308],
        [  92.38461538,   96.19230769,  100.        ]]])

In [6]: print(a21[(1,1,1)])
50.5
```

#### 2.4.4 切片

用一个值采用方括号下标方式引用，而如果想要引用多个值的话，可以考虑做一个切片。比如s[1:3]就是由s[1]和s[2]组成的列表。

例：
```python
In [10]: a22 = np.linspace(1,10,5)

In [11]: a22
Out[11]: array([  1.  ,   3.25,   5.5 ,   7.75,  10.  ])

In [12]: print(a22[2:4])
[ 5.5   7.75]
```

多维的切片也是同理，比如我们从一个3x3x3的立方体中切出一个2x2x2的小立方体：
```python
In [5]: a21
Out[5]: 
array([[[   1.        ,    4.80769231,    8.61538462],
        [  12.42307692,   16.23076923,   20.03846154],
        [  23.84615385,   27.65384615,   31.46153846]],

       [[  35.26923077,   39.07692308,   42.88461538],
        [  46.69230769,   50.5       ,   54.30769231],
        [  58.11538462,   61.92307692,   65.73076923]],

       [[  69.53846154,   73.34615385,   77.15384615],
        [  80.96153846,   84.76923077,   88.57692308],
        [  92.38461538,   96.19230769,  100.        ]]])

In [8]: slice1 = a21[1:3,1:3,1:3]

In [9]: slice1
Out[9]: 
array([[[  50.5       ,   54.30769231],
        [  61.92307692,   65.73076923]],

       [[  84.76923077,   88.57692308],
        [  96.19230769,  100.        ]]])
```

请注意，切片的语法不用元组，直接在方括号里切就好了。

另外，切片可以使用负数做下标，-1就是右数第一个元素。最左和最右都可以不写，比如从1到最右，可以写成a[1:]

例：
```python
In [11]: a22
Out[11]: array([  1.  ,   3.25,   5.5 ,   7.75,  10.  ])

In [12]: print(a22[2:4])
[ 5.5   7.75]

In [13]: a22[1:]
Out[13]: array([  3.25,   5.5 ,   7.75,  10.  ])

In [14]: a22[1:-1]
Out[14]: array([ 3.25,  5.5 ,  7.75])
```

#### 2.4.5 多维数组的数据类型

在前面的学习中，我们并不在意数据类型，一样也可以使用多维数组。但是，有了类型之后，数组可以更方便和更快速的操作。
我们前面所学习的生成数组的方法，其实都可以默认带一个dtype参数。
类型值常用的有int32, int64, uint32, uint64, float32, float64, complex64, complex128等。因为NumPy是个数学库，精确的类型对于提高计算速度是很有益的。

例：
```python
In [18]: a23 = np.logspace(1,10,5,base=2,dtype=np.float64)

In [19]: a23
Out[19]: 
array([    2.        ,     9.51365692,    45.254834  ,   215.2694823 ,
        1024.        ])
```

#### 2.4.6 对数组的每个元素都进行计算

数据只有可以计算才有价值。我们学会了生成数组，访问数组，下一步就是如何对数组进行计算。
NumPy提供了大量的针对数组进行运算的函数，比如X是一个数组，np.sin(X)可以对数组中每一个元素都进行sin运算。
例：
```python
In [20]: a24 = np.linspace(0, np.pi/2, 10, dtype=np.float64)

In [21]: a24
Out[21]: 
array([ 0.        ,  0.17453293,  0.34906585,  0.52359878,  0.6981317 ,
        0.87266463,  1.04719755,  1.22173048,  1.3962634 ,  1.57079633])

In [22]: a25 = np.sin(a24)

In [23]: a25
Out[23]: 
array([ 0.        ,  0.17364818,  0.34202014,  0.5       ,  0.64278761,
        0.76604444,  0.8660254 ,  0.93969262,  0.98480775,  1.        ])
```

这是一行的，多行的也照样管用，我们看个例子：
```python
In [24]: a26 = np.linspace(0, np.pi*2, 16, dtype=np.float32)

In [25]: a26
Out[25]: 
array([ 0.        ,  0.41887903,  0.83775806,  1.2566371 ,  1.67551613,
        2.09439516,  2.51327419,  2.93215322,  3.35103226,  3.76991129,
        4.18879032,  4.60766935,  5.02654839,  5.44542742,  5.86430645,
        6.28318548], dtype=float32)

In [27]: a27 = np.sin(a26.reshape(4,4))

In [28]: a27
Out[28]: 
array([[  0.00000000e+00,   4.06736642e-01,   7.43144870e-01,
          9.51056540e-01],
       [  9.94521916e-01,   8.66025388e-01,   5.87785184e-01,
          2.07911611e-01],
       [ -2.07911789e-01,  -5.87785363e-01,  -8.66025448e-01,
         -9.94521916e-01],
       [ -9.51056480e-01,  -7.43144751e-01,  -4.06736493e-01,
          1.74845553e-07]], dtype=float32)
```

数组之间支持加减乘除，乘方，取余。

例：给一个数组的每个元素都乘以2
```python
In [31]: a28 = np.array([1,2,3,4]).reshape(2,-1)

In [32]: a28
Out[32]: 
array([[1, 2],
       [3, 4]])

In [33]: a28*2
Out[33]: 
array([[2, 4],
       [6, 8]])
```

两个数组之间做加法：
```python
In [35]: a29 = np.ones((2,2))

In [36]: a29
Out[36]: 
array([[ 1.,  1.],
       [ 1.,  1.]])

In [37]: a28+a29
Out[37]: 
array([[ 2.,  3.],
       [ 4.,  5.]])
```

不但算术运算可以做，也可以针对整个数组做比较大小运算。

例：
```python
In [38]: a29>a28
Out[38]: 
array([[False, False],
       [False, False]], dtype=bool)
```

#### 2.4.7 汇总类的运算

除了对每个元素进行计算，我们还可以对这些元素进行汇总，比如求和sum，求平均值mean等。

例：
```python
In [40]: np.sum(a28)
Out[40]: 10

In [41]: np.mean(a28)
Out[41]: 2.5
```

#### 2.4.8 矩阵matrix

除了前面所讲的多维数组，NumPy还提供了矩阵类matrix. matrix的默认运算都是矩阵运算。
例:
```python
In [45]: a30 = np.matrix(np.linspace(1,10,9,dtype=np.float64).reshape(3,-1))

In [46]: a30
Out[46]: 
matrix([[  1.   ,   2.125,   3.25 ],
        [  4.375,   5.5  ,   6.625],
        [  7.75 ,   8.875,  10.   ]])
In [48]: a31 = np.matrix(np.ones((3,3)))

In [49]: a31
Out[49]: 
matrix([[ 1.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]])

In [50]: np.dot(a30,a31)
Out[50]: 
matrix([[  6.375,   6.375,   6.375],
        [ 16.5  ,  16.5  ,  16.5  ],
        [ 26.625,  26.625,  26.625]])
```

矩阵的逆阵，就可以直接用X**-1来表示。

例：
```python
In [52]: a30 ** -1
Out[52]: 
matrix([[  9.38565300e+14,  -1.87713060e+15,   9.38565300e+14],
        [ -1.87713060e+15,   3.75426120e+15,  -1.87713060e+15],
        [  9.38565300e+14,  -1.87713060e+15,   9.38565300e+14]])

In [53]: a30
Out[53]: 
matrix([[  1.   ,   2.125,   3.25 ],
        [  4.375,   5.5  ,   6.625],
        [  7.75 ,   8.875,  10.   ]])

In [54]: a30 * (a30 ** -1)
Out[54]: 
matrix([[ 0.8125 , -0.125  ,  0.     ],
        [ 0.15625, -1.0625 ,  1.     ],
        [ 0.     ,  0.     ,  2.     ]])
```


### 2.5 用JAX加速NumPy

NumPy最为核心的功能就是多维矩阵的支持。但是，NumPy不能支持GPU和TPU加速，对于我们将来要处理的计算来说不太实用，所以我们这里引入JAX库。

所以从矩阵开始，我们的数据处理转而用JAX来讲。

JAX就是为了NumPy加速设计的，所以前面NumPy的部分我们就不重复讲了。

GPU加速在后面章节有详细介绍，这里借JAX独有的TPU来说明一下。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/jax_logo_250px.webp)

JAX的安装文档请见[JAX官方文档](https://github.com/google/jax#installation)

TPU只有Google一家有，我们只能买到TPU的云服务，不过，我们可以使用Google Colab来使用TPU。

在Colab上，已经安装好了JAX和TPU的运行时。我们运行下面的代码即可激活TPU:

```python
import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()
```

我们来看看有多少个TPU设备可以使用:

```python
print(jax.device_count())
print(jax.local_device_count())
print(jax.devices())
```

输出结果如下：
```
8
8
[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]
```

说明我们有8个TPU设备可以用。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/Tensor_Processing_Unit_3.0.jpg)


#### 2.5.1 矩阵

下面我们就用jax.numpy来代替numpy来使用。

NumPy最重要的功能就是多维矩阵的支持。我们可以通过`np.array`来创建一个多维矩阵。

我们先从一维的向量开始：

```python
import jax.numpy as jnp
a1 = jnp.array([1,2,3])
print(a1)
```

然后我们可以使用二维数组来创建一个矩阵：

```python
a2 = jnp.array([[1,2],[0,4]])
print(a2)
```

矩阵可以统一赋初值。zeros函数可以创建一个全0的矩阵，ones函数可以创建一个全1的矩阵，full函数可以创建一个全是某个值的矩阵。

比如给10行10列的矩阵全赋0值，我们可以这样写：
```python
a3 = jnp.zeros((10,10))
print(a3)
```

全1的矩阵：
```python
a4 = jnp.ones((10,10))
```

全赋100的：
```python
a5 = jnp.full((10,10),100)
```

另外，我们还可以通过linspace函数生成一个序列。linpsace函数的第一个参数是序列的起始值，第二个参数是序列的结束值，第三个参数是序列的长度。比如我们可以生成一个从1到100的序列，长度为100：

```python
a7 = jnp.linspace(1,100,100) # 从1到100，生成100个数
a7.reshape(10,10)
print(a7)
```

最后，JAX给矩阵生成随机值的方式跟NumPy并不一样，并没有jnp.random这样的包。我们可以使用jax.random来生成随机值。JAX的随机数生成函数都需要一个显式的随机状态作为第一个参数，这个状态由两个无符号32位整数组成，称为一个key。用一个key不会修改它，所以重复使用同一个key会得到相同的结果。如果需要新的随机数，可以使用jax.random.split()来生成新的子key。

```python
from jax import random
key = random.PRNGKey(0) # a random key
key, subkey = random.split(key) # split a key into two subkeys
a8 = random.uniform(subkey,shape=(10,10)) # a random number using subkey
print(a8)
```

#### 2.5.2 范数

范数（Norm）是一个数学概念，用于测量向量空间中向量的“大小”。范数需要满足以下性质：

- 非负性：所有向量的范数都大于或等于零，只有零向量的范数为零。
- 齐次性：对任意实数λ和任意向量v，有||λv|| = |λ| ||v||。
- 三角不等式：对任意向量u和v，有||u + v|| ≤ ||u|| + ||v||。
在实际应用中，范数通常用于衡量向量或矩阵的大小，比如在机器学习中，范数常用于正则化项的计算。

常见的范数有：

- L0范数：向量中非零元素的个数。
- L1范数：向量中各个元素绝对值之和，也被称为曼哈顿距离。
- L2范数：向量中各个元素的平方和然后开方，也被称为欧几里得距离。
- 无穷范数：向量中各个元素绝对值的最大值。
需要注意的是，L0范数并不是严格意义上的范数，因为它违反了齐次性。但是在机器学习中，L0范数常用于衡量向量中非零元素的个数，因此也被称为“伪范数”。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/norm.png)

我们先从计算一个一维向量的L1范数开始，不要L1范数这个名字给吓到，其实就是绝对值之和：

```python
norm10_1 = jnp.linalg.norm(a10,ord=1)
print(norm10_1)
```

结果不出所料就是6.

下面我们再看L2范数，也就是欧几里得距离，也就是平方和开方：

```python
a10 = jnp.array([1, 2, 3])
norm10 = jnp.linalg.norm(a10)
print(norm10)
```

根据L2范数的定义，我们可以手动计算一下：norm10 = jnp.sort(1 + 2*2 + 3*3) = 3.7416573.

我们可以看到，上面的norm10的值跟我们手动计算的是一样的。

下面我们来计算无穷范数，其实就是最大值：
```python
norm10_inf = jnp.linalg.norm(a10, ord = jnp.inf)
print(norm10_inf)
```

结果为3.

我们来算一个大点的巩固一下：

```python
a10 = jnp.linspace(1,100,100) # 从1到100，生成100个数
n10 = jnp.linalg.norm(a10,ord=2)
print(n10)
```

这个结果为581.67865.

#### 2.5.3 逆矩阵

对角线是1，其它全是0的方阵，我们称为单位矩阵。在NumPy和JAX中，我们用eye函数来生成单位矩阵。

既然是方阵，就不用跟行和列两个值了，只需要一个值就可以了，这个值就是矩阵的行数和列数。用这一个值赋给eye函数的第一个参数，就可以生成一个单位矩阵。

下面我们来复习一下矩阵乘法是如何计算的。

对于矩阵A的每一行，我们需要与矩阵B的每一列相乘。这里的“相乘”意味着取A的一行和B的一列，然后将它们的对应元素相乘，然后将这些乘积相加。这个和就是结果矩阵中相应位置的元素。

举个例子，假设我们有两个2x2的矩阵A和B：

```
A = 1 2     B = 4 5
    3 4         6 7
```

我们可以这样计算矩阵A和矩阵B的乘积：

```
(1*4 + 2*6) (1*5 + 2*7)     16 19
(3*4 + 4*6) (3*5 + 4*7) =  34 43
```

我们用JAX来计算一下：
```python
ma1 = jnp.array([[1,2],[3,4]])
ma2 = jnp.array([[4,5],[6,7]])
ma3 = jnp.dot(ma1,ma2)
print(ma1)
print(ma2)
print(ma3)
```

输出为：
```
[[1 2]
 [3 4]]
[[4 5]
 [6 7]]
[[16 19]
 [36 43]]
```

如果A*B=I，I为单位矩阵，那么我们称B是A的逆矩阵。

我们可以用inv函数来计算矩阵的逆矩阵。

```python
ma1 = jnp.array([[1,2],[3,4]])
inv1 = jnp.linalg.inv(ma1)
print(inv1)
```

输出的结果为：
```
[[-2.0000002   1.0000001 ]
 [ 1.5000001  -0.50000006]]
 ```

#### 2.5.4 导数与梯度

导数是一个函数在某点处的变化率，用于描述函数在该点处的变化率。导数可以表示函数在该点处的斜率，即函数在该点处的陡峭程度。

梯度(gradient)是一个向量，表示函数在该点处的方向导数沿着该方向取得最大值。梯度可以表示函数在该点处的变化最快和变化率最大的方向。在单变量的实值函数中，梯度可以简单理解为导数。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/grad.png)

JAX作为支持深度学习的框架，对于梯度的支持是被优先考虑的。我们可以使用jax.grad函数来计算梯度。针对一个一元函数，梯度就是导数。我们可以用下面的代码来计算sin函数在x=1.0处的梯度：

```python
import jax
import jax.numpy as jnp

def f(x):
    return jnp.sin(x)

# 计算 f 在 x=1.0 处的梯度
grad_f = jax.grad(f)
print(grad_f(1.0))
```

我们如果每次都沿着梯度方向前进，那么我们就可以找到函数的极值。这种采用梯度方向前进的方法，就是梯度下降法。梯度下降法是一种常用的优化算法，它的核心思想是：如果一个函数在某点的梯度值为正，那么函数在该点沿着梯度方向下降的速度最快；如果一个函数在某点的梯度值为负，那么函数在该点沿着梯度方向上升的速度最快。因此，我们可以通过不断地沿着梯度方向前进，来找到函数的极值。

那么，梯度下降法有什么作用呢？我们可以用梯度下降法来求解函数的最小值。我们可以用下面的代码来求解函数$f(x)=x^2$的最小值：

```python
import jax
import jax.numpy as jnp

def f(x):
    return x ** 2

grad_f = jax.grad(f)

x = 2.0  # 初始点
learning_rate = 0.1  # 学习率
num_steps = 100  # 迭代步数

for i in range(num_steps):
    grad = grad_f(x)  # 计算梯度
    x = x - learning_rate * grad  # 按负梯度方向更新 x

print(x)  # 打印最终的 x 值，应接近 0（函数的最小值）
```

我这一次运行的结果是4.0740736e-10. 也就是说，我们用梯度下降法求解函数$f(x)=x^2$的最小值，最终得到的x值接近于0，也就是函数的最小值。

其中，学习率（或称为步长）是一个正数，用于控制每一步更新的幅度。学习率需要仔细选择，过大可能导致算法不收敛，过小可能导致收敛速度过慢。

#### 2.5.5 概率

唤醒完线性代数和高等数学的一些记忆之后，最后我们来回顾一下概率论。

我们还是从扔硬币说起。我们知道，假设一枚硬币是均匀的，那么只要扔的次数足够多，正面朝上的次数就会接近于总次数的一半。

这种只有两种可能结果的随机试验，我们给它起个高大上的名字叫做伯努利试验(Bernoulli trial)。

下面我们就用JAX的伯努利分布来模拟一下扔硬币的过程。

```python
import jax
import time
from jax import random

# 生成一个形状为 (10, ) 的随机矩阵，元素取值为 0 或 1，概率为 0.5
key = random.PRNGKey(int(time.time()))
rand_matrix = random.bernoulli(key, p=0.5, shape=(10, ))
print(rand_matrix)
mean_x = jnp.mean(rand_matrix)
print(mean_x)
```

mean函数用来求平均值，也叫做数学期望。

打印的结果可能是0.5，也可能是0.3，0.8等等。这是因为我们只扔了10次硬币，扔的次数太少了，所以正面朝上的次数不一定接近于总次数的一半。

这是其中一次0.6的结果：
```
[ True  True  True  True False False  True False False  True]
0.6
```

多跑几次，出现0.1，0.9都不稀奇：
```
[False False False False False False False False False  True]
0.1
```

当我们把shape改成100，1000，10000等更大的数之后，这个结果就离0.5越来越近了。

下面再复习下表示偏差的两个值：
- 方差（Variance）：方差是度量数据点偏离平均值的程度的一种方式。换句话说，它描述了数据点与平均值之间的平均距离的平方。
- 标准差（Standard Deviation）：标准差是方差的平方根。因为方差是在平均偏差的基础上平方得到的，所以它的量纲（单位）与原数据不同。为了解决这个问题，我们引入了标准差的概念。标准差与原数据有相同的量纲，更便于解释。

这两个统计量都反映了数据分布的离散程度。方差和标准差越大，数据点就越分散；反之，方差和标准差越小，数据点就越集中。

我们可以用JAX的var函数来计算方差，用std函数来计算标准差。

```python
import jax
import time
from jax import random

# 生成一个形状为 (1000, ) 的随机矩阵，元素取值为 0 或 1，概率为 0.5
key = random.PRNGKey(int(time.time()))
rand_matrix = random.bernoulli(key, p=0.5, shape=(1000, ))
#print(rand_matrix)
mean_x = jnp.mean(rand_matrix)
print(mean_x)
var_x = jnp.var(rand_matrix)
print(var_x)
std_x = jnp.std(rand_matrix)
print(std_x)
```

最后我们来复习一下之前讲到的信息量。我们来思考一个问题，如何能让伯努利分布的平均信息量最大？

我们先构造两个特殊情况，比如如果p=0，那么我们就永远不会得到正面朝上的结果，这个时候我们就知道了结果，信息量为0。如果p=1，那么我们就永远不会得到反面朝上的结果，这个时候我们也知道了结果，信息量也为0。

如果p=0.01，能给我们带来的平均信息量仍然不大，因为基本上我们可以盲猜结果是背面朝上的，偶然出现的正面朝上的结果，虽然带来了较大的单次信息量，但是出现的概率太低了，所以平均信息量仍然不大。

而如果p=0.5，我们就完全猜不到结果是正面朝上还是背面朝上，这个时候我们得到的平均信息量最大。

当然，这只是定性的分析，我们还需要给出一个定量的公式：

$$
H(X) = - \sum_{x \in X} p(x) \log_2 p(x)
$$

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/prop.png)

```python
import jax.numpy as jnp

# 计算离散型随机变量 X 的平均信息量
def avg_information(p):
    p = jnp.maximum(p, 1e-10)
    return jnp.negative(jnp.sum(jnp.multiply(p, jnp.log2(p))))

# 计算随机变量 X 取值为 0 和 1 的概率分别为 0.3 和 0.7 时的平均信息量
p = jnp.array([0.3, 0.7])
avg_info = avg_information(p)
print(avg_info)
```

我们试几次计算可以得到，当p为0.3时，平均信息量是0.8812325;当p为0.01时，平均信息量为0.08079329；当p为0.5时，平均信息量为1.0，达到最大。

如果嫌使用Python函数的计算慢，我们可以调用JAX的jit函数来加速。我们只需要在函数定义的前面加上@jit即可。

```python
import jax.numpy as jnp
from jax import jit

# 计算离散型随机变量 X 的平均信息量
@jit
def avg_information(p):
    p = jnp.maximum(p, 1e-10)
    return jnp.negative(jnp.sum(jnp.multiply(p, jnp.log2(p))))

# 计算随机变量 X 取值为 0 和 1 的概率分别为 0.3 和 0.7 时的平均信息量
p = jnp.array([0.01, 0.99])
avg_info = avg_information(p)
print(avg_info)
```

#### 2.5.6 JAX的计算图

JAX 通过函数变换（function transformations）来构造计算图，而不是显式地构建计算图。这种方式使得计算图的构建过程更加灵活和动态。以下是 JAX 如何构造和处理计算图的详细解释。

在 JAX 中，计算图的构造主要通过 jax.jit、jax.grad、jax.vmap 等函数变换来实现。这些函数变换会对输入的 Python 函数进行转换，生成对应的计算图。

jax.jit（Just-In-Time compilation，即时编译）用于将 Python 函数编译为高效的 XLA（Accelerated Linear Algebra）代码。它会将函数的计算图编译成高效的机器代码。

```python
import jax
import jax.numpy as jnp

def f(x):
    return jnp.sin(x) + jnp.cos(x)

jit_f = jax.jit(f)

x = jnp.array([1.0, 2.0, 3.0])
y = jit_f(x)
print(y)
```

在上面例子中，jax.jit 会将 f 的计算图编译为高效的机器代码。调用 jit_f 时，JAX 会使用这个编译好的计算图来进行计算。

jax.grad 用于计算函数的梯度。它会根据输入函数自动生成对数值进行反向传播所需的计算图。

```python
def f(x):
    return x**2 + 3 * x + 2

dfdx = jax.grad(f)

x = 3.0
grad = dfdx(x)
print(grad)
```

在上面例子中，jax.grad 会根据 f 构建一个计算其梯度的计算图。

jax.vmap（vectorized map）用于将函数向量化，使其可以批量处理输入数据。它会生成一个新的函数，这个函数在批量数据上应用原始函数。

```python
def f(x):
    return x ** 2

vmap_f = jax.vmap(f)

x = jnp.array([1.0, 2.0, 3.0])
y = vmap_f(x)
print(y)
```

在上面例子中，jax.vmap 会生成一个向量化的 f 函数，使其能够批量处理输入的数组。

JAX 的计算图是动态构建的，这意味着计算图会在函数变换时被构建和优化。与 TensorFlow 或 PyTorch 的静态计算图不同，JAX 的这种动态方式使得它更接近于普通的 Python 编程体验。

JAX 使用 XLA（Accelerated Linear Algebra）作为后端编译器。XLA 是一个用于加速线性代数计算的编译器框架，最初由 Google 开发。JAX 会将计算图转换为 XLA 操作，并利用 XLA 的优化和编译能力生成高效的机器代码。

另外，JAX还支持jax.pmap函数，用于在多个设备上并行地执行函数。这使得 JAX 可以在多个设备上并行地执行函数，从而加速计算。

```python
import jax
import jax.numpy as jnp

# 定义一个简单的函数
def f(x):
    return jnp.sin(x) + jnp.cos(x)

# 使用 jax.pmap 进行并行计算
pmap_f = jax.pmap(f)

# 创建输入数据
x = jnp.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])

# 将计算分配到多个 TPU 核上执行
y = pmap_f(x)
print(y)
```

## 第三章 机器学习框架基础

### 3.1 Scikit-Learn框架基础

Scikit-Learn是一个用于机器学习的Python库，它提供了大量的机器学习算法和工具，可以帮助我们快速构建和训练机器学习模型。Scikit-Learn的核心是估计器（Estimator）类，它封装了机器学习模型的训练和预测过程。Scikit-Learn还提供了大量的工具类和函数，用于数据预处理、特征工程、模型评估等任务。

可以说，本书有一半的内容是在跟sklearn库打交道。

Scikit-learn的开发始于2007年，最初是一个学生项目。

2010年，scikit-learn发布了第一个正式版本，成为一个重要的机器学习库。

2021年，sklearn正式发布1.0版本，正式官宣成熟。

### 3.3 PyTorch框架基础

同JAX一样，PyTorch也是一个支持GPU加速的深度学习框架。PyTorch的核心是张量（Tensor）类，它是一个多维数组，类似于NumPy的ndarray。PyTorch的张量支持GPU加速，可以在GPU上进行张量运算，从而加速深度学习模型的训练和推理。

#### 3.3.1 随机数生成

1. 生成均匀分布的随机数

使用 torch.rand 可以生成在 [0, 1) 区间内均匀分布的随机数。

```python
import torch

# 生成一个 3x3 的张量，元素在 [0, 1) 区间内
random_tensor = torch.rand(3, 3)
print(random_tensor)
```

运行结果如下：

```
tensor([[0.3687, 0.4345, 0.5181],
        [0.0358, 0.5245, 0.1144],
        [0.8433, 0.8607, 0.7082]])
```

2. 生成标准正态分布的随机数

使用 torch.randn 可以生成服从标准正态分布（均值为 0，标准差为 1）的随机数。

```python
import torch

# 生成一个 3x3 的张量，元素服从标准正态分布
normal_tensor = torch.randn(3, 3)
print(normal_tensor)
```

运行结果如下：

```
tensor([[ 1.1555e+00, -1.1852e+00, -1.0129e-03],
        [-5.6360e-02, -1.3856e-01,  3.1301e-01],
        [ 9.5240e-01,  6.7079e-01, -5.9284e-02]])
```

3. 生成指定范围内的均匀分布随机数
使用 torch.randint 可以生成指定整型范围内的随机数。

```python
import torch

# 生成一个 3x3 的张量，元素在 [0, 10) 区间内的随机整数
int_tensor = torch.randint(0, 10, (3, 3))
print(int_tensor)
```

结果都是整数：
```
tensor([[8, 7, 9],
        [0, 1, 6],
        [0, 2, 1]])
```

4. 生成指定均值和标准差的正态分布随机数

使用 torch.normal 可以生成具有指定均值和标准差的正态分布随机数。

```python
import torch

# 生成一个 3x3 的张量，元素服从均值为 2，标准差为 3 的正态分布
mean = 2
std = 3
normal_tensor = torch.normal(mean, std, size=(3, 3))
print(normal_tensor)
```

如果大家已经忘了什么是正态分布，我们可以一起画个图复习一下：
```python
import torch
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

# 设置随机种子以保证可重复性
torch.manual_seed(42)

# 生成正态分布数据
mean = 0.0  # 均值
std = 1.0   # 标准差
num_samples = 1000  # 样本数量

# 使用 PyTorch 生成正态分布数据
data = torch.normal(mean, std, size=(num_samples,))

# 将数据转换为 numpy 数组以便使用 Matplotlib
data_np = data.numpy()

# 绘制直方图
plt.hist(data_np, bins=30, density=True, alpha=0.6, color='g', label='Histogram')

# 绘制正态分布的概率密度函数（PDF）曲线
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = stats.norm.pdf(x, mean, std)
plt.plot(x, p, 'k', linewidth=2, label='Normal PDF')

# 添加标题和标签
plt.title('Normal Distribution')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()

# 显示图形
plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/normal_distribution.png)

5. 生成二项分布的随机数

使用 torch.bernoulli 可以生成服从伯努利分布的随机数。

```python
import torch

# 生成一个 3x3 的张量，元素服从伯努利分布（参数为 0.5）
prob_tensor = torch.full((3, 3), 0.5)
bernoulli_tensor = torch.bernoulli(prob_tensor)
print(bernoulli_tensor)
```

输出如下：

```
tensor([[0., 0., 1.],
        [1., 1., 1.],
        [1., 0., 0.]])
```

6. 生成多项分布的随机数

使用 torch.multinomial 可以生成服从多项分布的随机数。

```python
import torch

# 定义概率分布
probabilities = torch.tensor([0.1, 0.3, 0.6])

# 从定义的概率分布中抽取 5 个样本
multinomial_samples = torch.multinomial(probabilities, 5, replacement=True)
print(multinomial_samples)
```

7. 设置随机种子
为了确保随机数的可重复性，可以设置随机种子。

```python
import torch

# 设置随机种子
torch.manual_seed(42)

# 生成随机数
random_tensor = torch.rand(3, 3)
print(random_tensor)
```

#### 3.3.2 导数与梯度

在 PyTorch 中，计算导数和微分主要依赖于 torch.autograd 模块，它提供了自动微分功能。torch.autograd 可以对张量进行自动求导，是实现反向传播算法的基础。在这部分，我们将讨论如何使用 PyTorch 计算导数和微分，并提供一些示例代码。

首先，需要创建一个启用了梯度计算的张量。可以通过设置 requires_grad=True 来实现。

```python
import torch

# 创建一个张量并启用梯度计算
x = torch.tensor([2.0], requires_grad=True)
```
定义一个函数，并通过前向传播计算其值。然后，通过调用 backward() 方法计算导数。

```python
# 定义一个简单的函数 y = x^2
y = x ** 2

# 计算导数 dy/dx
y.backward()

# 打印导数
print(x.grad)  # 输出: tensor([4.])
```

在这个例子中，$y = x^2$ 的导数 $\frac{dy}{dx}$ 在 x = 2.0 处的值应该是 4.0。

如果函数涉及多个变量，可以对每个变量分别计算导数。

```python
# 创建多个张量并启用梯度计算
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
w = torch.tensor([0.1, 0.2, 0.3], requires_grad=True)
b = torch.tensor([0.5], requires_grad=True)

# 定义一个简单的线性函数 y = w * x + b
y = x * w + b

# 计算导数 dy/dx
y.backward(torch.tensor([1.0, 1.0, 1.0]))

# 打印导数
print(x.grad)  # 输出: tensor([0.1000, 0.2000, 0.3000])
print(w.grad)  # 输出: tensor([1., 2., 3.])
print(b.grad)  # 输出: tensor([3.])
```

在这个例子中，我们对 y 的每个元素分别计算了导数，并使用 backward() 方法传入一个与 y 形状相同的张量，以指定每个元素的权重。

PyTorch 也支持计算高阶导数。可以通过对第一次导数再次调用 backward() 来实现。

```python
import torch

# 创建一个张量并启用梯度计算
x = torch.tensor([2.0], requires_grad=True)

# 定义一个函数 y = x^3
y = x ** 3

# 计算 dy/dx
y.backward(retain_graph=True)

# 打印第一次导数
print(x.grad)  # 输出: tensor([12.])

# 计算高阶导数 d^2y/dx^2
x.grad.zero_()  # 在计算高阶导数前需要清除之前的梯度
grad_y = x.grad.clone()
grad_y.backward()

# 打印第二次导数
print(x.grad)  # 输出: tensor([6.])
```

在这个例子中，y = x^3 的第一次导数在 x = 2.0 处的值是 12.0，第二次导数的值是 6.0。

除了使用 backward() 方法，PyTorch 还提供了 torch.autograd.grad 函数，可以更加灵活地计算导数。

```python
import torch

# 创建一个张量并启用梯度计算
x = torch.tensor([2.0], requires_grad=True)

# 定义一个函数 y = x^2
y = x ** 2

# 使用 torch.autograd.grad 计算导数
grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))

# 打印导数
print(grad_x)  # 输出: (tensor([4.]),)
```

## 第四章 监督学习：分类预测

机器学习的主要问题包括两大类：数值预测和分类预测。按理说应该先从数值预测开始讲，但是，一些回归问题用到的工具在分类中先介绍会更容易理解。所以我们先从分类问题开始讲起。
分类是机器学习中最大的一个分类。很多问题最终都归结到分类问题上。比如手写数字识别，并不是直接识别数字，而是判断这个符号跟哪个数字的分类最接近。

### 4.1 逻辑回归

逻辑回归（Logistic Regression）是一种用于处理分类问题的统计模型，尽管名字中带有“回归”，但实际上它是一种分类方法。逻辑回归模型通过学习输入特征与输出类别之间的关系，来预测数据点属于某个类别的概率。

#### 4.1.1 逻辑回归的原理

逻辑回归本质上是线性回归模型的扩展，但它使用 logistic (sigmoid) 函数 将预测结果映射到 (0, 1) 区间，从而表示概率。线性回归模型输出的值可以是任意实数，而逻辑回归通过 sigmoid 函数将输出值限制在 0 到 1 之间。具体来说，逻辑回归模型首先假设数据服从伯努利分布，然后通过极大似然估计的方法来求解模型参数，使得模型在当前数据集上的预测结果与实际结果尽可能一致。

Logistic 函数（Sigmoid 函数）的数学表达式为：

$\sigma(z)=\frac{1}{1+e^{-z}}$​
 
其中，z是线性回归模型的输出，即：$z=\beta_0+\beta_1 x_1+...+\beta_n x_n$

在上式中，$\beta_0,\beta_1,...,\beta_n$是模型参数，$x_1,...,x_n$是输入特征。

逻辑回归模型输出的是样本属于某个类别的概率。例如，对于二分类问题，输出 σ(z) 表示样本属于类别 1 的概率，1−σ(z) 表示样本属于类别 0 的概率。

逻辑回归的实现步骤

- 数据准备：
    - 收集并预处理数据，确保数据质量和格式适合模型训练。
    - 特征缩放和归一化通常是有益的。
- 模型定义：
    - 定义逻辑回归模型的结构和参数。
- 模型训练：
    - 使用训练数据拟合模型参数，常用的方法是最大似然估计（Maximum Likelihood Estimation, MLE）。
    - 优化算法如梯度下降（Gradient Descent）用于找到最佳参数。
- 模型评估：
    - 使用验证集或交叉验证评估模型性能。
    - 常用的评估指标包括准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1 分数（F1 Score）和 ROC-AUC 曲线等。
- 模型预测：
    - 使用训练好的模型对新数据进行分类预测。

逻辑回归在许多领域有广泛的应用，特别适用于二分类问题，但也可以通过一些扩展方法用于多分类问题。

- 医疗诊断：预测病人是否患某种疾病（如糖尿病、心脏病等）。
- 金融领域：信用评分、贷款违约预测等。
- 市场营销：客户分类、客户流失预测等。
- 社会科学：调查数据分析，事件发生概率预测等。


逻辑回归的优缺点

| 优点                                 | 缺点                                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------------------- |
| 简单易理解：逻辑回归模型简单，易于解释和理解。               | 线性可分性假设：假设数据是线性可分的，对于非线性关系的分类问题效果不佳。 |
| 计算效率高：训练和预测速度快，适合大规模数据集。             | 过拟合：在特征数量较多的情况下，可能会发生过拟合。                       |
| 概率输出：输出的是样本属于某个类别的概率，便于进一步决策。   | 特征工程要求高：需要对输入特征进行仔细选择和处理。                       |
| 特征重要性：模型参数可以解释特征的重要性。                   |                                                                           |
我们可以使用最小二乘法或者梯度下降等方法来求解逻辑回归模型的参数。

#### 4.1.2 用SKlearn实现逻辑回归

通过sklearn库，我们可以很方便地使用逻辑回归模型。我们只需要调用sklearn.linear_model包中的LogisticRegression类，然后使用fit方法拟合数据，使用predict方法进行预测。

下面我们用LogisticRegression类来分类鸢尾花：

```python
# 导入必要的库
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_iris

# 加载鸢尾花数据集，数据集的内容我们在第二章已经介绍过了
iris = load_iris()
X = iris.data
y = iris.target

# 将数据集划分为训练集和测试集，在第二章我们也介绍过
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化逻辑回归模型
log_reg = LogisticRegression(max_iter=200)

# 训练模型
log_reg.fit(X_train, y_train)

# 使用模型进行预测
y_pred = log_reg.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f'准确率: {accuracy:.2f}')

# 打印分类报告
print("分类报告:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

输出结果如下：
```
准确率: 1.00
分类报告:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      1.00      1.00        13
   virginica       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45
```

从上述对鸢尾花数据集的分类结果中，可以得出以下结论：

分类器表现非常优秀

- 精确率（precision）、召回率（recall）、和F1-得分（f1-score）均为1.00，表示分类器在所有类别上的性能都是完美的。
- 精确率为1.00意味着分类器在预测某一类时从未产生误报。
- 召回率为1.00意味着分类器能够正确识别出所有实际属于某一类的样本。
- F1-得分为1.00意味着分类器在权衡精确率和召回率时表现出色。

每个类别的分类都很精确：

- 对于setosa、versicolor、virginica三类样本，分类器都能百分之百正确地分类。
- 支持（support）表示每类样本的数量，setosa有19个样本，versicolor和virginica各有13个样本，分类器在这些样本上的表现均为完美。
- 整体准确率（accuracy）**为1.00：这表示在45个样本中，分类器没有任何分类错误，所有样本都被正确分类。

宏平均（macro avg）和加权平均（weighted avg）：

- 宏平均（macro avg）为1.00，表示对每个类别的精确率、召回率和F1得分的简单平均。
- 加权平均（weighted avg）为1.00，表示对每个类别的精确率、召回率和F1得分的加权平均，考虑了每个类别中的样本数量。这进一步说明分类器在所有类别上都表现得非常一致和完美。

综上所述，分类结果表明，分类器在鸢尾花数据集上的表现是完美的，没有任何分类错误。这通常意味着模型性能极好，但在实际应用中，也需要警惕可能存在的过拟合情况，特别是当样本量较小时。

可以看到，在完全不了解逻辑回归原理的情况下，我们就可以通过sklearn库来实现逻辑回归模型，而且效果还非常好。

下面我们学习一个可视化工具：DecisionBoundaryDisplay。

DecisionBoundaryDisplay是sklearn库中的一个工具，用于可视化分类模型的决策边界。它通过绘制决策边界来展示模型在不同类别之间的划分边界，帮助我们理解模型是如何做出分类决策的。

决策边界是指在特征空间中，模型将不同类别的样本划分开的边界。DecisionBoundaryDisplay可以帮助我们直观地了解模型是如何根据输入特征将样本分为不同类别的。通过决策边界的可视化，我们可以观察到哪些特征对于分类起到了关键作用，以及模型可能存在的不足之处。

需要注意的是，DecisionBoundaryDisplay只能绘制二维数据的决策边界，因此在使用时需要确保数据只有两个特征或更少的维度。

下面我们用DecisionBoundaryDisplay来可视化逻辑回归模型在鸢尾花数据集上的决策边界：

```python
# 导入必要的库
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_iris
from sklearn.inspection import DecisionBoundaryDisplay

# 加载鸢尾花数据集，数据集的内容我们在第二章已经介绍过了
iris = load_iris()
X = iris.data[:, :2]  # 使用前两个特征进行可视化
y = iris.target

# 将数据集划分为训练集和测试集，在第二章我们也介绍过
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化逻辑回归模型
log_reg = LogisticRegression(max_iter=200)

# 训练模型
log_reg.fit(X_train, y_train)

# 使用模型进行预测
y_pred = log_reg.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f'准确率: {accuracy:.2f}')

# 打印分类报告
print("分类报告:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# 绘制决策边界
plt.figure(figsize=(10, 6))
DecisionBoundaryDisplay.from_estimator(log_reg, X_train, response_method="predict", alpha=0.5, cmap=plt.cm.RdYlBu)

# 绘制训练数据点
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k', s=20, cmap=plt.cm.RdYlBu)
plt.title('Logistic Regression Decision Boundary')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.show()
```

输出的结果如下：

```
准确率: 0.82
分类报告:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       0.78      0.54      0.64        13
   virginica       0.65      0.85      0.73        13

    accuracy                           0.82        45
   macro avg       0.81      0.79      0.79        45
weighted avg       0.83      0.82      0.82        45
```

小思考：为什么准确率变差了？

刚才说了，分了可视化，我们只取了特征的前两个维度，准确率当然就变差了。

我们来看看绘制的决策边界：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/log_reg_decision_boundary.png)

图中有三种颜色的区域，分别代表逻辑回归模型对不同类别的预测区域。
- 红色区域：左上角，表示模型预测为第一类（Setosa）。
- 黄色区域：底部，表示模型预测为第二类（Versicolor）。
- 蓝色区域：右上角，表示模型预测为第三类（Virginica）。

通过这个图，我们可以看到模型如何依据特征进行分类，以及不同类别之间的分界线位置。这有助于我们理解模型的决策过程，以及哪些特征对分类起到了关键作用。虽然准确率变差了，但是有助于我们直观地了解模型的分类效果。

#### 4.1.3 用逻辑回归来处理糖尿病数据集

再接再厉，我们用逻辑回归来处理糖尿病数据集：

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

# 加载糖尿病数据集
diabetes = load_diabetes()

# 创建DataFrame
X = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)
y = pd.Series(data=diabetes.target, name='target')

# 将目标变量转换为二分类
# 这里我们假设目标变量大于140视为1（有糖尿病），否则为0（无糖尿病）
y_binary = (y > 140).astype(int)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 训练逻辑回归模型
log_reg = LogisticRegression(max_iter=10000)
log_reg.fit(X_train_scaled, y_train)

# 进行预测
y_pred = log_reg.predict(X_test_scaled)

# 输出混淆矩阵和分类报告
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
```

运行结果如下：

```
Confusion Matrix:
[[36 13]
 [11 29]]

Classification Report:
              precision    recall  f1-score   support

           0       0.77      0.73      0.75        49
           1       0.69      0.72      0.71        40

    accuracy                           0.73        89
   macro avg       0.73      0.73      0.73        89
weighted avg       0.73      0.73      0.73        89
```

我们首先解释下混淆矩阵的结果：

- 行表示真实类别（实际标签）。
- 列表示预测类别（模型预测标签）。

|               | 预测为0 | 预测为1 |
|---------------|---------|---------|
| 实际为0       | 36      | 13      |
| 实际为1       | 11      | 29      |

- 36：真实类别为0且预测为0的样本数（真正类，True Negative）。
- 13：真实类别为0但预测为1的样本数（假阳性，False Positive）。
- 11：真实类别为1但预测为0的样本数（假阴性，False Negative）。
- 29：真实类别为1且预测为1的样本数（真正类，True Positive）。

我们再来看几个核心指标：

- precision (精确率): 预测为正类的样本中实际为正类的比例。
  - 类别0的精确率为0.77。
  - 类别1的精确率为0.69。

- recall (召回率): 实际为正类的样本中被正确预测为正类的比例。
  - 类别0的召回率为0.73。
  - 类别1的召回率为0.72。

- f1-score: 精确率和召回率的调和平均数，用于权衡两者之间的关系。
  - 类别0的F1得分为0.75。
  - 类别1的F1得分为0.71。

- support: 每个类别中样本的实际数量。
  - 类别0有49个样本。
  - 类别1有40个样本。

- accuracy (准确率): 总体预测正确的比例，为0.73（即73%的准确率）。

- macro avg (宏平均): 各类别指标的简单平均。
  - 宏平均精确率、召回率和F1得分均为0.73。

- weighted avg (加权平均): 各类别指标的加权平均，权重为每个类别中的样本数量。
  - 加权平均精确率、召回率和F1得分均为0.73。

下面我们将上面的ROC曲线画出来，代码的详细解释我直接写在对应的代码里：

```python
# 计算ROC曲线和AUC值

# 使用训练好的逻辑回归模型 log_reg 对标准化后的测试数据 X_test_scaled 进行预测，返回的是每个样本属于各个类别的概率。
# [:, 1]：选择属于类别1的概率，这些概率将用于计算ROC曲线。
y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]

# roc_curve(y_test, y_prob)：计算假阳性率（FPR）、真阳性率（TPR）和阈值（thresholds）。FPR 和 TPR 将用于绘制 ROC 曲线。
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
# auc(fpr, tpr)：计算ROC曲线下面积（AUC值），用于量化模型的整体性能。
roc_auc = auc(fpr, tpr)

# 绘制ROC曲线

# 创建一个新的绘图对象。
plt.figure()


# 绘制ROC曲线。
# - fpr：假阳性率数组。
# - tpr：真阳性率数组。
# - color='darkorange'：曲线颜色为深橙色。
# - lw=2：曲线宽度。
# - label=f'ROC curve (area = {roc_auc:.2f})'：曲线标签，显示AUC值（保留两位小数）。

plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')

# 绘制基线（对角线），表示随机猜测的性能。
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')

# 设置x轴范围为0到1。
plt.xlim([0.0, 1.0])

# 设置y轴范围为0到1.05。
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```

绘制出来的结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/roc_log_dia.png)

图中可以直观的感受到模型的性能，AUC值越接近1，模型性能越好。

#### 4.1.4 用逻辑回归处理威斯康星乳腺癌数据集

我们再巩固一下，用逻辑回归来处理威斯康星乳腺癌数据集：

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# 加载威斯康星乳腺癌数据集
data = load_breast_cancer()
X = data.data
y = data.target

# 打印数据集的基本信息
print("Feature names:", data.feature_names)
print("Target names:", data.target_names)
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

# 将数据集分成训练集和测试集，80% 训练，20% 测试
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建逻辑回归模型
model = LogisticRegression(max_iter=10000)

# 训练模型
model.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = model.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# 混淆矩阵
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# 分类报告
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)
```

我们来看看结果，首先是复习下数据集的基本信息：

```
Feature names: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'
 'mean smoothness' 'mean compactness' 'mean concavity'
 'mean concave points' 'mean symmetry' 'mean fractal dimension'
 'radius error' 'texture error' 'perimeter error' 'area error'
 'smoothness error' 'compactness error' 'concavity error'
 'concave points error' 'symmetry error' 'fractal dimension error'
 'worst radius' 'worst texture' 'worst perimeter' 'worst area'
 'worst smoothness' 'worst compactness' 'worst concavity'
 'worst concave points' 'worst symmetry' 'worst fractal dimension']
Target names: ['malignant' 'benign']
Shape of X: (569, 30)
Shape of y: (569,)
```

下面是混淆矩阵和分类报告：

```
Accuracy: 0.956140350877193
Confusion Matrix:
 [[39  4]
 [ 1 70]]
Classification Report:
               precision    recall  f1-score   support

           0       0.97      0.91      0.94        43
           1       0.95      0.99      0.97        71

    accuracy                           0.96       114
   macro avg       0.96      0.95      0.95       114
weighted avg       0.96      0.96      0.96       114
```

0.95这个准确率还是很不错的。

下面我们引入一个新工具库，seaBorn，用来绘制热力图：

```python
# 绘制混淆矩阵的热力图
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()
```

画出图如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/wiscoin_seaborn.png)

内容其实就是混淆矩阵，只不过用颜色表示了出来。

### 4.2 随机梯度下降

在SKlearn库中的文档中也承认，随机梯度下降是一种优化算法，放在监督学习的算法中是不合适的。它是梯度下降的一种变体，通过随机选择样本来估计梯度，从而加快训练速度。与传统的梯度下降法不同，随机梯度下降每次迭代只使用一个样本来更新模型参数，而不是使用所有样本的平均梯度。这种随机性使得随机梯度下降具有更快的收敛速度和更低的计算复杂度。

虽然随机梯度下降提出比较早，但是直到深度学习流行起来之后，才成为教材中的重要部分。

介绍随机梯度下降之前，我们先说一说标准的梯度下降。

标准梯度下降（Batch Gradient Descent）的更新规则如下：
$\theta = \theta - \eta \nabla_\theta J(\theta)$

其中：
- $\theta$ 是模型参数。
- $\eta$ 是学习率。
- $\nabla_\theta J(\theta)$ 是损失函数 $J(\theta)$ 对参数 $\theta$ 的梯度

随机梯度下降（Stochastic Gradient Descent）的更新规则如下：
$\theta = \theta - \eta \nabla_\theta J(\theta; x^{(i)}, y^{(i)})$

其中：
- $(x^{(i)}, y^{(i)})$ 是训练集中的第 $i$ 个样本。
- $\theta$ 是模型参数。
- $\eta$ 是学习率。
- $\nabla_\theta J(\theta; x^{(i)}, y^{(i)})$ 是损失函数 $J(\theta)$ 对参数 $\theta$ 在样本 $(x^{(i)}, y^{(i)})$ 上的梯度。

随机梯度下降每次只使用一个样本来更新参数，因此计算速度快，适合大规模数据集。

随机梯度下降的优点和缺点

| 优点 | 缺点 |
| --- | --- |
| 计算效率高 | 收敛速度慢 |
| 适用于大数据集 | 收敛不稳定 |
| 在线学习 | 需要调参 |

为了在计算效率和收敛稳定性之间找到平衡，可以使用 Mini-batch 梯度下降。它在每次迭代时使用一小批量样本（mini-batch）来计算梯度。这种方法结合了标准梯度下降和 SGD 的优点，既提高了计算效率，又减少了梯度的波动。

#### 4.2.1 用SGDClassifier进行分类

下面我们看一下使用随机梯度下降对鸢尾花数据集进行分类的例子：

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 数据预处理
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 初始化SGD分类器
sgd_clf = SGDClassifier(random_state=42)

# 训练模型
sgd_clf.fit(X_train, y_train)

# 测试模型
y_pred_train = sgd_clf.predict(X_train)
y_pred_test = sgd_clf.predict(X_test)

# 计算混淆矩阵
conf_matrix = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix:")
print(conf_matrix)

# 打印分类报告
class_report = classification_report(y_test, y_pred_test)
print("Classification Report:")
print(class_report)
```

我们来看下混淆矩阵和分类报告：

```
Confusion Matrix:
[[10  0  0]
 [ 0  9  0]
 [ 0  0 11]]
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        10
           1       1.00      1.00      1.00         9
           2       1.00      1.00      1.00        11

    accuracy                           1.00        30
   macro avg       1.00      1.00      1.00        30
weighted avg       1.00      1.00      1.00        30
```

我们可以看到，随机梯度下降在鸢尾花数据集上的分类效果非常好，准确率达到了100%。

下面我们再用SDGClassifier来处理糖尿病数据集：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc

# 加载糖尿病数据集
diabetes = datasets.load_diabetes()
X = diabetes.data
y = (diabetes.target > diabetes.target.mean()).astype(int)  # 目标值二值化

# 数据预处理
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练模型
clf = SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=42)
clf.fit(X_train, y_train)

# 预测结果
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)

# 计算准确率
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")

# 计算混淆矩阵
conf_matrix = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix:")
print(conf_matrix)

# 打印分类报告
class_report = classification_report(y_test, y_pred_test)
print("Classification Report:")
print(class_report)

# 计算 ROC 曲线
y_pred_prob = clf.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

# 绘制 ROC 曲线
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
```

输出结果如下：

```
Train Accuracy: 0.71671388101983
Test Accuracy: 0.7415730337078652
Confusion Matrix:
[[40 10]
 [13 26]]
Classification Report:
              precision    recall  f1-score   support

           0       0.75      0.80      0.78        50
           1       0.72      0.67      0.69        39

    accuracy                           0.74        89
   macro avg       0.74      0.73      0.74        89
weighted avg       0.74      0.74      0.74        89
```

绘制出来的ROC曲线如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/roc_sgd_dia.png)

最后，我们再看下用随机梯度下降来处理威斯康星乳腺癌数据集：

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载威斯康星乳腺癌数据集
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 拆分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 数据标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 初始化 SGDClassifier
sgd = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)

# 训练模型
sgd.fit(X_train, y_train)

# 预测
y_pred = sgd.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=cancer.target_names))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

输出结果如下：

```
Accuracy: 0.98
Classification Report:
              precision    recall  f1-score   support

   malignant       0.98      0.97      0.98        64
      benign       0.98      0.99      0.99       107

    accuracy                           0.98       171
   macro avg       0.98      0.98      0.98       171
weighted avg       0.98      0.98      0.98       171

Confusion Matrix:
[[ 62   2]
 [  1 106]]
 ```

这里的准确率是0.98，效果比用逻辑回归还要好。

目前的算法对照表：

|准确率	|乳腺癌	|糖尿病|
|---|---|---|
|逻辑回归	|0.96	|0.73|
|随机梯度下降	|0.98	|0.74|

#### 4.2.2 用JAX实现逻辑回归

SDGClassifier是sklearn库中的一个工具，我们可以直接使用，但是并不了解其原理。
下面我们用JAX库来实现逻辑回归模型，这样我们就可以更深入地了解逻辑回归的原理。这部分跟后面要讲到的深度学习关系密切。

```python
import jax
import jax.numpy as jnp
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import numpy as np

# 在Jupyter Notebook中使用TPU时的设置
import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

# 加载糖尿病数据集
diabetes = datasets.load_diabetes()
X = diabetes.data
y = (diabetes.target > diabetes.target.mean()).astype(int)  # 目标值二值化

# 数据预处理
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 初始化参数
key = jax.random.PRNGKey(0)
w = jax.random.normal(key, (X_train.shape[1],))
b = 0.0

# 定义逻辑回归模型
@jax.jit
def sigmoid(x):
    return 1 / (1 + jnp.exp(-x))

@jax.jit
def predict(w, b, X):
    return sigmoid(jnp.dot(X, w) + b)

# 定义损失函数（对数损失）
@jax.jit
def loss_fn(w, b, X, y):
    preds = predict(w, b, X)
    loss = -jnp.mean(y * jnp.log(preds) + (1 - y) * jnp.log(1 - preds))
    return loss

# 梯度计算
grad_fn = jax.jit(jax.grad(loss_fn, argnums=(0, 1)))

# 训练模型
learning_rate = 0.1
epochs = 1000

for epoch in range(epochs):
    grads_w, grads_b = grad_fn(w, b, X_train, y_train)
    w -= learning_rate * grads_w
    b -= learning_rate * grads_b

    if epoch % 100 == 0:
        loss = loss_fn(w, b, X_train, y_train)
        print(f"Epoch {epoch}, Loss: {loss}")

# 测试模型
y_pred_train = predict(w, b, X_train)
y_pred_test = predict(w, b, X_test)

# 将预测结果转换为分类标签
y_pred_train_class = (y_pred_train > 0.5).astype(int)
y_pred_test_class = (y_pred_test > 0.5).astype(int)

# 计算准确率
train_accuracy = accuracy_score(y_train, y_pred_train_class)
test_accuracy = accuracy_score(y_test, y_pred_test_class)

print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")

# 计算混淆矩阵
conf_matrix = confusion_matrix(y_test, y_pred_test_class)
print("Confusion Matrix:")
print(conf_matrix)

# 打印分类报告
class_report = classification_report(y_test, y_pred_test_class)
print("Classification Report:")
print(class_report)

# 计算 ROC 曲线
fpr, tpr, _ = roc_curve(y_test, y_pred_test)
roc_auc = auc(fpr, tpr)

# 绘制 ROC 曲线
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()
```

运行结果如下：
```
Epoch 0, Loss: 0.8585748076438904
Epoch 100, Loss: 0.48745399713516235
Epoch 200, Loss: 0.47592517733573914
Epoch 300, Loss: 0.4739748537540436
Epoch 400, Loss: 0.47342705726623535
Epoch 500, Loss: 0.47317200899124146
Epoch 600, Loss: 0.4729995131492615
Epoch 700, Loss: 0.47285935282707214
Epoch 800, Loss: 0.47273680567741394
Epoch 900, Loss: 0.4726276397705078
Train Accuracy: 0.7422096317280453
Test Accuracy: 0.7303370786516854
Confusion Matrix:
[[41  9]
 [15 24]]
Classification Report:
              precision    recall  f1-score   support

           0       0.73      0.82      0.77        50
           1       0.73      0.62      0.67        39

    accuracy                           0.73        89
   macro avg       0.73      0.72      0.72        89
weighted avg       0.73      0.73      0.73        89
```

这个结果，跟使用SGDClassifier的结果基本一致，说明我们的逻辑回归模型写得还不错。

ROC曲线也是差不多：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/roc_jax_sgd_dia.png)

要理解这部分代码，我们还得复习一下逻辑回归的原理。

4.1节中讲过，Logistic 函数（Sigmoid 函数）的数学表达式为：$\sigma(z)=\frac{1}{1+e^{-z}}$​
 
其中，z是线性回归模型的输出，即：$z=\beta_0+\beta_1 x_1+...+\beta_n x_n$

在上式中，$\beta_0,\beta_1,...,\beta_n$是模型参数，$x_1,...,x_n$是输入特征。

逻辑回归模型输出的是样本属于某个类别的概率。例如，对于二分类问题，输出 σ(z) 表示样本属于类别 1 的概率，1−σ(z) 表示样本属于类别 0 的概率。

于是我们就实现这个sigmoid函数：

```python
@jax.jit
def sigmoid(x):
    return 1 / (1 + jnp.exp(-x))
```

@jax.jit 是 JAX 的 Just-In-Time 编译器装饰器，通过 JIT 编译可以提升函数的执行效率。

然后定义预测函数：

```python
@jax.jit
def predict(w, b, X):
    return sigmoid(jnp.dot(X, w) + b)
```

predict 计算输入样本 X 对应的预测概率。具体步骤是先计算线性组合 jnp.dot(X, w) + b，然后应用 sigmoid 函数。
参数：

- w：模型权重向量。
- b：模型偏置。
- X：输入数据矩阵。

下面我们定义损失函数。损失函数（Loss Function），也称为代价函数（Cost Function）或误差函数（Error Function），是用于评估模型预测结果与真实结果之间差异的函数。在机器学习和深度学习中，损失函数的主要作用是为模型提供一个优化目标，使得模型能够逐步调整其参数，以最小化预测误差。

在逻辑回归中，常用的损失函数是对数损失（Log Loss）或二元交叉熵损失（Binary Cross-Entropy Loss）。

二元交叉熵损失（Binary Cross-Entropy Loss），也称为对数损失（Log Loss），是一种用于二分类问题的损失函数。它衡量的是模型预测的概率分布与真实分布之间的差异。具体来说，二元交叉熵损失计算的是每个样本的真实标签与预测标签之间的不确定性（即信息熵）。

计算公式如下：$\text{Loss}=-\frac{1}{N}\sum_{i=1}^N[y_i log(p_i)+(1-y_i)log(1-p_i)]$

代码实现如下，基本就是照搬公式：

```python
@jax.jit
def loss_fn(w, b, X, y):
    preds = predict(w, b, X)
    loss = -jnp.mean(y * jnp.log(preds) + (1 - y) * jnp.log(1 - preds))
    return loss
```

下面是梯度计算：

```python
grad_fn = jax.jit(jax.grad(loss_fn, argnums=(0, 1)))
```

grad_fn 计算损失函数 loss_fn 对参数 (w, b) 的梯度。

对应公式$\theta = \theta - \eta \nabla_\theta J(\theta)$中的$\nabla_\theta J(\theta)$。

下面是梯度下降的真正下降过程了，我们按照上面的公式实现：

```python
learning_rate = 0.1
epochs = 1000

for epoch in range(epochs):
    grads_w, grads_b = grad_fn(w, b, X_train, y_train)
    w -= learning_rate * grads_w
    b -= learning_rate * grads_b
```

训练好之后，我们就可以调用模型进行预测：

```python
y_pred_train = predict(w, b, X_train)
y_pred_test = predict(w, b, X_test)
```

predict预测函数返回的是概率，我们可以根据阈值0.5来判断类别。

```python
y_pred_train_class = (y_pred_train > 0.5).astype(int)
y_pred_test_class = (y_pred_test > 0.5).astype(int)
```

到这一节为止，我们终于把逻辑回归的原理彻底讲明白了。

### 4.3 决策树

决策树是一种用于分类和回归任务的监督学习算法。它通过一系列的二元（是/否）决策将数据划分为不同的类别或预测连续值。决策树模型通过递归地分割数据空间来构建树形结构，其中每个节点代表一个决策点或分裂点。

决策树由三个主要部分组成：

- 根节点（Root Node）：树的起点，包含整个数据集的所有样本。从这里开始，数据被逐步分裂。
- 内部节点（Internal Nodes）：每个内部节点表示对某个特征的决策或测试。根据特征值，将数据分成两个或多个分支。
- 叶节点（Leaf Nodes）：叶节点表示最终的分类或回归结果。不再进一步分裂。

我们来看一个实际中用决策树判断鸢尾花种类的例子：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/iris_decision_tree.png)

我们来看图中的根节点：

| 决策规则                | 基尼系数 (Gini) | 样本数量 (Samples) | 类别分布 (Value) | 预测类别 (Class) |
|-------------------------|-----------------|---------------------|------------------|------------------|
| petal length (cm) <= 2.45 | 0.664           | 105                 | [31, 37, 37]     | versicolor       |


根节点表示所有样本的起始点。一共有105个样本，其中31个属于类别0（setosa）、37个属于类别1（versicolor）和37个属于类别2（virginica）。根据 petal length 特征的值是否小于等于2.45，将数据分成两个子集。

下面我们开始树的第一次分裂：

| 节点               | 决策规则                | 基尼系数 | 样本数量 | 类别分布    | 预测类别   |
|--------------------|-------------------------|----------|----------|-------------|------------|
| 左子节点（第一级） | petal length (cm) <= 2.45 | 0.0      | 31       | [31, 0, 0]  | setosa     |
| 右子节点（第一级） | petal length (cm) > 2.45  | 0.5      | 74       | [0, 37, 37] | versicolor |

我们看到，第一级左子节点的基尼系数为0.0，表示该节点是纯的，所有样本都属于类别0（setosa）。因此，该节点不再分裂，直接预测为 setosa。所以，这个节点是叶子节点。

这第一次分类，就已经将三种鸢尾花之中的Setosa（山鸢尾）共31个完部分出来了。后面要么是Versicolor（变色鸢尾），要么是Virginica（维吉尼亚鸢尾）。
右子树就是内部节点，它包含74个样本，其中37个属于类别1（versicolor）和37个属于类别2（virginica）。

我们再来看右子树的第二次分裂：

| 节点               | 决策规则                | 基尼系数 | 样本数量 | 类别分布    | 预测类别   |
|--------------------|-------------------------|----------|----------|-------------|------------|
| 左子节点（第二级） | petal length (cm) <= 4.75 | 0.5      | 33       | [0, 32, 1]  | versicolor |
| 右子节点（第二级） | petal length (cm) > 4.75  | 0.214    | 41       | [0, 5, 36]  | virginica  |


根据这一个条件，左子树将Versicolor（变色鸢尾）中的32个样本完全分出来了，右子树将Virginica（维吉尼亚鸢尾）中的36个样本完全分出来了。左子树只需要再分一次将32个Versicolor（变色鸢尾）与唯一一个Virginica（维吉尼亚鸢尾）分离。而右子树遇到了点困难，需要多分几次。

通过这样的决策树的图，是不是对于鸢尾花的分类特征有了很直观的认识了？

#### 4.3.1 用DecisionTreeClassifier调用决策树

SKlearn库中的决策树分类器是DecisionTreeClassifier，我们可以直接调用这个分类器来训练数据。得到上面的结果非常容易，我们来看代码：

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 定义决策树分类器
clf = DecisionTreeClassifier(random_state=42)

# 训练模型
clf.fit(X_train, y_train)

# 预测测试集
y_pred = clf.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

# 可视化决策树
plt.figure(figsize=(20,10))
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()
```

显示出的混淆矩阵和分类报告如下：

```
Accuracy: 1.0
Confusion Matrix:
[[19  0  0]
 [ 0 13  0]
 [ 0  0 13]]
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        13
           2       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45
```

分类效果太好，我们也不用画ROC曲线了。

决策树通过递归分裂数据集来构建树形模型。常用的分裂标准包括：

- 信息增益（Information Gain）：基于熵（Entropy）减少的量度。
- 基尼指数（Gini Index）：衡量数据集的不纯度。
- 均方误差（Mean Squared Error, MSE）：用于回归问题，衡量预测值与真实值之间的误差。

具体步骤如下：

- 选择最佳分裂特征：
    - 计算每个特征的分裂标准（如信息增益或基尼指数）。
    - 选择分裂标准最高的特征。
- 分裂数据集：根据选择的特征，将数据集分裂成两个或多个子集。
- 递归构建子树：对每个子集重复步骤1和步骤2，直到满足停止条件（如达到最大树深度或叶节点纯度）。

决策树的优缺点

| 优点                   | 缺点                                                                 |
|----------------------------|--------------------------------------------------------------------------|
| 易于理解和解释：决策树的树形结构直观，决策路径容易理解。 | 容易过拟合：决策树容易生成过于复杂的模型，从而在训练数据上表现很好，但在测试数据上表现不佳。 |
| 无需特征缩放：决策树不需要对特征进行归一化或标准化处理。 | 不稳定性：对数据中的微小变化敏感，不同的数据集可能生成完全不同的树。                |
| 处理非线性关系：可以捕捉数据中的复杂非线性关系。         | 偏向于多值特征：在选择分裂特征时，可能偏向于具有更多取值的特征。                      |
| 处理缺失值：可以自然地处理数据中的缺失值。               |                                                                                  |


下面我们再用决策树来处理糖尿病数据集：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc

# 加载糖尿病数据集
diabetes = load_diabetes()
X = diabetes.data
y = diabetes.target

# 将目标变量转换为二进制分类问题（假设目标变量为0或1）
# 为简单起见，我们可以将目标变量分为两类：小于150为0，大于等于150为1
y = np.where(y < 150, 0, 1)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 初始化并训练决策树分类器
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# 预测测试集
y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)[:, 1]  # 获取预测概率

# 评估模型性能
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# 输出结果
print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(classification_rep)
print("Confusion Matrix:")
print(conf_matrix)

# 绘制决策树
plt.figure(figsize=(400, 200))
plot_tree(clf, filled=True, feature_names=diabetes.feature_names, class_names=['<150', '>=150'], rounded=True)
plt.show()

# 计算ROC曲线和AUC
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

# 绘制ROC曲线
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()
```

我们来看一下输出结果：
```
Accuracy: 0.6179775280898876
Classification Report:
              precision    recall  f1-score   support

           0       0.65      0.65      0.65        49
           1       0.57      0.57      0.57        40

    accuracy                           0.62        89
   macro avg       0.61      0.61      0.61        89
weighted avg       0.62      0.62      0.62        89

Confusion Matrix:
[[32 17]
 [17 23]]
```

综合起来刚刚及格吧。

ROC曲线肯定也不会太好看：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/roc_dia_decision_tree.png)

最后我们来看看决策过程吧：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/dia_decision_tree.png)

密密麻麻看不表吧，这其实就是我想传达的信息，这个模型不但复杂，而且效果不好。

我们取树根部分的局部来看下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/dia_part2.png)

我们还是可以看到分类决策的过程的。

最后，我们再用DecisionTreeRegressor来处理威斯康星乳腺癌数据集：

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# 确保图形在笔记本中显示
%matplotlib inline

# 加载威斯康星乳腺癌数据集
data = load_breast_cancer()
X = data.data
y = data.target

# 将数据集分成训练集和测试集，80% 训练，20% 测试
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建决策树分类器
model = DecisionTreeClassifier(random_state=42)

# 训练模型
model.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = model.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# 混淆矩阵
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# 分类报告
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)
```

运行结果如下：

```
Accuracy: 0.9473684210526315
Confusion Matrix:
 [[40  3]
 [ 3 68]]
Classification Report:
               precision    recall  f1-score   support

           0       0.93      0.93      0.93        43
           1       0.96      0.96      0.96        71

    accuracy                           0.95       114
   macro avg       0.94      0.94      0.94       114
weighted avg       0.95      0.95      0.95       114
```

得分基本上跟使用逻辑回归的差不多。

我们再学习一个新技术，如何将决策树生成的图保存成pdf文件：

```python
from sklearn.tree import export_graphviz
import graphviz

# 导出决策树为dot格式的数据
dot_data = export_graphviz(model, out_file=None, 
                           feature_names=data.feature_names,  
                           class_names=data.target_names,  
                           filled=True, rounded=True,  
                           special_characters=True)  

# 使用 graphviz 渲染决策树
graph = graphviz.Source(dot_data)  
graph.render("breast_cancer_decision_tree")  # 保存为PDF文件
graph  # 在Jupyter Notebook中显示
```

通过sklearn.tree包的export_graphviz函数，我们可以将决策树模型导出为dot格式的数据。然后，我们使用graphviz包的Source类来渲染决策树。最后，我们使用render方法将决策树保存为PDF文件。

这样我们就可以在本地查看生成的决策树图了，生成的pdf中的图片如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/ws_descison.png)

Sklearn库的强大并不仅仅在于它提供了许多机器学习算法，还在于它提供了许多工具函数，方便我们对模型进行可视化和评估。

目前为止的算法准确率表：

|准确率	|乳腺癌	|糖尿病|
|---|---|---|
|逻辑回归	|0.96	|0.73|
|随机梯度下降	|0.98	|0.74|
|决策树	|0.95	|0.62|

#### 4.3.2 JAX实现简易决策树算法

学会了如何调用库，我们下面学习算法的原理。原理部分我们还是使用JAX来实现。

我们首先介绍下基尼不纯度（Gini Impurity）。基尼不纯度是一个用于衡量分类问题中数据集纯度的指标。它表示了一个数据集中随机选取两个样本，其类别不同的概率。基尼不纯度越低，数据集越纯净。

基尼不纯度的公式
对于一个包含$n$个类别的数据集，基尼不纯度 G 定义为：

$G=1-\sum_{i=1}^n(p_i)^2$

其中：$p_i$是第$i$类的概率(即第i类样本数占总样本数的比例)。

我们将其翻译成代码：

```python
def gini_impurity(labels):
    _, counts = jnp.unique(labels, return_counts=True)
    probabilities = counts / counts.sum()
    return 1 - jnp.sum(probabilities ** 2)
```

将左右树的基尼不纯度加权求和，得到当前划分的基尼系数：

```python
gini = (len(left_labels) * gini_impurity(left_labels) + len(right_labels) * gini_impurity(right_labels)) / len(labels)
```

以此为依据，我们就可以写一个完整的树的划分：

```python
def find_best_split(dataset, labels):
    best_gini = float('inf')
    best_feature = None
    best_threshold = None

    n_features = dataset.shape[1]
    for feature in range(n_features):
        thresholds = jnp.unique(dataset[:, feature])
        for threshold in thresholds:
            _, left_labels, _, right_labels = split_dataset(dataset, labels, feature, threshold)
            if len(left_labels) == 0 or len(right_labels) == 0:
                continue
            gini = (len(left_labels) * gini_impurity(left_labels) + len(right_labels) * gini_impurity(right_labels)) / len(labels)
            if gini < best_gini:
                best_gini = gini
                best_feature = feature
                best_threshold = threshold

    return best_feature, best_threshold
```

首先，初始化最佳基尼不纯度 `best_gini` 为无穷大，最佳特征 `best_feature` 和最佳阈值 `best_threshold` 为 None。

然后，遍历每个特征，对于每个特征，找出所有可能的阈值。然后，对于每个阈值，使用 `split_dataset` 函数将数据集分割成左右两个子集，并获取对应的标签。

如果所有的标签都相同，或者达到了最大深度限制，那么返回出现次数最多的标签。

如果没有，则使用 find_best_split 函数找到最佳的分割特征和阈值。

如果没有找到有效的分割特征，那么返回出现次数最多的标签。

如果找到了，使用找到的最佳分割特征和阈值将数据集分割成左右两个子集，并获取对应的标签。

我们再看一下如何划分左右数据集，解释我直接写在代码里了：

```python
def split_dataset(dataset, labels, feature, threshold):
    # 创建一个布尔掩码 left_mask，其中的每个元素表示对应的数据点是否应该被分到左子集。如果数据点在指定特征上的值小于或等于阈值，那么这个元素就是 True。
    left_mask = dataset[:, feature] <= threshold
    # 创建另一个布尔掩码 right_mask，其中的每个元素表示对应的数据点是否应该被分到右子集。这是通过取 left_mask 的逻辑非来实现的。
    right_mask = ~left_mask
    return dataset[left_mask], labels[left_mask], dataset[right_mask], labels[right_mask]
```

以划分数据为基础，我们就可以写出决策树的训练函数，这是一个递归的过程：

```python
    def fit(self, dataset, labels, depth=0):
        if len(jnp.unique(labels)) == 1 or (self.max_depth is not None and depth >= self.max_depth):
            return jnp.argmax(jnp.bincount(labels))

        feature, threshold = find_best_split(dataset, labels)
        if feature is None:
            return jnp.argmax(jnp.bincount(labels))

        left_dataset, left_labels, right_dataset, right_labels = split_dataset(dataset, labels, feature, threshold)
        left_subtree = self.fit(left_dataset, left_labels, depth + 1)
        right_subtree = self.fit(right_dataset, right_labels, depth + 1)

        return (feature, threshold, left_subtree, right_subtree)
```

训练之后，我们就可以用决策树进行单点预测：

```python
    def predict_one(self, x, tree):
        if not isinstance(tree, tuple):
            return tree

        feature, threshold, left_subtree, right_subtree = tree
        return jax.lax.cond(
            x[feature] <= threshold,
            lambda _: self.predict_one(x, left_subtree),
            lambda _: self.predict_one(x, right_subtree),
            operand=None
        )
```

跟排序算法有点像，如果数据点在指定特征上的值小于或等于阈值，那么递归地对左子树进行预测；否则，递归地对右子树进行预测。operand=None 表示条件判断的结果不需要额外的操作数。

最后，我们再来个vmap，对每个数据进行预测：

```python
    def predict(self, dataset):
        return vmap(lambda x: self.predict_one(x, self.tree))(dataset)
```

决策树的可视化如此重要，我们手动写的也要增加这个功能：

```python
    def visualize_tree(self, tree=None, depth=0):
        if tree is None:
            tree = self.tree

        if not isinstance(tree, tuple):
            print(f"{'  ' * depth}Leaf: Class {tree}")
            return

        feature, threshold, left_subtree, right_subtree = tree
        print(f"{'  ' * depth}Feature {feature} <= {threshold:.2f}")
        self.visualize_tree(left_subtree, depth + 1)
        print(f"{'  ' * depth}Feature {feature} > {threshold:.2f}")
        self.visualize_tree(right_subtree, depth + 1)
```

最后我们把决策树串起来：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score
from sklearn.preprocessing import OneHotEncoder
import jax
import jax.numpy as jnp
from sklearn import datasets
from sklearn.model_selection import train_test_split
from jax import vmap

# 初始化 TPU
import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

# 加载 Iris 数据集
iris = datasets.load_iris()
data = iris.data
labels = iris.target

# 将数据分为训练集和测试集
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)

# 将数据转换为 JAX 数组
train_data = jnp.array(train_data)
test_data = jnp.array(test_data)
train_labels = jnp.array(train_labels)
test_labels = jnp.array(test_labels)

def gini_impurity(labels):
    _, counts = jnp.unique(labels, return_counts=True)
    probabilities = counts / counts.sum()
    return 1 - jnp.sum(probabilities ** 2)

def split_dataset(dataset, labels, feature, threshold):
    left_mask = dataset[:, feature] <= threshold
    right_mask = ~left_mask
    return dataset[left_mask], labels[left_mask], dataset[right_mask], labels[right_mask]

def find_best_split(dataset, labels):
    best_gini = float('inf')
    best_feature = None
    best_threshold = None

    n_features = dataset.shape[1]
    for feature in range(n_features):
        thresholds = jnp.unique(dataset[:, feature])
        for threshold in thresholds:
            _, left_labels, _, right_labels = split_dataset(dataset, labels, feature, threshold)
            if len(left_labels) == 0 or len(right_labels) == 0:
                continue
            gini = (len(left_labels) * gini_impurity(left_labels) + len(right_labels) * gini_impurity(right_labels)) / len(labels)
            if gini < best_gini:
                best_gini = gini
                best_feature = feature
                best_threshold = threshold

    return best_feature, best_threshold

class DecisionTree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = None

    def fit(self, dataset, labels, depth=0):
        if len(jnp.unique(labels)) == 1 or (self.max_depth is not None and depth >= self.max_depth):
            return jnp.argmax(jnp.bincount(labels))

        feature, threshold = find_best_split(dataset, labels)
        if feature is None:
            return jnp.argmax(jnp.bincount(labels))

        left_dataset, left_labels, right_dataset, right_labels = split_dataset(dataset, labels, feature, threshold)
        left_subtree = self.fit(left_dataset, left_labels, depth + 1)
        right_subtree = self.fit(right_dataset, right_labels, depth + 1)

        return (feature, threshold, left_subtree, right_subtree)

    def predict_one(self, x, tree):
        if not isinstance(tree, tuple):
            return tree

        feature, threshold, left_subtree, right_subtree = tree
        return jax.lax.cond(
            x[feature] <= threshold,
            lambda _: self.predict_one(x, left_subtree),
            lambda _: self.predict_one(x, right_subtree),
            operand=None
        )

    def predict(self, dataset):
        return vmap(lambda x: self.predict_one(x, self.tree))(dataset)

    def predict_proba(self, dataset):
        predictions = self.predict(dataset)
        proba = jnp.zeros((len(predictions), 3))
        for i, label in enumerate(predictions):
            proba = proba.at[i, label].set(1)
        return proba

    def visualize_tree(self, tree=None, depth=0):
        if tree is None:
            tree = self.tree

        if not isinstance(tree, tuple):
            print(f"{'  ' * depth}Leaf: Class {tree}")
            return

        feature, threshold, left_subtree, right_subtree = tree
        print(f"{'  ' * depth}Feature {feature} <= {threshold:.2f}")
        self.visualize_tree(left_subtree, depth + 1)
        print(f"{'  ' * depth}Feature {feature} > {threshold:.2f}")
        self.visualize_tree(right_subtree, depth + 1)

# 创建决策树分类器实例
tree = DecisionTree(max_depth=3)

# 训练模型
tree.tree = tree.fit(train_data, train_labels)

# 可视化决策
print("Decision Tree structure:")
tree.visualize_tree()

# 对测试数据进行预测
test_predictions = tree.predict(test_data)

# 计算准确率
accuracy = accuracy_score(test_labels, test_predictions)
print(f'Accuracy: {accuracy * 100:.2f}%')

# 计算混淆矩阵
conf_matrix = confusion_matrix(test_labels, test_predictions)
print('Confusion Matrix:')
print(conf_matrix)

# 获取预测概率
test_proba = tree.predict_proba(test_data)

# 对标签进行 one-hot 编码
encoder = OneHotEncoder(sparse_output=False)
test_labels_onehot = encoder.fit_transform(test_labels.reshape(-1, 1))

# 计算每个类别的 ROC 曲线和 AUC
fpr = {}
tpr = {}
roc_auc = {}

for i in range(3):
    fpr[i], tpr[i], _ = roc_curve(test_labels_onehot[:, i], test_proba[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# 绘制多类 ROC 曲线
plt.figure()
colors = ['blue', 'green', 'red']
for i, color in zip(range(3), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Multi-Class')
plt.legend(loc="lower right")
plt.show()
```

我们来看下结果：

```
Decision Tree structure:
Feature 2 <= 1.90
  Leaf: Class 0
Feature 2 > 1.90
  Feature 2 <= 4.70
    Feature 3 <= 1.60
      Leaf: Class 1
    Feature 3 > 1.60
      Leaf: Class 2
  Feature 2 > 4.70
    Feature 3 <= 1.70
      Leaf: Class 1
    Feature 3 > 1.70
      Leaf: Class 2
Accuracy: 100.00%
Confusion Matrix:
[[10  0  0]
 [ 0  9  0]
 [ 0  0 11]]
```

我们把决策树输出绘制成一张图：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/decision_tree_1.png)

分类过程跟前面DecisionTreeClassifier的结果基本一致，效果也很好。

下面我们用我们手写的决策树讨论下糖尿病数据集：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
import jax
import jax.numpy as jnp
from jax import vmap

# 初始化 TPU
import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

# 加载糖尿病数据集
diabetes = load_diabetes()
data = diabetes.data
labels = (diabetes.target > diabetes.target.mean()).astype(int)  # 将目标值二值化

# 将数据分为训练集和测试集
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)

# 将数据转换为 JAX 数组
train_data = jnp.array(train_data)
test_data = jnp.array(test_data)
train_labels = jnp.array(train_labels)
test_labels = jnp.array(test_labels)

def gini_impurity(labels):
    _, counts = jnp.unique(labels, return_counts=True)
    probabilities = counts / counts.sum()
    return 1 - jnp.sum(probabilities ** 2)

def split_dataset(dataset, labels, feature, threshold):
    left_mask = dataset[:, feature] <= threshold
    right_mask = ~left_mask
    return dataset[left_mask], labels[left_mask], dataset[right_mask], labels[right_mask]

def find_best_split(dataset, labels):
    best_gini = float('inf')
    best_feature = None
    best_threshold = None

    n_features = dataset.shape[1]
    for feature in range(n_features):
        thresholds = jnp.unique(dataset[:, feature])
        for threshold in thresholds:
            _, left_labels, _, right_labels = split_dataset(dataset, labels, feature, threshold)
            if len(left_labels) == 0 or len(right_labels) == 0:
                continue
            gini = (len(left_labels) * gini_impurity(left_labels) + len(right_labels) * gini_impurity(right_labels)) / len(labels)
            if gini < best_gini:
                best_gini = gini
                best_feature = feature
                best_threshold = threshold

    return best_feature, best_threshold

class DecisionTree:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = None

    def fit(self, dataset, labels, depth=0):
        if len(jnp.unique(labels)) == 1 or (self.max_depth is not None and depth >= self.max_depth):
            return jnp.argmax(jnp.bincount(labels))

        feature, threshold = find_best_split(dataset, labels)
        if feature is None:
            return jnp.argmax(jnp.bincount(labels))

        left_dataset, left_labels, right_dataset, right_labels = split_dataset(dataset, labels, feature, threshold)
        left_subtree = self.fit(left_dataset, left_labels, depth + 1)
        right_subtree = self.fit(right_dataset, right_labels, depth + 1)

        return (feature, threshold, left_subtree, right_subtree)

    def predict_one(self, x, tree):
        if not isinstance(tree, tuple):
            return tree

        feature, threshold, left_subtree, right_subtree = tree
        return jax.lax.cond(
            x[feature] <= threshold,
            lambda _: self.predict_one(x, left_subtree),
            lambda _: self.predict_one(x, right_subtree),
            operand=None
        )

    def predict(self, dataset):
        return vmap(lambda x: self.predict_one(x, self.tree))(dataset)

    def predict_proba(self, dataset):
        predictions = self.predict(dataset)
        proba = jnp.zeros((len(predictions), 2))
        for i, label in enumerate(predictions):
            proba = proba.at[i, label].set(1)
        return proba

    def visualize_tree(self, tree=None, depth=0):
        if tree is None:
            tree = self.tree

        if not isinstance(tree, tuple):
            print(f"{'  ' * depth}Leaf: Class {tree}")
            return

        feature, threshold, left_subtree, right_subtree = tree
        print(f"{'  ' * depth}Feature {feature} <= {threshold:.2f}")
        self.visualize_tree(left_subtree, depth + 1)
        print(f"{'  ' * depth}Feature {feature} > {threshold:.2f}")
        self.visualize_tree(right_subtree, depth + 1)

# 创建决策树分类器实例
tree = DecisionTree(max_depth=5)

# 训练模型
tree.tree = tree.fit(train_data, train_labels)

# 可视化决策树
print("Decision Tree structure:")
tree.visualize_tree()

# 对测试数据进行预测
test_predictions = tree.predict(test_data)

# 计算准确率
accuracy = accuracy_score(test_labels, test_predictions)
print(f'Accuracy: {accuracy * 100:.2f}%')

# 计算混淆矩阵
conf_matrix = confusion_matrix(test_labels, test_predictions)
print('Confusion Matrix:')
print(conf_matrix)

# 获取预测概率
test_proba = tree.predict_proba(test_data)

# 对标签进行 one-hot 编码
encoder = OneHotEncoder(sparse_output=False)
test_labels_onehot = encoder.fit_transform(test_labels.reshape(-1, 1))

# 计算每个类别的 ROC 曲线和 AUC
fpr = {}
tpr = {}
roc_auc = {}

for i in range(2):
    fpr[i], tpr[i], _ = roc_curve(test_labels_onehot[:, i], test_proba[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# 绘制多类 ROC 曲线
plt.figure()
colors = ['blue', 'red']
for i, color in zip(range(2), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Binary Classification')
plt.legend(loc="lower right")
plt.show()
```

首先我们看下我们的成绩：

```
Accuracy: 74.16%
Confusion Matrix:
[[40 10]
 [13 26]]
```

效果比官方库的还好！

来看下我们70多分的ROC曲线：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/roc_dia_dt_jax.png)

下面节选下我们的决策树结构的左子树：

```
Feature 2 <= 0.01
├── Feature 8 <= 0.00
│   ├── Feature 3 <= -0.01
│   │   ├── Feature 2 <= 0.00
│   │   │   ├── Feature 7 <= -0.01
│   │   │   │   └── Leaf: Class 0
│   │   │   └── Feature 7 > -0.01
│   │   │       └── Leaf: Class 0
│   │   └── Feature 2 > 0.00
│   │       └── Leaf: Class 1
│   └── Feature 3 > -0.01
│       ├── Feature 5 <= -0.04
│       │   ├── Feature 5 <= -0.07
│       │   │   └── Leaf: Class 0
│       │   └── Feature 5 > -0.07
│       │       └── Leaf: Class 1
│       └── Feature 5 > -0.04
│           ├── Feature 4 <= 0.05
│           │   └── Leaf: Class 0
│           └── Feature 4 > 0.05
│               └── Leaf: Class 1
└── Feature 8 > 0.00
    ├── Feature 5 <= -0.03
    │   ├── Feature 2 <= -0.06
    │   │   └── Leaf: Class 0
    │   └── Feature 2 > -0.06
    │       ├── Feature 4 <= -0.10
    │       │   └── Leaf: Class 0
    │       └── Feature 4 > -0.10
    │           └── Leaf: Class 1
    └── Feature 5 > -0.03
        ├── Feature 6 <= 0.03
        │   ├── Feature 4 <= -0.01
        │   │   └── Leaf: Class 0
        │   └── Feature 4 > -0.01
        │       └── Leaf: Class 1
        └── Feature 6 > 0.03
            ├── Feature 5 <= -0.01
            │   └── Leaf: Class 0
            └── Feature 5 > -0.01
                └── Leaf: Class 0
```

画成图效果是这样的：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/decision_tree_2.png)

#### 4.3.3 ID3算法

上节我们的决策树算法使用的是CART算法。CART（Classification and Regression Trees）算法是由Breiman等人在1984年提出的一种用于生成决策树的算法。CART算法基于基尼不纯度（Gini Impurity）来选择每个节点的特征。其目标是通过选择能够最小化基尼不纯度的特征，逐步构建决策树。本节和下节我们将介绍另外两种决策树算法：ID3算法和C4.5算法。

ID3（Iterative Dichotomiser 3）算法是由Ross Quinlan在1986年提出的一种用于生成决策树的算法。ID3算法基于信息论中的信息增益（Information Gain）来选择每个节点的特征。其目标是通过选择能够最大化信息增益的特征，逐步构建决策树。

信息增益是衡量选择某一特征进行分割后，数据集的纯度增加的程度。

首先我们计算数据集的熵（Entropy）。熵是数据集纯度的度量，定义为：

$H(D)= -\sum_{i=1}^n p_i \log_2 p_i$

其中，$p_i$ 是第 $i$ 类样本的概率。

我们用代码实现：

```python
    def _entropy(self, labels):
        # 使用 NumPy 的 unique 函数找出 labels 中所有不同的标签，并且通过设置 return_counts=True 返回这些不同标签的出现次数（counts）。这里下划线 _ 被用作占位符，表示我们不关心唯一标签的具体值，只关心它们的计数。
        _, counts = np.unique(labels, return_counts=True)
        # 计算每个不同标签出现的概率。这是通过将每个标签的出现次数除以标签总数来实现的。
        probabilities = counts / len(labels)
        return -np.sum(probabilities * np.log2(probabilities))
```

然后，我们要计算特征𝐴对数据集𝐷的条件熵。条件熵衡量的是在已知特征𝐴的情况下数据集 𝐷的纯度，定义为：

$H(D|A)=\sum_{v\in \text{Values(A)}} \frac{|D_v|}{|D|} H(D_v)$

其中，$D_v$ 是特征𝐴的第 $v$ 个取值对应的数据子集。Values(A)是特征A的所有可能取值。

最后，我们计算信息增益。信息增益是数据集的熵减去特征𝐴对数据集𝐷的条件熵，定义为：

$Gain(D,A)=H(D)-H(D|A)$

我们来用代码实现：

```python
    def _information_gain(self, data, labels, feature):
        # 计算数据集的熵
        original_entropy = self._entropy(labels)
        # 使用 NumPy 的 unique 函数找出指定特征列中的所有不同值以及它们出现的次数
        values, counts = np.unique(data[:, feature], return_counts=True)
        # 计算特征 A 对数据集 D 的条件熵
        weighted_entropy = sum((counts[i] / np.sum(counts)) * self._entropy(labels[data[:, feature] == values[i]]) for i in range(len(values)))
        return original_entropy - weighted_entropy

```

下面我们用信息熵增益来寻找最佳划分：

```python
    def _best_feature_to_split(self, data, labels):
        best_feature = -1
        best_gain = -1
        num_features = data.shape[1] # 获取数据集中特征的数量
        for feature in range(num_features):
            # 对当前遍历到的特征，调用 _information_gain 方法计算其信息增益
            gain = self._information_gain(data, labels, feature)
            if gain > best_gain:
                best_gain = gain
                best_feature = feature
        return best_feature
```

有了最佳划分算法之后，我们来构建决策树：

```python
    def _build_tree(self, data, labels):
        # 检查当前数据集的标签是否全部相同，如果是，说明已经到达叶节点，可以直接返回该标签
        if len(np.unique(labels)) == 1:
            return labels[0]

        # 检查当前数据集的特征是否为空，如果是，说明没有更多的特征可以用来进一步划分数据，此时返回出现次数最多的标签作为叶节点的预测结果
        if data.shape[1] == 0:
            return np.bincount(labels).argmax()

        # 调用 _best_feature_to_split 方法找到当前数据集的最佳划分特征
        best_feature = self._best_feature_to_split(data, labels)
        # 创建一个以最佳特征为键的字典，用于存储决策树的节点
        tree = {best_feature: {}}
        # 获取最佳特征的所有唯一值，这些值将用于划分数据集
        values = np.unique(data[:, best_feature])

        for value in values:
            sub_data = data[data[:, best_feature] == value] # 根据当前特征值筛选出子数据集
            sub_labels = labels[data[:, best_feature] == value] # 同时筛选出对应的子标签集
            # 递归调用 _build_tree 方法构建子树，注意在递归调用时需要删除已经使用过的特征，以避免重复划分
            subtree = self._build_tree(np.delete(sub_data, best_feature, axis=1), sub_labels)
            # 将构建好的子树添加到当前节点的字典中，键为特征值，值为子树
            tree[best_feature][value] = subtree
        return tree
```

下面我们来写预测函数：

```python
    def _predict_instance(self, instance, tree):
        # 检查当前的树节点是否是字典类型，如果不是，说明已经到达叶节点，直接返回节点的值作为预测结果
        if not isinstance(tree, dict):
            return tree

        # 获取当前树节点的特征
        feature = next(iter(tree))
        # 获取当前实例在当前特征上的值
        value = instance[feature]
        # 根据特征值获取对应的子树，如果不存在，则返回 None
        subtree = tree[feature].get(value, None)

        # 如果子树不存在，说明在训练集中没有遇到这样的特征值组合，此时返回训练集标签中出现次数最多的标签作为预测结果
        if subtree is None:
            return np.bincount(labels).argmax()
        
        # 如果子树存在，递归调用 _predict_instance 方法，继续沿着决策树进行预测，注意在递归调用时需要删除已经使用过的特征，以避免重复判断
        return self._predict_instance(np.delete(instance, feature), subtree)

    def predict(self, data):
        # 使用列表推导式，对数据集中的每个实例调用 _predict_instance 方法进行预测，并将预测结果收集到一个列表中
        predictions = [self._predict_instance(instance, self.tree) for instance in data]
        # 将预测结果列表转换为 NumPy 数组并返回
        return np.array(predictions)
```

我们把代码串起来：

```python
import numpy as np
import pandas as pd

class ID3:
    def __init__(self):
        self.tree = {}

    def fit(self, data, labels):
        self.tree = self._build_tree(data, labels)
        return self.tree

    def _entropy(self, labels):
        _, counts = np.unique(labels, return_counts=True)
        probabilities = counts / len(labels)
        return -np.sum(probabilities * np.log2(probabilities))

    def _information_gain(self, data, labels, feature):
        original_entropy = self._entropy(labels)
        values, counts = np.unique(data[:, feature], return_counts=True)
        weighted_entropy = sum((counts[i] / np.sum(counts)) * self._entropy(labels[data[:, feature] == values[i]]) for i in range(len(values)))
        return original_entropy - weighted_entropy

    def _best_feature_to_split(self, data, labels):
        best_feature = -1
        best_gain = -1
        num_features = data.shape[1]
        for feature in range(num_features):
            gain = self._information_gain(data, labels, feature)
            if gain > best_gain:
                best_gain = gain
                best_feature = feature
        return best_feature

    def _build_tree(self, data, labels):
        if len(np.unique(labels)) == 1:
            return labels[0]

        if data.shape[1] == 0:
            return np.bincount(labels).argmax()

        best_feature = self._best_feature_to_split(data, labels)
        tree = {best_feature: {}}
        values = np.unique(data[:, best_feature])

        for value in values:
            sub_data = data[data[:, best_feature] == value]
            sub_labels = labels[data[:, best_feature] == value]
            subtree = self._build_tree(np.delete(sub_data, best_feature, axis=1), sub_labels)
            tree[best_feature][value] = subtree

        return tree

    def predict(self, data):
        predictions = [self._predict_instance(instance, self.tree) for instance in data]
        return np.array(predictions)

    def _predict_instance(self, instance, tree):
        if not isinstance(tree, dict):
            return tree

        feature = next(iter(tree))
        value = instance[feature]
        subtree = tree[feature].get(value, None)

        if subtree is None:
            return np.bincount(labels).argmax()
        
        return self._predict_instance(np.delete(instance, feature), subtree)

# 示例数据集
data = np.array([
    [1, 1, 0],
    [1, 0, 0],
    [0, 1, 1],
    [0, 1, 1],
    [0, 0, 0]
])
labels = np.array([0, 0, 1, 1, 0])

# 创建ID3实例并训练
id3 = ID3()
tree = id3.fit(data, labels)
print("Decision Tree:", tree)

# 预测
test_data = np.array([
    [1, 1, 0],
    [0, 0, 0]
])
predictions = id3.predict(test_data)
print("Predictions:", predictions)
```

运行结果如下：

```
Decision Tree: {2: {0: 0, 1: 1}}
Predictions: [0 0]
```

这里，我们创建了一个包含5个样本的示例数据集。data是一个二维数组，每行代表一个样本，每列代表一个特征。labels是一个一维数组，包含每个样本的标签或类别。

data中的每一行是一个样本：
第一个样本 [1, 1, 0] 的标签是 0
第二个样本 [1, 0, 0] 的标签是 0
第三个样本 [0, 1, 1] 的标签是 1
第四个样本 [0, 1, 1] 的标签是 1
第五个样本 [0, 0, 0] 的标签是 0

我们使用训练好的决策树对新的数据进行预测。

test_data包含两个样本：
第一个测试样本 [1, 1, 0]
第二个测试样本 [0, 0, 0]

从输出结果上看，预测结果为：

- 第一个测试样本 [1, 1, 1] 被预测为类别 0
- 第二个测试样本 [0, 0, 0] 被预测为类别 0

这两个结果都完全正确。

#### 4.3.4 C4.5算法

C4.5算法是由Ross Quinlan在1993年提出的一种改进的决策树算法，它是ID3算法的扩展和改进版本。C4.5算法与ID3算法类似，也是一种用于分类的递归决策树生成算法。C4.5在选择特征进行数据分割时，使用了信息增益比（Gain Ratio）作为度量标准，而不是单纯的信息增益。

在C4.5算法中，信息增益比定义为：

$GainRatio(D,A)=\frac{Gain(D,A)}{IV(A)}$

其中，$Gain(D,A)$ 是特征𝐴对数据集𝐷的信息增益，$IV(A)$ 是特征𝐴的固有值（Intrinsic Value），定义为：

$IV(A)=-\sum_{v\in \text{Values(A)}} \frac{|D_v|}{|D|} \log_2 \frac{|D_v|}{|D|}$

我们用代码实现固有值：

```python
    def _intrinsic_value(self, data, feature):
        # 使用 NumPy 的 unique 函数找出指定特征列中的所有不同值以及它们出现的次数
        values, counts = np.unique(data[:, feature], return_counts=True)
        # 计算每个不同特征值出现的概率。这是通过将每个特征值的出现次数除以数据集的总长度来实现的
        probabilities = counts / len(data)
        return -np.sum(probabilities * np.log2(probabilities))
```

然后我们计算信息增益比：

```python
    def _gain_ratio(self, data, labels, feature):
        info_gain = self._information_gain(data, labels, feature)
        intrinsic_value = self._intrinsic_value(data, feature)
        if intrinsic_value == 0:
            return 0
        return info_gain / intrinsic_value
```

将代码串起来：

```python
import numpy as np

class C45:
    def __init__(self):
        self.tree = {}

    def fit(self, data, labels):
        self.tree = self._build_tree(data, labels)
        return self.tree

    def _entropy(self, labels):
        _, counts = np.unique(labels, return_counts=True)
        probabilities = counts / len(labels)
        return -np.sum(probabilities * np.log2(probabilities))

    def _information_gain(self, data, labels, feature):
        original_entropy = self._entropy(labels)
        values, counts = np.unique(data[:, feature], return_counts=True)
        weighted_entropy = sum((counts[i] / np.sum(counts)) * self._entropy(labels[data[:, feature] == values[i]]) for i in range(len(values)))
        return original_entropy - weighted_entropy

    def _intrinsic_value(self, data, feature):
        values, counts = np.unique(data[:, feature], return_counts=True)
        probabilities = counts / len(data)
        return -np.sum(probabilities * np.log2(probabilities))

    def _gain_ratio(self, data, labels, feature):
        info_gain = self._information_gain(data, labels, feature)
        intrinsic_value = self._intrinsic_value(data, feature)
        if intrinsic_value == 0:
            return 0
        return info_gain / intrinsic_value

    def _best_feature(self, data, labels):
        n_features = data.shape[1]
        gain_ratios = [self._gain_ratio(data, labels, feature) for feature in range(n_features)]
        return np.argmax(gain_ratios)

    def _build_tree(self, data, labels):
        if len(np.unique(labels)) == 1:
            return labels[0]
        if data.shape[1] == 0:
            return np.bincount(labels).argmax()
        
        best_feature = self._best_feature(data, labels)
        tree = {best_feature: {}}
        unique_values = np.unique(data[:, best_feature])

        for value in unique_values:
            subset_data = data[data[:, best_feature] == value]
            subset_labels = labels[data[:, best_feature] == value]
            subtree = self._build_tree(np.delete(subset_data, best_feature, axis=1), subset_labels)
            tree[best_feature][value] = subtree

        return tree

    def predict(self, data):
        return np.array([self._predict_single(sample, self.tree) for sample in data])

    def _predict_single(self, sample, tree):
        if not isinstance(tree, dict):
            return tree
        feature = next(iter(tree))
        feature_value = sample[feature]
        if feature_value in tree[feature]:
            return self._predict_single(sample, tree[feature][feature_value])
        else:
            return np.bincount(list(tree[feature].values())).argmax()

# 示例数据集
data = np.array([
    [1, 1, 0],
    [1, 0, 0],
    [0, 1, 1],
    [0, 1, 1],
    [0, 0, 0]
])
labels = np.array([0, 0, 1, 1, 0])

# 创建C4.5实例并训练
c45 = C45()
tree = c45.fit(data, labels)
print("Decision Tree:", tree)

# 预测
test_data = np.array([
    [0, 1, 1],
    [0, 0, 0]
])
predictions = c45.predict(test_data)
print("Predictions:", predictions)
```

输出如下：
```
Decision Tree: {2: {0: 0, 1: 1}}
Predictions: [1 0]
```

可见结果也是正确的。

#### 4.3.5 剪枝

剪枝（Pruning）是决策树算法中防止过拟合的一种重要技术。过拟合是指模型在训练数据上表现很好，但在新数据上表现较差。剪枝通过减少决策树的复杂度来提高模型的泛化能力。决策树剪枝主要有两种方法：预剪枝（Pre-pruning）和后剪枝（Post-pruning）。

预剪枝是在构建决策树的过程中，通过设置一些条件提前停止树的生长，避免生成过于复杂的树。这些条件通常包括：

| 参数名称   | 描述                                                 |
| ---------- | ---------------------------------------------------- |
| 最大深度   | 设置树的最大深度，超过这个深度的节点不再分裂。       |
| 最小样本数 | 设置节点分裂所需的最小样本数，样本数小于这个值的节点不再分裂。 |
| 最小增益   | 设置分裂所需的最小信息增益或基尼系数增益，增益小于这个值时不再分裂。 |

下面我们就在CART算法中加入预剪枝的三个参数：

```python
class CART:
    def __init__(self, max_depth=None, min_samples_split=2, min_gain=0):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_gain = min_gain
        self.tree = {}
```

预剪枝并不需要专门的剪枝函数，而是在构建决策树的过程中，通过设置这些参数来控制树的生长。我们在构建决策树的过程中，对每个节点都会检查这些参数，如果满足预剪枝条件，则停止分裂。例如，我们在构建树的过程中，对每个节点都会检查是否达到最大深度，如果达到最大深度，则停止分裂。

```python
    def _build_tree(self, data, labels, depth):
        if len(np.unique(labels)) == 1:
            return labels[0]
        if self.max_depth is not None and depth >= self.max_depth:
            return np.bincount(labels).argmax()
        if len(labels) < self.min_samples_split:
            return np.bincount(labels).argmax()
```

我们来看下完整代码：

```python
import numpy as np

class CART:
    def __init__(self, max_depth=None, min_samples_split=2, min_gain=0):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_gain = min_gain
        self.tree = {}

    def fit(self, data, labels):
        self.tree = self._build_tree(data, labels, depth=0)
        return self.tree

    def _gini(self, labels):
        _, counts = np.unique(labels, return_counts=True)
        probabilities = counts / len(labels)
        return 1 - np.sum(probabilities ** 2)

    def _best_split(self, data, labels):
        best_gini = float('inf')
        best_split = None
        n_features = data.shape[1]

        for feature in range(n_features):
            thresholds = np.unique(data[:, feature])
            for threshold in thresholds:
                left_mask = data[:, feature] <= threshold
                right_mask = data[:, feature] > threshold
                left_labels = labels[left_mask]
                right_labels = labels[right_mask]

                if len(left_labels) == 0 or len(right_labels) == 0:
                    continue

                weighted_gini = (len(left_labels) * self._gini(left_labels) + len(right_labels) * self._gini(right_labels)) / len(labels)

                if weighted_gini < best_gini:
                    best_gini = weighted_gini
                    best_split = (feature, threshold)

        return best_split

    def _build_tree(self, data, labels, depth):
        if len(np.unique(labels)) == 1:
            return labels[0]
        if self.max_depth is not None and depth >= self.max_depth:
            return np.bincount(labels).argmax()
        if len(labels) < self.min_samples_split:
            return np.bincount(labels).argmax()

        best_split = self._best_split(data, labels)
        if best_split is None:
            return np.bincount(labels).argmax()

        feature, threshold = best_split
        left_mask = data[:, feature] <= threshold
        right_mask = data[:, feature] > threshold
        left_data, right_data = data[left_mask], data[right_mask]
        left_labels, right_labels = labels[left_mask], labels[right_mask]

        if self._gini(labels) - (len(left_labels) * self._gini(left_labels) + len(right_labels) * self._gini(right_labels)) / len(labels) < self.min_gain:
            return np.bincount(labels).argmax()

        left_branch = self._build_tree(left_data, left_labels, depth + 1)
        right_branch = self._build_tree(right_data, right_labels, depth + 1)
        return {feature: {'threshold': threshold, 'left': left_branch, 'right': right_branch}}

    def predict(self, data):
        return np.array([self._predict_single(sample, self.tree) for sample in data])

    def _predict_single(self, sample, tree):
        if not isinstance(tree, dict):
            return tree
        feature = list(tree.keys())[0]
        if sample[feature] <= tree[feature]['threshold']:
            return self._predict_single(sample, tree[feature]['left'])
        else:
            return self._predict_single(sample, tree[feature]['right'])

# 示例数据集
data = np.array([
    [1, 1, 0],
    [1, 0, 0],
    [0, 1, 1],
    [0, 1, 1],
    [0, 0, 0]
])
labels = np.array([0, 0, 1, 1, 0])

# 创建CART实例并训练
cart = CART(max_depth=2, min_samples_split=2, min_gain=0.01)
tree = cart.fit(data, labels)
print("Decision Tree:", tree)

# 预测
test_data = np.array([
    [1, 1, 1],
    [0, 0, 0]
])
predictions = cart.predict(test_data)
print("Predictions:", predictions)
```

后剪枝是在构建完整的决策树之后，再对树进行简化。常见的后剪枝策略有：

- 基于验证集的剪枝：将数据集分为训练集和验证集，首先用训练集构建完整的决策树，然后在验证集上进行剪枝，去掉那些对验证集性能贡献不大的节点。
- 代价复杂度剪枝（Cost Complexity Pruning）：通过引入一个惩罚项来控制树的复杂度。具体做法是：
    - 对每个非叶子节点计算剪枝后的代价复杂度。
    - 从最小的代价复杂度开始剪枝，直到树的性能不再提升。

| 剪枝方法  | 描述            |
| ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| 基于验证集的剪枝              | 将数据集分为训练集和验证集，首先用训练集构建完整的决策树，然后在验证集上进行剪枝，去掉那些对验证集性能贡献不大的节点。                       |
| 代价复杂度剪枝（Cost Complexity Pruning） | 通过引入一个惩罚项来控制树的复杂度。具体做法是：<br>1. 对每个非叶子节点计算剪枝后的代价复杂度。<br>2. 从最小的代价复杂度开始剪枝，直到树的性能不再提升。 |

下面我们实现一个基于验证集的剪枝。这个方法通过递归的方式，不断地评估决策树的每个非叶节点，决定是否将其剪枝为叶节点，以此来优化决策树的性能。

```python
    # tree 是待剪枝的决策树，validation_data 是验证数据集（用于评估剪枝效果），validation_labels 是对应的验证标签集
    def _prune_tree(self, tree, validation_data, validation_labels):
        # 检查当前的树节点是否是字典类型，如果不是，说明已经到达叶节点，直接返回节点的值
        if not isinstance(tree, dict):
            return tree

        feature = list(tree.keys())[0]
        left_tree = tree[feature]['left']
        right_tree = tree[feature]['right']

        left_mask = validation_data[:, feature] <= tree[feature]['threshold']
        right_mask = validation_data[:, feature] > tree[feature]['threshold']
        # 根据当前节点的阈值，生成左右子树的验证数据掩码
        left_data, right_data = validation_data[left_mask], validation_data[right_mask]
        left_labels, right_labels = validation_labels[left_mask], validation_labels[right_mask]

        # 如果左子树或右子树的验证数据为空，说明没有足够的样本进行剪枝评估，此时返回验证标签中出现次数最多的标签作为叶节点的预测结果
        if left_data.size == 0 or right_data.size == 0:
            return np.bincount(validation_labels).argmax()

        # 递归地对左右子树进行剪枝操作
        tree[feature]['left'] = self._prune_tree(left_tree, left_data, left_labels)
        tree[feature]['right'] = self._prune_tree(right_tree, right_data, right_labels)

        # 如果左右子树都已经剪枝为叶节点，进行下一步的剪枝评估
        if not isinstance(tree[feature]['left'], dict) and not isinstance(tree[feature]['right'], dict):
            # 获取左右子树的预测结果
            left_pred = tree[feature]['left']
            right_pred = tree[feature]['right']
            # 计算左右子树的预测错误率
            left_error = np.sum(left_labels != left_pred)
            right_error = np.sum(right_labels != right_pred)
            # 计算合并后的错误率
            combined_error = left_error + right_error

            # 获取将当前节点剪枝为叶节点时的预测结果
            leaf_pred = np.bincount(validation_labels).argmax()
            # 计算剪枝为叶节点时的错误率
            leaf_error = np.sum(validation_labels != leaf_pred)

            # 如果剪枝为叶节点的错误率不大于合并后的错误率，说明剪枝可以提高模型性能，此时返回叶节点的预测结果，完成剪枝
            if leaf_error <= combined_error:
                return leaf_pred

        return tree
```

最后，我们把前后剪枝的代码串起来：

```python
import numpy as np

class CART:
    def __init__(self, max_depth=None, min_samples_split=2, min_gain=0):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_gain = min_gain
        self.tree = {}

    def fit(self, data, labels, validation_data=None, validation_labels=None):
        self.tree = self._build_tree(data, labels, depth=0)
        if validation_data is not None and validation_labels is not None:
            self._prune_tree(self.tree, validation_data, validation_labels)
        return self.tree

    def _gini(self, labels):
        _, counts = np.unique(labels, return_counts=True)
        probabilities = counts / len(labels)
        return 1 - np.sum(probabilities ** 2)

    def _best_split(self, data, labels):
        best_gini = float('inf')
        best_split = None
        n_features = data.shape[1]

        for feature in range(n_features):
            thresholds = np.unique(data[:, feature])
            for threshold in thresholds:
                left_mask = data[:, feature] <= threshold
                right_mask = data[:, feature] > threshold
                left_labels = labels[left_mask]
                right_labels = labels[right_mask]

                if len(left_labels) == 0 or len(right_labels) == 0:
                    continue

                weighted_gini = (len(left_labels) * self._gini(left_labels) + len(right_labels) * self._gini(right_labels)) / len(labels)

                if weighted_gini < best_gini:
                    best_gini = weighted_gini
                    best_split = (feature, threshold)

        return best_split

    def _build_tree(self, data, labels, depth):
        if len(np.unique(labels)) == 1:
            return labels[0]
        if self.max_depth is not None and depth >= self.max_depth:
            return np.bincount(labels).argmax()
        if len(labels) < self.min_samples_split:
            return np.bincount(labels).argmax()

        best_split = self._best_split(data, labels)
        if best_split is None:
            return np.bincount(labels).argmax()

        feature, threshold = best_split
        left_mask = data[:, feature] <= threshold
        right_mask = data[:, feature] > threshold
        left_data, right_data = data[left_mask], data[right_mask]
        left_labels, right_labels = labels[left_mask], labels[right_mask]

        if self._gini(labels) - (len(left_labels) * self._gini(left_labels) + len(right_labels) * self._gini(right_labels)) / len(labels) < self.min_gain:
            return np.bincount(labels).argmax()

        left_branch = self._build_tree(left_data, left_labels, depth + 1)
        right_branch = self._build_tree(right_data, right_labels, depth + 1)
        return {feature: {'threshold': threshold, 'left': left_branch, 'right': right_branch}}

    def _prune_tree(self, tree, validation_data, validation_labels):
        if not isinstance(tree, dict):
            return tree

        feature = list(tree.keys())[0]
        left_tree = tree[feature]['left']
        right_tree = tree[feature]['right']

        left_mask = validation_data[:, feature] <= tree[feature]['threshold']
        right_mask = validation_data[:, feature] > tree[feature]['threshold']
        left_data, right_data = validation_data[left_mask], validation_data[right_mask]
        left_labels, right_labels = validation_labels[left_mask], validation_labels[right_mask]

        if left_data.size == 0 or right_data.size == 0:
            return np.bincount(validation_labels).argmax()

        tree[feature]['left'] = self._prune_tree(left_tree, left_data, left_labels)
        tree[feature]['right'] = self._prune_tree(right_tree, right_data, right_labels)

        if not isinstance(tree[feature]['left'], dict) and not isinstance(tree[feature]['right'], dict):
            left_pred = tree[feature]['left']
            right_pred = tree[feature]['right']
            left_error = np.sum(left_labels != left_pred)
            right_error = np.sum(right_labels != right_pred)
            combined_error = left_error + right_error

            leaf_pred = np.bincount(validation_labels).argmax()
            leaf_error = np.sum(validation_labels != leaf_pred)

            if leaf_error <= combined_error:
                return leaf_pred

        return tree

    def predict(self, data):
        return np.array([self._predict_single(sample, self.tree) for sample in data])

    def _predict_single(self, sample, tree):
        if not isinstance(tree, dict):
            return tree
        feature = list(tree.keys())[0]
        if sample[feature] <= tree[feature]['threshold']:
            return self._predict_single(sample, tree[feature]['left'])
        else:
            return self._predict_single(sample, tree[feature]['right'])

# 示例数据集
data = np.array([
    [1, 1, 0],
    [1, 0, 0],
    [0, 1, 1],
    [0, 1, 1],
    [0, 0, 0]
])
labels = np.array([0, 0, 1, 1, 0])

# 创建验证集
validation_data = np.array([
    [1, 1, 1],
    [0, 0, 1],
    [1, 0, 1]
])
validation_labels = np.array([0, 1, 0])

# 创建CART实例并训练
cart = CART(max_depth=3, min_samples_split=2, min_gain=0.01)
tree = cart.fit(data, labels, validation_data, validation_labels)
print("Decision Tree:", tree)

# 预测
test_data = np.array([
    [1, 1, 1],
    [0, 0, 0]
])
predictions = cart.predict(test_data)
print("Predictions:", predictions)
```

在测试数据上没问题，那我们换成鸢尾花数据集试试：

```python
# 加载鸢尾花数据集
iris = load_iris()
data, labels = iris.data, iris.target

# 将数据集分为训练集和验证集
train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)

# 初始化和训练决策树模型
model = CART(max_depth=5, min_samples_split=2, min_gain=0.01)
tree = model.fit(train_data, train_labels, val_data, val_labels)

print("Decision Tree:", tree)

# 预测并评估模型
predictions = model.predict(val_data)
accuracy = np.mean(predictions == val_labels)
print(f"Validation Accuracy: {accuracy:.2f}")
```

准确率是100%，说明我们做得还不错。

下面我们再挑战一下糖尿病数据集，看看我们的决策树模型的泛化能力：

```python
# 加载糖尿病数据集
diabetes = load_diabetes()
data, labels = diabetes.data, diabetes.target

# 将目标值二值化，以便进行分类任务
labels = (labels > np.median(labels)).astype(int)

# 将数据集分为训练集和验证集
train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)

# 初始化和训练决策树模型
model = CART(max_depth=5, min_samples_split=2, min_gain=0.01)
tree=model.fit(train_data, train_labels, val_data, val_labels)
print("Decision Tree:", tree)


# 预测并评估模型
predictions = model.predict(val_data)
accuracy = np.mean(predictions == val_labels)
print(f"Validation Accuracy: {accuracy:.2f}")
```

运行结果如下：

```
Decision Tree: {2: {'threshold': 0.008883414898524095, 'left': {8: {'threshold': -0.00422151393810765, 'left': 0, 'right': 1}}, 'right': {3: {'threshold': -0.019441826196154435, 'left': 0, 'right': {0: {'threshold': 0.027178291080364757, 'left': {8: {'threshold': 0.06345271983825305, 'left': {5: {'threshold': 0.09169121572527314, 'left': 1, 'right': 0}}, 'right': 1}}, 'right': 1}}}}}}
Validation Accuracy: 0.79
```

准确率是79%，说明我们的决策树模型在糖尿病数据集上的泛化能力还不错。

我们来看下ROC曲线：

```python
# 计算并绘制ROC曲线
fpr, tpr, thresholds = roc_curve(val_labels, predictions)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()
```

画出图下如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/roc_prune.png)

而且从树结构上看，我们把上节密密麻麻的决策树剪成了一棵小树，而且准确率还有所提升。

### 4.4 支持向量机

在深度学习流行之前，支持向量机曾经是最有前途的机器学习方向。支持向量机有良好的理论基础，可以解决线性和非线性分类问题，也可以用于回归问题。

#### 4.4.1 支持向量机的基本原理

我们先介绍三个基本概念：

1. 超平面:
在二维空间中，超平面就是一条直线；在三维空间中，超平面是一平面；而在更高维空间中，它是一个 d−1 维的子空间。对于一个给定的训练数据集，SVM 试图找到一个能够将不同类别分开的最优超平面。
2. 间隔:间隔（Margin）是指从超平面到最近的训练样本（支持向量）的最短距离。SVM 的目标是最大化这个间隔，从而提高分类器的泛化能力。
3. 支持向量: 支持向量是指那些位于间隔边界上的训练样本。它们是决定最优超平面位置的关键点。

基本的SVM模型是一个线性分类器，其目的是找到一个超平面，将不同类别的数据点分开。在线性可分的情况下，SVM可以通过以下优化问题来找到最优超平面：

$$\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2$$

$$ \text{subject to } y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1, \quad \forall i $$

其中，$\mathbf{w}$ 是超平面的法向量，$b$ 是偏置项，$\mathbf{x}_i$ 是训练样本，$y_i$是对应的标签。

在实际应用中，许多数据集并不是线性可分的。例如，数据点可能分布在一个复杂的非线性边界上。这时，线性SVM无法有效地分类这些数据。

这时候我们引入一个新概念：核技巧。

核技巧的核心思想是将原始数据映射到一个高维特征空间，使得在这个高维空间中数据变得线性可分。具体来说，核函数$k(\mathbf{x}, \mathbf{z})$定义为在隐式映射 $\phi(\cdot)$ 下的内积：

$$ k(\mathbf{x}, \mathbf{z}) = \langle \phi(\mathbf{x}), \phi(\mathbf{z}) \rangle $$

通过使用核函数，我们可以在不显式计算高维映射 $\phi(\cdot)$ 的情况下，直接在原始空间中计算内积。这极大地简化了计算复杂度。

不熟悉内积概念的同学，我们来复习一下。内积是向量空间中的一种运算，它将两个向量映射到一个标量。具体地，对于一个向量空间 $V$ 中的任意两个向量 $\mathbf{u}$ 和 $\mathbf{v}$，其内积记为 $\langle \mathbf{u}, \mathbf{v} \rangle$。

在欧几里得空间 $\mathbb{R}^n$ 中，内积通常定义为两个向量的点积（也称标量积）。例如，对于向量 $\mathbf{u} = (u_1, u_2, \ldots, u_n)$和 $\mathbf{v} = (v_1, v_2, \ldots, v_n)$，其内积定义为：

$$ \langle \mathbf{u}, \mathbf{v} \rangle = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n $$

将核函数引入支持向量机的优化问题，可以得到核化支持向量机。其优化问题变为：

$$ \min_{\mathbf{\alpha}} \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j) - \sum_i \alpha_i $$

$$ \text{subject to } 0 \leq \alpha_i \leq C, \quad \sum_i \alpha_i y_i = 0 $$

其中，$\alpha_i$ 是拉格朗日乘子，$C$ 是正则化参数。决策函数为：

$$ f(\mathbf{x}) = \sum_i \alpha_i y_i k(\mathbf{x}_i, \mathbf{x}) + b $$

常见的核函数包括


| 核函数类型               | 数学表达式                                            |
|--------------------------|----------------------------------------------------|
| 线性核函数               | $K(x_i, x_j) = x_i \cdot x_j$                       |
| 多项式核函数             | $K(x_i, x_j) = (\gamma x_i \cdot x_j + r)^d$        |
| 高斯径向基核函数（RBF）  | $K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$       |
| Sigmoid核函数            | $K(x_i, x_j) = \tanh(\gamma x_i \cdot x_j + r)$     |

SVM 的优缺点


| 优点             | 缺点                                                   |
|------------------|--------------------------------------------------------|
| 高效：在高维空间中仍然有效。         | 计算复杂度高：对大规模数据集的训练时间较长。               |
| 鲁棒性：在样本数量远小于特征数量时仍然表现良好。 | 参数选择困难：核函数和正则化参数的选择对模型性能影响较大。 |
| 灵活性：通过自定义核函数，可以适应不同的分类任务。 | 对噪声敏感：对噪声数据和重叠的类不太鲁棒。                |

在支持向量机（SVM）中，核函数（Kernel Function）允许我们在高维特征空间中进行计算，而无需显式地转换数据。为了证明某个核函数是有效的，我们需要证明它是一个正定核（Positive Definite Kernel）。

Mercer 定理为我们提供了一个判断核函数是否有效的标准。Mercer 定理指出：如果函数 $K(\mathbf{x}, \mathbf{y})$ 是对称的，并且对于任意的非零函数 $g(\mathbf{x})$，满足以下条件：

$$ \int \int g(\mathbf{x}) K(\mathbf{x}, \mathbf{y}) g(\mathbf{y}) \, d\mathbf{x} \, d\mathbf{y} \geq 0 $$

则 $K(\mathbf{x}, \mathbf{y})$ 是一个有效的核函数。换句话说，核函数 $K(\mathbf{x}, \mathbf{y})$ 必须生成一个对称的、正定的格拉姆矩阵。

我们再补充介绍下什么是格拉姆矩阵。格拉姆矩阵是由一组向量两两之间的内积所构成的矩阵。

格拉姆矩阵定义：给定一个向量集合 {x₁, x₂, ..., xₙ}，其中每个 xᵢ 都是一个 d 维向量，则该向量集合的 Gram 矩阵是一个 n × n 的对称矩阵 K，其元素 Kᵢⱼ 定义为：Kᵢⱼ = <xᵢ, xⱼ>

其中 <·, ·> 表示向量之间的内积。

例子：

假设我们有三个二维向量：

```
x₁ = [1, 2]
x₂ = [3, 0]
x₃ = [-1, 1]
```

则它们的格拉姆矩阵为：

```
K = 
| <x₁, x₁>  <x₁, x₂>  <x₁, x₃> |
| <x₂, x₁>  <x₂, x₂>  <x₂, x₃> |
| <x₃, x₁>  <x₃, x₂>  <x₃, x₃> |
  =
|  5   3   1 |
|  3   9  -3 |
|  1  -3   2 |
```

格拉姆矩阵性质：

- 对称性: Gram 矩阵是对称的，即 Kᵢⱼ = Kⱼᵢ。
- 半正定性: Gram 矩阵是半正定的，即对于任意非零向量 z，都有 zᵀKz ≥ 0。
- 维数: Gram 矩阵的维数取决于向量个数，而与向量的维数无关。

我们回到证明核函数存在性的步骤。需要证明对称性和正定性两步：

首先，核函数 $K(\mathbf{x}, \mathbf{y})$ 必须满足对称性条件，即 $K(\mathbf{x}, \mathbf{y}) = K(\mathbf{y}, \mathbf{x})$。

其次，对于任何有限集合的样本点 $\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\}$，对应的 Gram 矩阵 $K$ 必须是正定的。即，对于任意的非零向量 $\mathbf{c} = (c_1, c_2, \ldots, c_n)^\top$，满足以下条件：
$$ \sum_{i=1}^n \sum_{j=1}^n c_i c_j K(\mathbf{x}_i, \mathbf{x}_j) \geq 0 $$

下面我们分别看一下主要的核函数是不是可以满足 Mercer 定理的要求。

| 核函数类型 | 定义 | 对称性 | 正定性 |
|------------|------|--------|--------|
| 线性核     | $K(\mathbf{x}, \mathbf{y}) = \mathbf{x}^\top \mathbf{y}$ | 是 | 是 |
| 多项式核   | $K(\mathbf{x}, \mathbf{y}) = (\mathbf{x}^\top \mathbf{y} + c)^d$ | 是 | 是 |
| 高斯核（RBF核） | $K(\mathbf{x}, \mathbf{y}) = \exp\left(-\frac{\|\mathbf{x} - \mathbf{y}\|^2}{2\sigma^2}\right)$ | 是 | 是 |

最后我们讲一个新概念，再生核希尔伯特空间（Reproducing Kernel Hilbert Space, RKHS）。

再生核希尔伯特空间，如其名，首先是一个希尔伯特空间。希尔伯特空间是一个完备的内积空间。内积空间又是一个定义了内积运算的向量空间。

我们得从向量空间说起了。

一个向量空间（或线性空间）是一个集合$V$，它满足以下两个条件：

1. 向量加法：在集合$V$中定义了一个二元运算$+$，称为向量加法。对于任意的$\mathbf{u}, \mathbf{v} \in V$，$\mathbf{u} + \mathbf{v} \in V$。
2. 标量乘法：在集合$V$中定义了一个二元运算$\cdot$，称为标量乘法。对于任意的$\mathbf{v} \in V$和标量$a$（标量来自一个数域，如实数域 $\mathbb{R}$ 或复数域$\mathbb{C}$），$a \cdot \mathbf{v} \in V$。

向量加法必须满足加法公理，也就是满足交换律、结合律、零向量和加法逆元这四个条件。

1. 交换律：$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ 对于所有 $\mathbf{u}, \mathbf{v} \in V$。
2. 结合律：$(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$ 对于所有 $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$。
3. 零向量：存在一个零向量$\mathbf{0} \in V$，使得$\mathbf{v} + \mathbf{0} = \mathbf{v}$对于所有$\mathbf{v} \in V$。
4. 加法逆元：对于每一个$\mathbf{v} \in V$，存在一个向量$-\mathbf{v} \in V$，使得$\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$。

同样，标量乘法公理要满足以下四个条件。

1. 分配律（标量对向量加法）： $a \cdot (\mathbf{u} + \mathbf{v}) = a \cdot \mathbf{u} + a \cdot \mathbf{v}$ 对于所有 $a \in \mathbb{F}$ 和所有 $\mathbf{u}, \mathbf{v} \in V$。
2. 分配律（标量加法对向量）： $(a + b) \cdot \mathbf{v} = a \cdot \mathbf{v} + b \cdot \mathbf{v}$ 对于所有 $a, b \in \mathbb{F}$ 和所有 $\mathbf{v} \in V$。
3. 结合律（标量乘法）： $a \cdot (b \cdot \mathbf{v}) = (a \cdot b) \cdot \mathbf{v}$ 对于所有 $a, b \in \mathbb{F}$ 和所有 $\mathbf{v} \in V$。
4. 单位元：$1 \cdot \mathbf{v} = \mathbf{v}$ 对于所有 $\mathbf{v} \in V$，这里的 $1$ 是标量乘法的单位元。


我们给向量空间再增加一个内积运算，就得到了内积空间。

内积运算$\langle \cdot, \cdot \rangle$将两个向量映射到一个标量。具体来说，对于所有的$\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$和标量$a$，内积满足以下性质：

1. 共轭对称性：$\langle \mathbf{u}, \mathbf{v} \rangle = \overline{\langle \mathbf{v}, \mathbf{u} \rangle}$
2. 线性性：$\langle a\mathbf{u} + \mathbf{v}, \mathbf{w} \rangle = a \langle \mathbf{u}, \mathbf{w} \rangle + \langle \mathbf{v}, \mathbf{w} \rangle$
3. 正定性：$\langle \mathbf{u}, \mathbf{u} \rangle \geq 0$ 且 $\langle \mathbf{u}, \mathbf{u} \rangle = 0$ 当且仅当 $\mathbf{u} = 0$

共轭对称性是内积空间中内积运算的一个基本性质，它描述了内积运算在两个向量交换位置时所发生的变化。具体地说，在一个内积空间 $V$ 上，内积 $\langle \mathbf{u}, \mathbf{v} \rangle$ 对于任意的向量 $\mathbf{u}, \mathbf{v} \in V$，满足以下性质：

$$ \langle \mathbf{u}, \mathbf{v} \rangle = \overline{\langle \mathbf{v}, \mathbf{u} \rangle} $$

这里，$\overline{\langle \mathbf{v}, \mathbf{u} \rangle}$ 表示内积 $\langle \mathbf{v}, \mathbf{u} \rangle$ 的复共轭。如果内积是实数值的，那么复共轭运算就等同于取原值，这时共轭对称性就简化为：

$$ \langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle $$

为了理解完备性，我们需要知道什么是柯西序列。在一个度量空间中，如果对于任意的 $\epsilon > 0$，存在一个正整数 N，使得对于所有的 $m, n > N$，有：$\|\mathbf{u}_n - \mathbf{u}_m\| < \epsilon$，我们就称这个序列 $\{\mathbf{u}_n\}$ 为柯西序列，直观地说，柯西序列的元素在序列的后期会越来越接近彼此。

如果所有的柯西序列都收敛到空间中的某个点，我们就一个度量空间是**完备的**。

在内积空间中，我们使用内积诱导的范数来定义距离：$\|\mathbf{u}\| = \sqrt{\langle \mathbf{u}, \mathbf{u} \rangle}$

因此，如果对于任意的柯西序列 $\{\mathbf{u}_n\}$，存在某个 $\mathbf{u} \in V$，使得：$\lim_{n \to \infty} \|\mathbf{u}_n - \mathbf{u}\| = 0$, 我们就称这个内积空间是完备的。

我们再来说再生核。

再生核是一个函数 $k: X \times X \to \mathbb{R}$ 或 $k: X \times X \to \mathbb{C} $，满足以下性质：对于任意的$x \in X$，存在一个函数$k_x(\cdot) = k(x, \cdot)$属于某个希尔伯特空间$\mathcal{H}$，使得对于所有的$f \in \mathcal{H}$ 和 $x \in X$，都有
$f(x) = \langle{f, k_x} \rangle_{\mathcal{H}}$

其中$\langle \cdot, \cdot \rangle_{\mathcal{H}}$ 表示$\mathcal{H}$中的内积。

这意味着核函数$k$具有再生性质，即函数值$f(x)$可以通过内积来表示。

如果一个希尔伯特空间，其中的每一个函数$f$都可以通过一个再生核$k$来表示。具体地说，存在一个核函数$k$，使得对于任意的$x \in X$和$f \in \mathcal{H} $，都有$f(x) = \langle{f, k_x} \rangle_{\mathcal{H}}$并且$k_x \in \mathcal{H}$，那么，我们就称这个希尔伯特空间$\mathcal{H}$为再生核希尔伯特空间. 


令$\mathbb{H}$为核函数k对应的再生核希尔伯特空间，$\|h\|_\mathbb{H}$表示H空间中关于h的范数，对于任意单调递增函数$\Omega:[0,\infty]\mapsto \mathbb{R}$和任意非负损失函数$\mathscr{l}: \mathbb{R}^m\mapsto [0,\infty]$，优化问题

$\min_{h\in \mathbb{H}} F(h)= \Omega(\|h\|_{\mathbb{H}}) + \mathbb{l}(h(x_1),h(x_2),...,h(x_m))$ 的解总可写为：

$h^*(x) = \sum_{i=1}^m \alpha_i k(x_i,x)$

在支持向量机中，我们通常会遇到高维特征空间，而直接在这样高维的空间中计算是非常困难的。表示定理告诉我们，我们可以通过核函数来隐式地在这个高维空间中计算，而无需显式地映射到那个空间。这意味着我们可以通过核函数来计算两个向量在高维空间中的内积，而无需知道它们在高维空间中的确切表示。

具体来说，对于任意的单调增函数和非负损失函数，优化问题的解总可以写为核函数的线性组合。这里的核函数是一个衡量两个向量相似度的函数，它在高维空间中对应着向量的内积。因此，通过选择合适的核函数，我们可以有效地在高维空间中找到最优的分类超平面，而无需直接处理高维空间的复杂性。

#### 4.4.2 调用SVC进行分类

我们先用线性核函数来实现鸢尾花的分类：

```python
# 导入必要的库
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_iris

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 将数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化支持向量机分类器
svm_clf = SVC(kernel='linear')

# 训练模型
svm_clf.fit(X_train, y_train)

# 使用模型进行预测
y_pred = svm_clf.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f'准确率: {accuracy:.2f}')

# 打印分类报告
print("分类报告:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

运行结果如下：
```
准确率: 1.00
分类报告:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      1.00      1.00        13
   virginica       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45
```

我们可以看到，使用线性核函数的支持向量机在鸢尾花数据集上取得了100%的准确率。


下面我们再来看使用多项式核函数如何来实现鸢尾花的分类：

```python
# 导入必要的库
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 将数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化支持向量机分类器，使用多项式核函数
svm_clf = SVC(kernel='poly', degree=3, gamma='scale', coef0=1)

# 训练模型
svm_clf.fit(X_train, y_train)

# 使用模型进行预测
y_pred = svm_clf.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# 打印分类报告
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# 打印混淆矩阵
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# 使用PCA将数据降至二维以便可视化
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# 再次划分为训练集和测试集
X_train_reduced, X_test_reduced = train_test_split(X_reduced, test_size=0.3, random_state=42)

# 训练SVM模型
svm_clf.fit(X_train_reduced, y_train)

# 绘制决策边界
def plot_decision_boundaries(X, y, model_class, **model_params):
    h = .02  # 网格步长
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    model = model_class(**model_params)
    model.fit(X, y)
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('SVM with Polynomial Kernel (degree=3)')
    plt.show()

# 绘制鸢尾花数据集的决策边界
plot_decision_boundaries(X_train_reduced, y_train, SVC, kernel='poly', degree=3, gamma='scale', coef0=1)
```

运行结果如下：
```
Accuracy: 0.98
Classification Report:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      0.92      0.96        13
   virginica       0.93      1.00      0.96        13

    accuracy                           0.98        45
   macro avg       0.98      0.97      0.97        45
weighted avg       0.98      0.98      0.98        45

Confusion Matrix:
[[19  0  0]
 [ 0 12  1]
 [ 0  0 13]]
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/iris_svm_poly.png)

用多项式核函数的支持向量机在鸢尾花数据集上取得了98%的准确率，比线性的要弱一点。

我们再来看使用高斯径向基核函数如何来实现鸢尾花的分类：

```python
# 导入必要的库
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 将数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化支持向量机分类器，使用高斯径向基核函数
svm_clf = SVC(kernel='rbf', gamma='scale')

# 训练模型
svm_clf.fit(X_train, y_train)

# 使用模型进行预测
y_pred = svm_clf.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# 打印分类报告
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# 打印混淆矩阵
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# 使用PCA将数据降至二维以便可视化
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# 再次划分为训练集和测试集
X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(X_reduced, y, test_size=0.3, random_state=42)

# 训练SVM模型
svm_clf.fit(X_train_reduced, y_train_reduced)

# 绘制决策边界
def plot_decision_boundaries(X, y, model_class, **model_params):
    h = .02  # 网格步长
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    model = model_class(**model_params)
    model.fit(X, y)
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('SVM with RBF Kernel')
    plt.show()

# 绘制鸢尾花数据集的决策边界
plot_decision_boundaries(X_train_reduced, y_train_reduced, SVC, kernel='rbf', gamma='scale')
```

运行结果如下：
```
Accuracy: 1.00
Classification Report:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      1.00      1.00        13
   virginica       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45

Confusion Matrix:
[[19  0  0]
 [ 0 13  0]
 [ 0  0 13]]
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/iris_svm_rbf.png)

可以看到，使用高斯径向基核函数的支持向量机在鸢尾花数据集上取得了100%的准确率。

最后，我们再看用sigmoid核函数如何来实现鸢尾花的分类：

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 将数据集分割为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 使用 SVM 和 sigmoid 核函数进行分类
clf = SVC(kernel='sigmoid')
clf.fit(X_train, y_train)

# 预测测试集
y_pred = clf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# 输出分类报告
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

输出结果如下：

```
Accuracy: 0.89
Classification Report:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       0.90      0.69      0.78        13
   virginica       0.75      0.92      0.83        13

    accuracy                           0.89        45
   macro avg       0.88      0.87      0.87        45
weighted avg       0.90      0.89      0.89        45
```

可以看到，使用sigmoid核函数的支持向量机在鸢尾花数据集上取得了89%的准确率。选择不同的核函数对于结果是有影响的。

我们再来看一个用SVC处理威斯康星乳腺癌数据集的例子：

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# 加载威斯康星乳腺癌数据集
cancer = datasets.load_breast_cancer()
X = cancer.data
y = cancer.target

# 将数据集分割为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 使用 SVM 和核函数进行分类
#clf = SVC(kernel='linear')
#clf = SVC(kernel='poly', degree=3, gamma='scale', coef0=1)
#clf = SVC(kernel='rbf', gamma='scale')
clf = SVC(kernel='sigmoid')
clf.fit(X_train, y_train)

# 预测测试集
y_pred = clf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# 输出分类报告
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=cancer.target_names))
```

请大家自己手动修改核函数，看看不同核函数的效果。

运行的结果如下：

| 核函数 | 准确率 |
|--------|--------|
| 线性核 | 0.98 |
| 多项式核 | 0.99 |
| 高斯径向基核 | 0.98 |
| sigmoid核 | 0.96 |

可以看到，使用多项式核函数的支持向量机在威斯康星乳腺癌数据集上取得了99%的准确率，效果最好。

按照惯例，我们再比较下不同核函数处理糖尿病数据集的效果：

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# 加载糖尿病数据集
diabetes = datasets.load_diabetes()
X = diabetes.data
y = (diabetes.target > 140).astype(int)  # 将连续目标变量二值化

# 将数据集分割为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 使用 SVM 和线性核函数进行分类
clf = SVC(kernel='linear')
#clf = SVC(kernel='poly', degree=3, gamma='scale', coef0=1)
#clf = SVC(kernel='rbf', gamma='scale')
#clf = SVC(kernel='sigmoid')

clf.fit(X_train, y_train)

# 预测测试集
y_pred = clf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# 输出分类报告
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=["Non-diabetic", "Diabetic"]))
```

我们来看下4种核函数的准确率：

| 核函数 | 准确率 |
|--------|--------|
| 线性核 | 0.77 |
| 多项式核 | 0.73 |
| 高斯径向基核 | 0.76 |
| sigmoid核 | 0.71 |

可以看到，使用线性核函数的支持向量机在糖尿病数据集上取得了77%的准确率，效果最好。

下面再总结下到目前为止我们用的各种算法在不同数据集上的准确率：

|准确率	|乳腺癌	|糖尿病|
|---|---|---|
|逻辑回归	|0.96	|0.73|
|随机梯度下降	|0.98	|0.74|
|决策树	|0.95	|0.62| 
| 支持向量机线性核 | 0.98 | 0.77 |
| 支持向量机多项式核 | 0.99 | 0.73|
| 支持向量机高斯径向基核 | 0.98 | 0.76|
| 支持向量机sigmoid核 | 0.96 | 0.71|

#### 4.4.3 用JAX实现支持向量机

虽然前面我们用了不少公式介绍支持向量机。但是Sklearn库封装的太好了，有一个重要细节仍然没有讲到。那就是支持向量机的损失函数。

Hinge loss(合页损失)是用于训练支持向量机（SVM）的损失函数。它在二分类问题中特别常用，用于最大化分类边界的间隔。Hinge loss 的公式如下：
$\text{Hinge loss} = \sum_{i=1}^{n} \max(0, 1 - y_i \cdot f(x_i))$

其中：

- $n$ 是样本数量。
- $y_i$ 是样本 $i$ 的真实标签，取值为 $\pm1$。
- $f(x_i)$ 是样本$i$经过模型预测的值。

对于每个样本$i$：

   - 如果$y_i \cdot f(x_i) \geq 1$，那么该样本的损失为 0，表示该样本已经被正确分类并且位于边界的外侧。
   - 如果$y_i \cdot f(x_i) < 1$，那么该样本的损失为$1 - y_i \cdot f(x_i)$，表示该样本被错误分类或位于边界内侧。

总的 Hinge loss 是所有样本的损失之和。

在支持向量机中，目标是找到一个超平面将数据分开，同时最大化边界间隔。Hinge loss 通过惩罚那些离决策边界太近或被错误分类的点，来实现这一目标。

假设有一个简单的二分类问题，标签$y$取值为$\{-1, 1\}$，对于一个样本$x$，如果模型预测$f(x)$ 为 0.5 而真实标签$y$为 1，那么 Hinge loss 为：

$$ \text{Hinge loss} = \max(0, 1 - 1 \cdot 0.5) = \max(0, 0.5) = 0.5 $$

如果 $f(x)$ 为 1.5 而真实标签 $y$ 为 1，那么 Hinge loss 为：

$$ \text{Hinge loss} = \max(0, 1 - 1 \cdot 1.5) = \max(0, -0.5) = 0 $$

这说明样本已经被正确分类并且远离决策边界，因此不需要额外的损失。

下面我们用代码实现：

```python
# Hinge loss
def hinge_loss(weights, X, y):
    margins = y * jnp.dot(X, weights)
    return jnp.mean(jnp.maximum(0, 1 - margins))

# 定义损失函数
def loss_fn(weights, X, y, C=1.0):
    # 该损失值由两部分组成：一部分是权重的L2范数的平方的一半（即正则化项），另一部分是乘以正则化系数C的合页损失
    return 0.5 * jnp.dot(weights[:-1], weights[:-1]) + C * hinge_loss(weights, X, y)
```

我们写一个简单的用线性核函数的支持向量机：

```python
import jax.numpy as jnp
from jax import grad, jit
from jax import random
import jax
import optax
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# 加载数据集
cancer = datasets.load_breast_cancer()
X = cancer.data
y = cancer.target * 2 - 1  # 将标签转换为 -1 和 1

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 增加偏置项
X_train = jnp.hstack([X_train, jnp.ones((X_train.shape[0], 1))])
X_test = jnp.hstack([X_test, jnp.ones((X_test.shape[0], 1))])

# 初始化参数
key = random.PRNGKey(42)
weights = random.normal(key, (X_train.shape[1],))

# Hinge loss
def hinge_loss(weights, X, y):
    margins = y * jnp.dot(X, weights)
    return jnp.mean(jnp.maximum(0, 1 - margins))

# 定义损失函数
def loss_fn(weights, X, y, C=1.0):
    return 0.5 * jnp.dot(weights[:-1], weights[:-1]) + C * hinge_loss(weights, X, y)

# 梯度
loss_grad_fn = grad(loss_fn)

# 优化器
optimizer = optax.sgd(learning_rate=0.01)
opt_state = optimizer.init(weights)

# 训练函数
@jit
def train_step(opt_state, weights, X, y):
    loss, grads = jax.value_and_grad(loss_fn)(weights, X, y)
    updates, opt_state = optimizer.update(grads, opt_state, weights)
    weights = optax.apply_updates(weights, updates)
    return opt_state, weights, loss

# 训练模型
for epoch in range(1000):
    opt_state, weights, loss = train_step(opt_state, weights, X_train, y_train)
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss}")

# 预测函数
def predict(weights, X):
    return jnp.sign(jnp.dot(X, weights))

# 预测测试集
y_pred = predict(weights, X_test)

# 二值化预测结果
y_pred = (y_pred > 0).astype(int)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
```

下面我们再看如何实现多项式核函数：

```python
# 定义多项式核函数
def polynomial_kernel(X1, X2, degree=3, coef0=1):
    # 计算并返回多项式核函数的结果。这里首先计算X1和X2的点积，然后加上coef0，最后将结果提升到degree次幂。
    return (jnp.dot(X1, X2.T) + coef0) ** degree

# 计算核矩阵
def compute_kernel_matrix(X1, X2, kernel_function):
    n_samples_X1 = X1.shape[0]
    # 第二个特征矩阵，通常与X1相同，用于计算样本间的核函数值。
    n_samples_X2 = X2.shape[0]
    kernel_matrix = jnp.zeros((n_samples_X1, n_samples_X2))
    for i in range(n_samples_X1):
        for j in range(n_samples_X2):
            kernel_matrix = kernel_matrix.at[i, j].set(kernel_function(X1[i], X2[j]))
    return kernel_matrix

# 计算训练集的核矩阵
K_train = compute_kernel_matrix(X_train, X_train, lambda x1, x2: polynomial_kernel(x1, x2, degree=3))
```

完整代码留给读者做练习。

下面我们来了解高斯径向基核函数：

高斯径向基函数（Gaussian Radial Basis Function，简称 RBF）的公式如下：$K(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)$

其中：
- $K(x, x')$是输入向量$x$和$x'$之间的相似度。
- $\|x - x'\|$表示向量$x$和$x'$之间的欧氏距离。
- $\sigma$是一个参数，称为尺度参数（scale parameter）或带宽（bandwidth），决定了高斯函数的宽度。

或者，有时也以另外一种形式表示，其中$\gamma = \frac{1}{2\sigma^2}$ ：

$K(x, x') = \exp(-\gamma \|x - x'\|^2)$

我们用代码来实现：

```python
# 定义 RBF 核函数
def rbf_kernel(X1, X2, gamma=0.1):
    sq_dists = jnp.sum((X1[:, None, :] - X2[None, :, :]) ** 2, axis=-1)
    return jnp.exp(-gamma * sq_dists)
```

完整代码如下：

```python
import jax.numpy as jnp
from jax import grad, jit, value_and_grad
from jax import random
import optax
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

# 加载数据集
cancer = datasets.load_breast_cancer()
X = cancer.data
y = cancer.target * 2 - 1  # 将标签转换为 -1 和 1

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 定义 RBF 核函数
def rbf_kernel(X1, X2, gamma=0.1):
    sq_dists = jnp.sum((X1[:, None, :] - X2[None, :, :]) ** 2, axis=-1)
    return jnp.exp(-gamma * sq_dists)

# 计算核矩阵
def compute_kernel_matrix(X1, X2, kernel_function):
    return kernel_function(X1, X2)

# 计算训练集的核矩阵
K_train = compute_kernel_matrix(X_train, X_train, lambda x1, x2: rbf_kernel(x1, x2, gamma=0.1))

# 初始化参数
key = random.PRNGKey(42)
alpha = random.normal(key, (X_train.shape[0],))

# Hinge loss
def hinge_loss(alpha, K, y):
    margins = y * jnp.dot(K, alpha)
    return jnp.mean(jnp.maximum(0, 1 - margins))

# 定义损失函数
def loss_fn(alpha, K, y, C=1.0):
    return 0.5 * jnp.dot(alpha, jnp.dot(K, alpha)) + C * hinge_loss(alpha, K, y)

# 优化器
optimizer = optax.sgd(learning_rate=0.01)
opt_state = optimizer.init(alpha)

# 训练函数
@jit
def train_step(opt_state, alpha, K, y):
    loss, grads = value_and_grad(loss_fn)(alpha, K, y)
    updates, opt_state = optimizer.update(grads, opt_state, alpha)
    alpha = optax.apply_updates(alpha, updates)
    return opt_state, alpha, loss

# 训练模型
for epoch in range(1000):
    opt_state, alpha, loss = train_step(opt_state, alpha, K_train, y_train)
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss}")

# 计算测试集的核矩阵
K_test = compute_kernel_matrix(X_test, X_train, lambda x1, x2: rbf_kernel(x1, x2, gamma=0.1))

# 预测函数
def predict(alpha, K):
    return jnp.sign(jnp.dot(K, alpha))

# 预测测试集
y_pred = predict(alpha, K_test)

# 二值化预测结果
y_pred = (y_pred > 0).astype(int)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
```

最后，sigmoid算法的代码如下：

```python
# 定义 Sigmoid 核函数
def sigmoid_kernel(X1, X2, alpha=0.1, c=0.0):
    return jnp.tanh(alpha * jnp.dot(X1, X2.T) + c)
```

完整代码留给读者作练习。

通过上面4个核函数的实现，相信大家对于核函数各个参数的意义有了更深的理解。支持向量机有很好的数学理论基础，所以希望大家就此对于机器学习的理论有更深的理解。

### 4.5 k近邻算法

最近邻方法（Nearest Neighbor）是一类用于模式识别和回归分析的非参数统计方法。它的基本思想是，通过在数据集中找到与目标样本最接近的一个或多个样本，并基于这些邻近样本的性质来进行预测或分类。

#### 4.5.1 k近邻算法的基本原理

是简单的最近邻方法是K最近邻算法（K-Nearest Neighbors, KNN）。

KNN的主要特点和步骤如下：
- 训练阶段
    - KNN 算法实际上没有显式的训练阶段，只是简单地存储所有的训练数据。
- 预测阶段
    - 给定一个新的样本，算法计算该样本与训练数据集中每个样本的距离（通常使用欧氏距离）。
    - 选取距离最近的 K 个训练样本（称为“邻居”）。
    - 对于分类任务，通过邻居的类别进行投票，并将得票最多的类别作为预测结果。
    - 对于回归任务，计算邻居的平均值或加权平均值作为预测结果。

KNN算法的步骤：

1. 选择参数 K：K 是一个超参数，代表选择的邻居数量。K 的值一般为正整数，且通常通过交叉验证来选择一个合适的值。
2. 计算距离：使用距离度量（例如欧氏距离、曼哈顿距离或其他）计算待分类样本与每个训练样本之间的距离。
3. 找出最近的 K 个邻居：根据距离从小到大排序，选取前 K 个样本。
4. 进行投票或平均：
    - 对于分类任务，根据邻居的类别进行投票，选出得票最多的类别。
    - 对于回归任务，计算邻居的平均值作为预测结果。

通常使用的距离度量包括：

| 名称 | 公式 |
|------|------|
| 欧氏距离（Euclidean Distance） | $d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$ |
| 曼哈顿距离（Manhattan Distance） | $d(x, y) = \sum_{i=1}^{n}|x_i - y_i|$ |
| 切比雪夫距离（Chebyshev Distance） | $d(x, y) = \max(\|x_1 - y_1\|, \|x_2 - y_2\|, ..., \|x_n - y_n\|)$ |


如果公式看起来不太好理解，我们可以用代码来实现：

欧几里得距离：

```python
import jax.numpy as jnp

def euclidean_distance(x, y):
    return jnp.sqrt(jnp.sum((x - y) ** 2))

# 示例
x = jnp.array([1.0, 2.0, 3.0])
y = jnp.array([4.0, 5.0, 6.0])

distance = euclidean_distance(x, y)
```

曼哈顿距离：

```python
import jax.numpy as jnp

def manhattan_distance(x, y):
    return jnp.sum(jnp.abs(x - y))

# 示例
x = jnp.array([1.0, 2.0, 3.0])
y = jnp.array([4.0, 5.0, 6.0])

distance = manhattan_distance(x, y)
```

切比雪夫距离：

```python
import jax.numpy as jnp

def chebyshev_distance(x, y):
    return jnp.max(jnp.abs(x - y))

# 示例
x = jnp.array([1.0, 2.0, 3.0])
y = jnp.array([4.0, 5.0, 6.0])

distance = chebyshev_distance(x, y)
```

KNN算法的优缺点：

|   | 优点                                                         | 缺点                                                         |
|---|--------------------------------------------------------------|--------------------------------------------------------------|
| 1 | 简单易实现。                                                | 计算复杂度高，尤其是对于大规模数据集。                       |
| 2 | 无需显式的训练过程。                                        | 存储需求大，需要存储全部训练数据。                           |
| 3 | 对于特定问题（如推荐系统）可以提供良好的性能。             | 对于高维数据效果不好（维度灾难）。                          |
| 4 |                                                              | 对噪声和不平衡数据敏感。                                     |


#### 4.5.2 用KNeighborsClassifier实现K近邻算法

KNeighborsClassifier的用法非常简单，只要指定K值即可。下面我们用鸢尾花数据集来演示：

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载鸢尾花数据集
iris = load_iris()
X, y = iris.data, iris.target

# 拆分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 数据标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 初始化 KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)  # 选择 3 个邻居

# 训练模型
knn.fit(X_train, y_train)

# 预测
y_pred = knn.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

运行结果如下：

```
Accuracy: 0.91
Classification Report:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        15
  versicolor       0.79      1.00      0.88        15
   virginica       1.00      0.73      0.85        15

    accuracy                           0.91        45
   macro avg       0.93      0.91      0.91        45
weighted avg       0.93      0.91      0.91        45

Confusion Matrix:
[[15  0  0]
 [ 0 15  0]
 [ 0  4 11]]
 ```

 准确率为91%，表现一般般。

 我们如果将k从3改为11，效果如下：

 ```
 ]
0 秒
knn = KNeighborsClassifier(n_neighbors=11)  # 选择 3 个邻居

# 训练模型
knn.fit(X_train, y_train)

# 预测
y_pred = knn.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)

Accuracy: 0.96
Classification Report:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        15
  versicolor       0.88      1.00      0.94        15
   virginica       1.00      0.87      0.93        15

    accuracy                           0.96        45
   macro avg       0.96      0.96      0.96        45
weighted avg       0.96      0.96      0.96        45

Confusion Matrix:
[[15  0  0]
 [ 0 15  0]
 [ 0  2 13]]
 ```

我们可以看到，准确率从91%提升到了96%。

那么，如何确定最佳的K值呢？我们可以通过交叉验证来选择最佳的K值，sklearn为我们提供了cross_val_score函数来完成交叉验证。我们以鸢尾花数据集为例：

```python
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score


# 加载鸢尾花数据集
iris = load_iris()
X, y = iris.data, iris.target


# 定义可能的 k 值范围
k_range = range(1, 31)
k_scores = []

# 进行 k 折交叉验证
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
    k_scores.append(scores.mean())

# 找到最优的 k 值
best_k = k_range[np.argmax(k_scores)]
print(f"The best k value is: {best_k}")
```

经过交叉验证，我们得到最佳的K值为13。

下面我们再用威斯康星乳腺癌数据集来巩固一下。

有了上面的经验，我们上来先做交叉验证，找到最佳的K值：

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载威斯康星乳腺癌数据集
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 定义可能的 k 值范围
k_range = range(1, 32)
k_scores = []

# 进行 k 折交叉验证
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
    k_scores.append(scores.mean())

# 找到最优的 k 值
best_k = k_range[np.argmax(k_scores)]
print(f"The best k value is: {best_k}")
```

得到结果为10。下面我们用10近邻来训练模型：

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载威斯康星乳腺癌数据集
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 拆分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 数据标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 初始化 KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)  # 选择 5 个邻居

# 训练模型
knn.fit(X_train, y_train)

# 预测
y_pred = knn.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=cancer.target_names))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

运行结果如下：

```
Accuracy: 0.96
Classification Report:
              precision    recall  f1-score   support

   malignant       1.00      0.89      0.94        64
      benign       0.94      1.00      0.97       107

    accuracy                           0.96       171
   macro avg       0.97      0.95      0.96       171
weighted avg       0.96      0.96      0.96       171

Confusion Matrix:
[[ 57   7]
 [  0 107]]
```

准确率为96%，比鸢尾花的结果要好。

最后我们再练习用糖尿病数据集，先确定k值：

```python
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载糖尿病数据集
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# 将目标变量二值化（0 表示没有糖尿病，1 表示有糖尿病），这里简单地使用中位数进行二值化
y = (y > np.median(y)).astype(int)

# 定义可能的 k 值范围
k_range = range(1, 100)
k_scores = []

# 进行 k 折交叉验证
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
    k_scores.append(scores.mean())

# 找到最优的 k 值
best_k = k_range[np.argmax(k_scores)]
print(f"The best k value is: {best_k}")
```

这个k值竟然有26这么大！

我们来这个26-近邻来训练模型：

```python
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载糖尿病数据集
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# 将目标变量二值化（0 表示没有糖尿病，1 表示有糖尿病），这里简单地使用中位数进行二值化
y = (y > np.median(y)).astype(int)

# 拆分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 数据标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 初始化 KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=26)  # 选择 5 个邻居

# 训练模型
knn.fit(X_train, y_train)

# 预测
y_pred = knn.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=["No Diabetes", "Diabetes"]))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

运行结果如下：

```
Accuracy: 0.74
Classification Report:
              precision    recall  f1-score   support

 No Diabetes       0.75      0.73      0.74        67
    Diabetes       0.74      0.76      0.75        66

    accuracy                           0.74       133
   macro avg       0.74      0.74      0.74       133
weighted avg       0.74      0.74      0.74       133

Confusion Matrix:
[[49 18]
 [16 50]]
```

|准确率	|乳腺癌	|糖尿病|
|---|---|---|
|逻辑回归	|0.96	|0.73|
|随机梯度下降	|0.98	|0.74 |
|决策树	|0.95	|0.62 | 
| 支持向量机线性核 | 0.98 | 0.77 |
| 支持向量机多项式核 | 0.99 | 0.73 |
| 支持向量机高斯径向基核 | 0.98 | 0.76 |
| 支持向量机sigmoid核 | 0.96 | 0.71 |
| k近邻 | 0.96 | 0.74 |

#### 4.5.3 JAX实现K近邻算法

我们用JAX来实现K近邻算法。k近邻算法的核心是计算距离，我们可以用JAX的vmap来实现并行计算。

首先是计算距离：

```python
def euclidean_distance(point1, point2):
    return jnp.sqrt(jnp.sum((point1 - point2) ** 2))
```

然后把最近的邻居找出来：

```python
# KNN 分类函数
def knn_classify(train_data, train_labels, query_point, k):
    distances = vmap(lambda point: euclidean_distance(point, query_point))(train_data)
    nearest_neighbors_idx = jnp.argsort(distances)[:k]
    nearest_labels = train_labels[nearest_neighbors_idx]

    # 计算每个标签的频率
    num_classes = jnp.max(train_labels) + 1
    counts = jnp.zeros(num_classes)
    counts = counts.at[nearest_labels].add(1)

    # 返回出现次数最多的标签
    return jnp.argmax(counts)
```

我们用到了vmap，还有jax.numpy的argsort排序。这样既显得代码简洁，也可以有效利用JAX的并行计算能力。

同样，预测函数也是用vmap搞定：

```python
# 对测试数据进行预测
def predict(test_data, train_data, train_labels, k):
    predictions = vmap(lambda point: knn_classify(train_data, train_labels, point, k))(test_data)
    return predictions
```

最后我们把代码串起来：

```python
import jax
import jax.numpy as jnp
from jax import vmap
from sklearn import datasets
from sklearn.model_selection import train_test_split

import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

# 加载 Iris 数据集
iris = datasets.load_iris()
data = iris.data
labels = iris.target

# 将数据分为训练集和测试集
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=42)

# 将数据转换为 JAX 数组
train_data = jnp.array(train_data)
test_data = jnp.array(test_data)
train_labels = jnp.array(train_labels)
test_labels = jnp.array(test_labels)

# 欧氏距离函数
def euclidean_distance(point1, point2):
    return jnp.sqrt(jnp.sum((point1 - point2) ** 2))

# KNN 分类函数
def knn_classify(train_data, train_labels, query_point, k):
    distances = vmap(lambda point: euclidean_distance(point, query_point))(train_data)
    nearest_neighbors_idx = jnp.argsort(distances)[:k]
    nearest_labels = train_labels[nearest_neighbors_idx]

    # 计算每个标签的频率
    num_classes = jnp.max(train_labels) + 1
    counts = jnp.zeros(num_classes)
    counts = counts.at[nearest_labels].add(1)

    # 返回出现次数最多的标签
    return jnp.argmax(counts)

# 对测试数据进行预测
def predict(test_data, train_data, train_labels, k):
    predictions = vmap(lambda point: knn_classify(train_data, train_labels, point, k))(test_data)
    return predictions

# 设置 k 的值
k = 3

# 对测试数据进行预测
test_predictions = predict(test_data, train_data, train_labels, k)

# 计算准确率
accuracy = jnp.mean(test_predictions == test_labels)
print(f'Accuracy: {accuracy * 100:.2f}%')
```

### 4.6 朴素贝叶斯方法

朴素贝叶斯（Naive Bayes）方法是一类基于贝叶斯定理的简单而强大的分类算法，特别适用于大规模数据集。尽管其假设特征之间相互独立（这在实际情况中很少成立，因此称为“朴素”），但它在许多实际应用中表现良好，特别是文本分类和垃圾邮件过滤。

贝叶斯定理
贝叶斯定理描述了后验概率$P(C∣X)$是如何通过先验概率$ P(C)$、似然$ P(X∣C) $和证据 $P(X)$ 计算得到的：
$P(C|X) = \frac{P(X|C)P(C)}{P(X)}$

其中：

- $P(C|X)$是给定特征$X$时类别$C$的后验概率。
- $P(C)$是类别$C$的先验概率。
- $P(X|C)$是类别$C$给定的情况下的似然。
- $P(X)$是特征$X$的证据。

朴素贝叶斯方法通过假设特征之间相互独立，简化了概率的计算：$P(X|C) = P(x_1, x_2, ..., x_n|C) = P(x_1|C)P(x_2|C)...P(x_n|C)$

因此，后验概率可以表示为：$P(C|X) \propto  P(C) \prod_i^n P(x_i|C)$

根据贝叶斯定理，朴素贝叶斯分类器选择使后验概率最大的类别:
$\hat{C}=argmax P(C) \prod_i^n P(x_i|C)$

朴素贝叶斯分类器的主要类型有：

- 高斯朴素贝叶斯（Gaussian Naive Bayes）：假设特征值服从正态分布，适用于连续数据。
- 多项式朴素贝叶斯（Multinomial Naive Bayes）：适用于离散数据，常用于文本分类，假设特征值是文档中单词的出现次数。
- 伯努利朴素贝叶斯（Bernoulli Naive Bayes）：适用于二元数据，假设特征值是布尔变量（例如，单词是否出现在文档中）。

#### 4.6.1 高斯朴素贝叶斯分类器

高斯朴素贝叶斯分类器是一种适用于连续数据的朴素贝叶斯分类器，假设特征值服从正态分布。我们用鸢尾花数据集来演示：

```python
# 导入必要的库
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 将数据集划分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 初始化高斯朴素贝叶斯分类器
gnb = GaussianNB()

# 训练模型
gnb.fit(X_train, y_train)

# 进行预测
y_pred = gnb.predict(X_test)

# 评估模型性能
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# 使用PCA将数据降至二维以便可视化
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# 再次划分为训练集和测试集
X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = train_test_split(X_reduced, y, test_size=0.3, random_state=42)

# 训练朴素贝叶斯模型
gnb.fit(X_train_reduced, y_train_reduced)

# 进行预测
y_pred_reduced = gnb.predict(X_test_reduced)

# 绘制结果
plt.figure(figsize=(8, 6))
plt.scatter(X_test_reduced[:, 0], X_test_reduced[:, 1], c=y_pred_reduced, cmap='viridis', edgecolor='k', s=100)
plt.title('Naive Bayes Classification with PCA on Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar()
plt.show()
```

运行的效果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/iris_bayes.png)

下面我们看下朴素贝叶斯算法对于威斯康星乳腺癌数据集的效果：

```python
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载威斯康星乳腺癌数据集
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 拆分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 数据标准化（对于高斯朴素贝叶斯分类器不是必须的，但有时可以提高效果）
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 初始化 GaussianNB
gnb = GaussianNB()

# 训练模型
gnb.fit(X_train, y_train)

# 预测
y_pred = gnb.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=cancer.target_names))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

输出结果如下：

```
Accuracy: 0.94
Classification Report:
              precision    recall  f1-score   support

   malignant       0.93      0.89      0.91        64
      benign       0.94      0.96      0.95       107

    accuracy                           0.94       171
   macro avg       0.94      0.93      0.93       171
weighted avg       0.94      0.94      0.94       171

Confusion Matrix:
[[ 57   7]
 [  4 103]]
 ```

 可以看到，准确率为94%，效果还不错。

 最后我们再处理一下糖尿病数据集：

 ```python
 import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载糖尿病数据集
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# 将目标变量二值化（0 表示没有糖尿病，1 表示有糖尿病），这里简单地使用中位数进行二值化
y = (y > np.median(y)).astype(int)

# 拆分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 数据标准化（对高斯朴素贝叶斯分类器不是必须的，但有时可以提高效果）
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 初始化 GaussianNB
gnb = GaussianNB()

# 训练模型
gnb.fit(X_train, y_train)

# 预测
y_pred = gnb.predict(X_test)

# 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=["No Diabetes", "Diabetes"]))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

输出结果如下：

 ```
 Accuracy: 0.79
Classification Report:
              precision    recall  f1-score   support

 No Diabetes       0.80      0.78      0.79        67
    Diabetes       0.78      0.80      0.79        66

    accuracy                           0.79       133
   macro avg       0.79      0.79      0.79       133
weighted avg       0.79      0.79      0.79       133

Confusion Matrix:
[[52 15]
 [13 53]]
 ```

 这个结果怎么样？从前面一直学过来的读者一定有概念了。这是目前为止最好的结果，刷新了线性核支持向量机的结果。

|准确率	|乳腺癌	|糖尿病|
|---|---|---|
|逻辑回归	|0.96	|0.73|
|随机梯度下降	|0.98	|0.74 |
|决策树	|0.95	|0.62 | 
| 支持向量机线性核 | 0.98 | 0.77 |
| 支持向量机多项式核 | 0.99 | 0.73 |
| 支持向量机高斯径向基核 | 0.98 | 0.76 |
| 支持向量机sigmoid核 | 0.96 | 0.71 |
| k近邻 | 0.96 | 0.74 |
| 高斯朴素贝叶斯 | 0.94 | 0.79 |

#### 4.6.2 JAX实现朴素贝叶斯算法

既然朴素贝叶斯的算法效果这么好，我们当然也要用JAX来实现一下。

比如我们来看一下如何用JAX实现朴素贝叶斯算法。

```python
# 计算每个类的先验概率和高斯分布参数
def compute_gaussian_parameters(X, y):
    classes = jnp.unique(y)
    n_classes = len(classes)
    n_features = X.shape[1]
    
    means = jnp.zeros((n_classes, n_features))
    variances = jnp.zeros((n_classes, n_features))
    priors = jnp.zeros(n_classes)
    
    for cls in classes:
        X_cls = X[y == cls]
        means = means.at[cls].set(jnp.mean(X_cls, axis=0))
        variances = variances.at[cls].set(jnp.var(X_cls, axis=0) + 1e-6)  # Add a small value to avoid division by zero
        priors = priors.at[cls].set(len(X_cls) / len(X))
    
    return means, variances, priors

means, variances, priors = compute_gaussian_parameters(X_train, y_train)
```

在上面的代码中，我们定义了一个 compute_gaussian_parameters 函数，用于计算每个类的先验概率、均值和方差。

千万不要被先验概率这样的名词吓到，就是该类别样本数占总样本数的比例而己。

我们复习下高斯分布概率密度函数公式：

$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)$

其中：
- $\mu$是均值（mean）。
- $\sigma^2$ 是方差（variance）。
- $x$是要计算概率密度的点。

然后我们按公式用代码实现：

```python
# 计算高斯分布的概率密度函数
def gaussian_pdf(x, mean, var):
    coeff = 1.0 / jnp.sqrt(2 * jnp.pi * var)
    exponent = jnp.exp(-((x - mean) ** 2) / (2 * var))
    return coeff * exponent
```

高斯朴素贝叶斯分类器的工作原理分为三步：

- 先验概率：反映类别出现的先验信息。
- 似然：反映在给定类别条件下，特征值出现的概率。
- 后验概率：结合先验概率和似然，计算样本属于某类别的概率。高斯朴素贝叶斯分类器假设特征之间相互独立，因此可以将各特征的似然相乘（在对数空间相加）。

下面我们算似然。

我们用`gaussian_pdf(X, means[cls], variances[cls])` 计算样本在该类别下的概率密度。

然后用 `jnp.log` 取对数。

再用 `jnp.sum(..., axis=1)` 对特征求和，得到每个样本的对数似然。

接着将对数先验概率和对数似然相加，得到对数后验概率，并存储在 `log_probs` 的相应位置。

最后 `jnp.argmax(log_probs, axis=1)` 找出每个样本的最大对数概率对应的类别索引，即预测的类别。

写出来如下：

```python
# 预测函数
def predict(X, means, variances, priors):
    n_samples = X.shape[0]
    n_classes = means.shape[0]
    
    log_probs = jnp.zeros((n_samples, n_classes))
    
    for cls in range(n_classes):
        log_prior = jnp.log(priors[cls])
        log_likelihood = jnp.sum(jnp.log(gaussian_pdf(X, means[cls], variances[cls])), axis=1)
        log_probs = log_probs.at[:, cls].set(log_prior + log_likelihood)
    
    return jnp.argmax(log_probs, axis=1)
```

最后我们把代码串起来：

```python
import jax
import jax.numpy as jnp
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 数据预处理
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 计算每个类的先验概率和高斯分布参数
def compute_gaussian_parameters(X, y):
    classes = jnp.unique(y)
    n_classes = len(classes)
    n_features = X.shape[1]
    
    means = jnp.zeros((n_classes, n_features))
    variances = jnp.zeros((n_classes, n_features))
    priors = jnp.zeros(n_classes)
    
    for cls in classes:
        X_cls = X[y == cls]
        means = means.at[cls].set(jnp.mean(X_cls, axis=0))
        variances = variances.at[cls].set(jnp.var(X_cls, axis=0) + 1e-6)  # Add a small value to avoid division by zero
        priors = priors.at[cls].set(len(X_cls) / len(X))
    
    return means, variances, priors

means, variances, priors = compute_gaussian_parameters(X_train, y_train)

# 计算高斯分布的概率密度函数
def gaussian_pdf(x, mean, var):
    coeff = 1.0 / jnp.sqrt(2 * jnp.pi * var)
    exponent = jnp.exp(-((x - mean) ** 2) / (2 * var))
    return coeff * exponent

# 预测函数
def predict(X, means, variances, priors):
    n_samples = X.shape[0]
    n_classes = means.shape[0]
    
    log_probs = jnp.zeros((n_samples, n_classes))
    
    for cls in range(n_classes):
        log_prior = jnp.log(priors[cls])
        log_likelihood = jnp.sum(jnp.log(gaussian_pdf(X, means[cls], variances[cls])), axis=1)
        log_probs = log_probs.at[:, cls].set(log_prior + log_likelihood)
    
    return jnp.argmax(log_probs, axis=1)

# 训练集上的预测
y_train_pred = predict(X_train, means, variances, priors)
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"Train Accuracy: {train_accuracy}")

# 测试集上的预测
y_test_pred = predict(X_test, means, variances, priors)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy}")
```

运行结果如下：

```
Train Accuracy: 0.95
Test Accuracy: 1.0
```

测试准确率高达100%，说明我们手动实现的算法，效果也非常不错。

### 4.7 高斯过程

高斯过程（Gaussian Process, GP）是概率论和数理统计中随机过程的一种，它是一系列服从正态分布的随机变量在一指数集（index set）内的组合。在高斯过程中，任意随机变量的线性组合都服从正态分布，每个有限维分布都是联合正态分布，且其本身在连续指数集上的概率密度函数即是所有随机变量的高斯测度，因此被视为联合正态分布的无限维广义延伸。高斯过程由其数学期望和协方差函数完全决定，并继承了正态分布的诸多性质。

高斯过程的例子包括维纳过程、奥恩斯坦-乌伦贝克过程等。在人工智能领域，高斯过程模型属于无参数模型，可以解决高维空间（实际上是无限维）的数学问题，可以面对复杂的数学问题。结合贝叶斯概率算法，高斯过程可以实现通过先验概率，推导未知后验输入变量的后验概率。高斯过程观测变量空间是连续域，可以是时间或空间。

高斯过程分类器（GPC）是一种基于高斯过程（Gaussian Process, GP）的非参数贝叶斯分类方法。高斯过程分类器通过将高斯过程的柔性和贝叶斯框架结合起来，对于分类问题提供了强大的非线性建模能力。

在 GPC 中，假设存在一个隐函数 $f(\mathbf{x})$，其值决定了输入 $\mathbf{x}$ 属于某个类别的概率。通过高斯过程来建模隐函数 $f(\mathbf{x})$。

链接函数（如逻辑函数或 probit 函数）将隐函数值映射到类别的概率。对于逻辑函数，映射关系为：$p(y=1|\mathbf{x}) = \sigma(f(\mathbf{x})) = \frac{1}{1 + \exp(-f(\mathbf{x}))}$
其中，$\sigma(\cdot)$ 是逻辑函数。

高斯过程分类器分为训练阶段和预测阶段两部分：

1. 训练阶段：
    - 给定训练数据 $\{\mathbf{X}, \mathbf{y}\}$，其中 $\mathbf{X}$ 是输入矩阵，$\mathbf{y}$ 是类别标签向量。假设隐函数 $f$ 服从高斯过程：
      $f(\mathbf{x}) \sim \mathcal{GP}(0, k(\mathbf{x}, \mathbf{x}'))$
      其中，$k(\mathbf{x}, \mathbf{x}')$ 是核函数。
    - 通过贝叶斯推断，计算隐函数的后验分布。由于引入了非线性链接函数，后验分布通常是非高斯的，需要使用近似推断方法，如拉普拉斯近似或期望传播（EP）。

2. 预测阶段：
    - 给定新的输入 $\mathbf{x}_*$，计算其隐函数值的后验分布，并通过链接函数获得类别概率：
      $p(y_*=1|\mathbf{x}_*, \mathbf{X}, \mathbf{y}) = \int \sigma(f_*) p(f_*|\mathbf{x}_*, \mathbf{X}, \mathbf{y}) df_*$
      其中，$f_*$ 是新输入点的隐函数值。

我们来看如何使用高斯过程分类器来实现鸢尾花的分类：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 加载鸢尾花数据集
iris = load_iris()
X, y = iris.data, iris.target

# 仅使用前两维特征进行可视化
X = X[:, :2]

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 定义核函数
kernel = 1.0 * RBF(length_scale=1.0)

# 创建高斯过程分类器
gpc = GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=10)

# 训练模型
gpc.fit(X_train, y_train)

# 预测
y_pred = gpc.predict(X_test)

# 打印模型的性能
print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')
print('Classification Report:')
print(classification_report(y_test, y_pred))
print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

# 可视化决策边界
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))

Z = gpc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.viridis)
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, edgecolor='k', cmap=plt.cm.viridis)
plt.title('Gaussian Process Classifier on Iris Dataset')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.show()
```

代码的主要过程如下：

- 加载和预处理数据：
    - 使用load_iris()加载鸢尾花数据集。
    - 仅使用前两维特征（萼片长度和宽度）以便于可视化。
    - 划分数据集为训练集和测试集。
    - 使用StandardScaler对特征进行标准化。
- 定义和训练GPC模型：
    - 使用高斯径向基函数（RBF）作为核函数。
    - 创建GaussianProcessClassifier实例并训练模型。
- 模型预测和评估：
    - 预测测试集的标签并计算准确率。
    - 打印分类报告和混淆矩阵以评估模型性能。
- 可视化决策边界：
    - 生成网格点并预测每个网格点的类别，以绘制决策边界。
    - 绘制测试集的散点图，并在背景上显示决策边界。

运行结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gpc_iris.png)

下面我们来看看高斯过程处理威斯康星乳腺癌数据集的效果：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

# 加载威斯康星乳腺癌数据集
data = load_breast_cancer()
X, y = data.data, data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 定义高斯过程分类器及其核函数
kernel = 1.0 * RBF(1.0)
gpc = GaussianProcessClassifier(kernel=kernel, random_state=42)

# 训练分类器
gpc.fit(X_train, y_train)

# 预测
y_pred = gpc.predict(X_test)

# 评估分类器性能
print("Classification report:")
print(classification_report(y_test, y_pred))

print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))
```

输出结果如下：

```
Classification report:
              precision    recall  f1-score   support

           0       0.98      0.97      0.98        64
           1       0.98      0.99      0.99       107

    accuracy                           0.98       171
   macro avg       0.98      0.98      0.98       171
weighted avg       0.98      0.98      0.98       171

Confusion matrix:
[[ 62   2]
 [  1 106]]
```

准确率为98%，也是相当不错的结果。

最后我们再看看高斯过程处理糖尿病数据集的效果：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

# 加载糖尿病数据集
data = load_diabetes()
X, y = data.data, data.target

# 将目标变量转化为二分类问题
# 假设我们将目标值大于中位数的样本标记为1，其他为0
median_target = np.median(y)
y = (y > median_target).astype(int)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 定义高斯过程分类器及其核函数
kernel = 1.0 * RBF(1.0)
gpc = GaussianProcessClassifier(kernel=kernel, random_state=42)

# 训练分类器
gpc.fit(X_train, y_train)

# 预测
y_pred = gpc.predict(X_test)

# 评估分类器性能
print("Classification report:")
print(classification_report(y_test, y_pred))

print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

# 可视化混淆矩阵
cm = confusion_matrix(y_test, y_pred)
plt.matshow(cm, cmap=plt.cm.Blues)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
```

输出结果如下：

```
Classification report:
              precision    recall  f1-score   support

           0       0.77      0.73      0.75        67
           1       0.74      0.77      0.76        66

    accuracy                           0.75       133
   macro avg       0.75      0.75      0.75       133
weighted avg       0.75      0.75      0.75       133

Confusion matrix:
[[49 18]
 [15 51]]
```

准确率为75%，这个结果还是强于逻辑回归和决策树等的，处于中等水平。

|准确率	|乳腺癌	|糖尿病|
|---|---|---|
| 逻辑回归	|0.96	|0.73|
| 随机梯度下降	|0.98	|0.74 |
| 决策树	|0.95	|0.62 | 
| 支持向量机线性核 | 0.98 | 0.77 |
| 支持向量机多项式核 | 0.99 | 0.73 |
| 支持向量机高斯径向基核 | 0.98 | 0.76 |
| 支持向量机sigmoid核 | 0.96 | 0.71 |
| k近邻 | 0.96 | 0.74 |
| 高斯朴素贝叶斯 | 0.94 | 0.79 |
| 高斯过程 | 0.98 | 0.75 |

### 4.8 半监督学习

半监督学习 (Semi-supervised Learning) 结合了少量标记数据（有标签）和大量未标记数据（无标签）进行训练。此方法介于监督学习和无监督学习之间，旨在通过利用未标记数据来提高模型的性能。

在许多实际应用中，获取大量的未标记数据相对容易且成本较低，但获取带标签的数据却费用高昂且耗时。半监督学习通过有效利用未标记数据，可以在减少标注成本的同时提高模型的准确性和鲁棒性。

半监督学习的主要方法

| 方法类型                             | 描述 |
|--------------------------------------|------|
| 自训练 (Self-training)               | 1. 在初始阶段，使用有标签的数据训练一个初始模型。<br>2. 用该模型预测未标记数据中的标签，并将高置信度的预测结果加入有标签数据集中，重新训练模型。<br>3. 反复迭代上述过程。 |
| 共训练 (Co-training)                 | 1. 将特征分为两部分，分别训练两个模型。<br>2. 每个模型用自己的预测结果去标记未标记数据，并将高置信度的预测结果提供给另一个模型。<br>3. 两个模型相互合作，共同提高性能。 |
| 图半监督学习 (Graph-based Semi-supervised Learning) | 1. 构建样本之间的图结构，用节点表示样本，用边表示样本之间的相似度。<br>2. 利用图结构信息，通过传播标签来预测未标记数据的标签。 |
| 生成对抗网络 (GAN) 的半监督学习     | 1. 使用生成对抗网络中的生成器和判别器，其中判别器不仅预测真假样本，还预测样本的类别。<br>2. 结合生成器生成的样本和部分有标签的数据来训练判别器。 |


下面我们采用将威斯康星乳腺癌数据集的部分标签数据删除，然后使用自训练方法来训练模型：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.semi_supervised import LabelSpreading
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

# 加载威斯康星乳腺癌数据集
data = load_breast_cancer()
X, y = data.data, data.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 在训练集中生成未标记数据
# 为简单起见，我们将训练集中 90% 的标签设为 -1（表示未标记）
rng = np.random.RandomState(42)
random_unlabeled_points = rng.rand(len(y_train)) < 0.9
y_train[random_unlabeled_points] = -1

# 标准化特征
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 定义半监督学习模型
label_spread = LabelSpreading(kernel='rbf', alpha=0.8)

# 训练模型
label_spread.fit(X_train, y_train)

# 预测
y_pred = label_spread.predict(X_test)

# 评估模型性能
print("Classification report:")
print(classification_report(y_test, y_pred))

print("Confusion matrix:")
print(confusion_matrix(y_test, y_pred))

# 可视化混淆矩阵
cm = confusion_matrix(y_test, y_pred)
plt.matshow(cm, cmap=plt.cm.Blues)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
```

可以看到，虽然我们删除了大量的标签数据，但通过半监督学习的方法，我们依然可以得到不错的结果。

```
Classification report:
              precision    recall  f1-score   support

           0       1.00      0.56      0.72        64
           1       0.79      1.00      0.88       107

    accuracy                           0.84       171
   macro avg       0.90      0.78      0.80       171
weighted avg       0.87      0.84      0.82       171

Confusion matrix:
[[ 36  28]
 [  0 107]]
```

## 第五章 监督学习：数值预测

我们先来看数值预测，也就是根据已有的数据去预测未知的数据。被举烂了的例子就是预测房价。

### 5.1 线性回归

最简单的数值预测方法就是用一条直线来拟合现有的数据。

假设我们有一组数据点 $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$，并希望找到一个函数 $f(x)$ 来拟合这些数据点。最小二乘法的目标是找到函数 $f(x)$ 的参数，使得以下目标函数最小：$S = \sum_{i=1}^{n} [y_i - f(x_i)]^2$

其中，$S$ 是误差平方和。

在线性回归中，拟合函数通常是一个线性函数，即：$f(x) = \beta_0 + \beta_1 x$

此时，最小二乘法通过求解以下方程组来找到最佳参数 $\beta_0$ 和 $\beta_1$：

$
\begin{cases}
\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) = 0 \\
\sum_{i=1}^{n} x_i (y_i - \beta_0 - \beta_1 x_i) = 0
\end{cases}
$

#### 5.1.1 用LinearRegression实现线性回归

我们举个例子，预测2024年开年的上证指数收盘价。我们先把1月初几天的数据输入进来：

```python
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

X = np.array([[20240102],[20240103],[20240104],[20240105],[20240108],[20240109]]).reshape(-1,1)
y = [2962.28,2967.25,2954.35,2929.18,2887.54,2893.25]

plt.figure()
plt.title('2024年1月上证指数走势图')
plt.xlabel('日期')
plt.ylabel('收盘价')
plt.plot(X,y,'o',c='r')
plt.grid(True)
plt.show()
```

![stock_2.png](https://upload-images.jianshu.io/upload_images/1638145-39a85358716f1683.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


matplotlib默认不支持中文字体，所以我们使用下面两条语句来显示中文：
plt.rcParams['font.sans-serif'] = ['SimHei']
设置 matplotlib 图表的默认字体为 SimHei。这是因为 matplotlib 默认不支持中文，设置中文字体是为了在图表中显示中文标签。

plt.rcParams['axes.unicode_minus'] = False
解决在图表中显示负号时出现的问题。在默认情况下，matplotlib 会使用 Unicode 字符来显示负号，这可能导致显示问题。

上面的红点就是现有的股价的点，我们可以看到，基本上是一条向下跌落的直线。我们不用什么复杂的模型，就用一条直线来预测后一天的股价就可以了。

我们下面使用线性回归方法来预测2024年1月10日的股价，代码如下：

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)
date1 = np.array([[20240110]])
predicted_price = model.predict(date1)[0]
print('20240108收盘价: %.2f' % predicted_price)
```

最后得到的结果是：

```
20240110收盘价: 2873.51
```

实际2024年1月10日的收盘价是多少呢？答案是2877.70。准确度还是相当不错的。当然了，我们是赶上下降趋势没有反弹了。

下面我们使用sklearn自带的数据生成功能，来生成一些数据，然后用线性回归来拟合。

```python
from sklearn.datasets import make_regression

X, y = make_regression(n_samples=20, n_features=5, noise=1, random_state=42)
```

上面的代码中，我们生成了20个样本，每个样本有5个特征，噪声为1。我们可以看一下生成的数据：

```python
array([[ 0.33126343,  0.93128012,  0.97554513, -0.83921752, -0.30921238],
       [ 1.56464366,  0.36139561, -2.6197451 ,  1.53803657, -0.03582604],
       [ 0.34361829, -0.71984421, -1.76304016, -0.46063877,  1.05712223],
       [-1.05771093, -0.60170661,  0.82254491,  1.85227818, -0.01349722],
       [ 0.09176078,  0.8219025 , -1.98756891,  0.08704707, -0.29900735],
       [-1.91328024, -0.46341769, -1.72491783, -0.46572975,  0.24196227],
       [-0.32766215,  0.09707755, -0.39210815,  0.96864499, -0.70205309],
       [-1.32818605, -1.22084365,  0.19686124,  0.2088636 , -1.95967012],
       [-1.19620662, -0.47917424,  0.81252582, -0.18565898, -1.10633497],
       [-0.3011037 ,  0.73846658, -1.47852199,  0.17136828, -0.11564828],
       [ 1.52302986,  0.49671415, -0.23415337, -0.1382643 ,  0.64768854],
       [ 0.00511346, -1.46351495, -0.23458713,  0.29612028,  0.26105527],
       [ 0.36163603,  1.35624003, -0.64511975, -0.07201012,  1.0035329 ],
       [-0.5297602 , -0.50175704,  0.51326743,  0.91540212,  0.32875111],
       [-0.46947439, -0.23413696,  0.54256004,  1.57921282,  0.76743473],
       [-0.90802408, -0.56228753, -1.4123037 , -1.01283112,  0.31424733],
       [-1.42474819,  1.46564877, -0.54438272, -0.2257763 ,  0.0675282 ],
       [-0.60063869,  0.11092259, -0.29169375, -1.15099358,  0.37569802],
       [-0.51827022, -0.21967189, -0.8084936 ,  0.35711257,  1.47789404],
       [ 0.61167629,  0.32408397,  1.03099952, -0.38508228, -0.676922  ]]),
array([ 119.69958473, -145.95166014, -182.6773034 ,   56.63896461,
       -152.0768812 , -254.96997319,  -30.11603227,  -97.08191688,
         -0.6801637 , -116.23023453,   54.98216813,  -74.47911599,
         18.16181852,   33.83208816,   67.78733098, -204.61933136,
        -42.47803285,  -65.88718206,  -76.91418018,  114.0665755 ])
```

生成好数据之后，我们调用线性回归模型来拟合数据：

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,y)
```

然后我们可以看一下拟合的结果：

```python
print(model.predict([[0,0,0,0,0]]))
```

下面我们再尝试去处理一个更加复杂的数据集，这次我们使用波士顿房价数据集。
因为某些原因，sklearn从1.2版本中已经去掉了波士顿房价数据集，所以我们需要使用keras来加载波士顿房价数据集。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 加载波士顿房价数据集
from keras.datasets import boston_housing

(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 创建线性回归模型
model = LinearRegression()

# 训练模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 打印模型的性能指标
print(f'Mean squared error: {mean_squared_error(y_test, y_pred):.2f}')
print(f'R^2 score: {r2_score(y_test, y_pred):.2f}')

# 可视化结果：只绘制一个特征（例如，RM）与房价的关系
plt.figure(figsize=(10, 6))
plt.scatter(X_test[:, 5], y_test, color='red', label='Actual Prices')
plt.scatter(X_test[:, 5], y_pred, color='blue', label='Predicted Prices')
plt.xlabel('Average number of rooms per dwelling (RM)')
plt.ylabel('House Price')
plt.title('Boston House Price Prediction using Linear Regression')
plt.legend()
plt.show()

# 打印模型的系数和截距
print(f'Model Coefficients: {model.coef_}')
print(f'Model Intercept: {model.intercept_}')
```

运行结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/boston_lr.png)

### 5.1.2 JAX实现线性回归

我们先看来如何用JAX来实现波士顿房价预测。

```python
import jax
import jax.numpy as jnp
from keras.datasets import boston_housing
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

# 加载波士顿房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 数据预处理
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 初始化参数
key = jax.random.PRNGKey(0)
w = jax.random.normal(key, (X_train.shape[1],))
b = 0.0

# 定义线性回归模型
def predict(w, b, X):
    return jnp.dot(X, w) + b

# 定义损失函数
def loss_fn(w, b, X, y):
    preds = predict(w, b, X)
    return jnp.mean((preds - y) ** 2)  # 均方误差

# 梯度下降
grad_fn = jax.jit(jax.grad(loss_fn, argnums=(0, 1)))

# 训练模型
learning_rate = 0.01
epochs = 1000

for epoch in range(epochs):
    grads_w, grads_b = grad_fn(w, b, X_train, y_train)
    w -= learning_rate * grads_w
    b -= learning_rate * grads_b

    if epoch % 100 == 0:
        loss = loss_fn(w, b, X_train, y_train)
        print(f"Epoch {epoch}, Loss: {loss}")

# 测试模型
y_pred_train = predict(w, b, X_train)
y_pred_test = predict(w, b, X_test)

# 计算训练和测试的均方误差
train_mse = mean_squared_error(y_train, y_pred_train)
test_mse = mean_squared_error(y_test, y_pred_test)

print(f"Train MSE: {train_mse}")
print(f"Test MSE: {test_mse}")

# 可视化
plt.scatter(y_test, y_pred_test)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted Prices")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()
```

在上面的代码中，我们首先加载波士顿房价数据集，并对数据进行标准化处理。然后我们初始化模型参数 w 和 b，并定义了线性回归模型 predict 和损失函数 loss_fn。接着我们使用梯度下降法训练模型，最后计算训练和测试的均方误差，并进行可视化。

输出的结果如下：
```
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz
57026/57026 [==============================] - 0s 0us/step
Epoch 0, Loss: 674.8168334960938
Epoch 100, Loss: 33.74641418457031
Epoch 200, Loss: 23.55044174194336
Epoch 300, Loss: 22.765607833862305
Epoch 400, Loss: 22.442581176757812
Epoch 500, Loss: 22.27152442932129
Epoch 600, Loss: 22.176055908203125
Epoch 700, Loss: 22.120210647583008
Epoch 800, Loss: 22.085887908935547
Epoch 900, Loss: 22.06372833251953
Train MSE: 22.048864804304124
Test MSE: 23.05307299071211
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/boston_jax.png)

有些代码头次见不熟悉没关系。我们对一些代码进行下解释。

```python
grad_fn = jax.jit(jax.grad(loss_fn, argnums=(0, 1)))
```

jax.grad 函数用于计算标量函数（如损失函数）相对于其参数的梯度。这里的 loss_fn 是损失函数，它的参数有 w（权重）和 b（偏置）。

`argnums=(0, 1)` 指定了对 loss_fn 的前两个参数（即 w 和 b）计算梯度。

jax.jit 是一个装饰器，它通过将 Python 函数编译成高效的 XLA（加速线性代数）代码来加速函数的执行。

`jax.jit(jax.grad(...))` 将梯度计算函数 grad_fn 编译成高效的机器代码，以提高梯度计算的性能。

训练部分：

```python
learning_rate = 0.01
epochs = 1000

for epoch in range(epochs):
    grads_w, grads_b = grad_fn(w, b, X_train, y_train)
    w -= learning_rate * grads_w
    b -= learning_rate * grads_b

    if epoch % 100 == 0:
        loss = loss_fn(w, b, X_train, y_train)
        print(f"Epoch {epoch}, Loss: {loss}")
```

通过 grad_fn 计算当前权重 w 和偏置 b 对于训练数据 X_train 和 y_train 的梯度 grads_w 和 grads_b。

然后按照梯度下降算法的规则，使用学习率 learning_rate 更新权重 w 和偏置 b。具体来说，从当前的 w 和 b 中减去梯度乘以学习率。

### 5.2 Lasso回归与最小角回归

#### 5.2.1 Lasso回归

Lasso 回归（Least Absolute Shrinkage and Selection Operator）是一种线性回归方法，旨在通过对回归系数施加 $ L1 $ 正则化（即回归系数的绝对值之和）实现特征选择和模型稀疏化。Lasso 回归不仅可以用于预测，还能帮助我们理解哪些特征对目标变量最为重要。

Lasso 回归的目标是最小化以下目标函数：

$ \underset{\beta}{\min} \left\{ \frac{1}{2n} \sum_{i=1}^{n} \left( y_i - \sum_{j=1}^{p} x_{ij} \beta_j \right)^2 + \alpha \sum_{j=1}^{p} |\beta_j| \right\} $

其中：
- $ n $ 是样本数。
- $ p $ 是特征数。
- $ y_i $ 是第 $ i $ 个样本的实际值。
- $ x_{ij} $ 是第 $ i $ 个样本的第 $ j $ 个特征值。
- $ \beta_j $ 是第 $ j $ 个特征的回归系数。
- $ \alpha $ 是正则化参数，控制正则化的强度。

Lasso 回归的主要功能有：

- 特征选择：Lasso 回归可以将一些回归系数缩减为零，从而自动选择出对模型有重要贡献的特征。
- 防止过拟合：通过引入正则化项，Lasso 回归可以防止模型在训练数据上过拟合，从而提高模型在测试数据上的泛化能力。
- 模型稀疏化：Lasso 回归倾向于产生稀疏模型，即只有少数特征的系数非零。对于高维数据（特征数远大于样本数）尤为重要，因为它能简化模型，降低计算复杂度。

下面我们看如何用Lasso回归来预测波士顿房价：

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error
from keras.datasets import boston_housing

# 加载波士顿房价数据集
(X_train_full, y_train_full), (X_test, y_test) = boston_housing.load_data()

# 划分训练集和验证集
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=0)


# 创建并训练 Lasso 回归模型
lasso = Lasso(alpha=1.0)
lasso.fit(X_train, y_train)

# 进行预测
y_train_pred = lasso.predict(X_train)
y_test_pred = lasso.predict(X_test)

# 计算均方误差
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f'训练集均方误差: {train_mse}')
print(f'测试集均方误差: {test_mse}')

# 输出 Lasso 回归系数
print("Lasso 回归系数:", lasso.coef_)
```

输出结果如下：

```
训练集均方误差: 29.341397538231636
测试集均方误差: 26.250061684108413
Lasso 回归系数: [-0.05807504  0.0685187  -0.          0.         -0.          0.74629764
  0.03929297 -0.72118044  0.3090587  -0.01661717 -0.60356941  0.00983247
 -0.7810909 ]
 ```

在上面的代码中，Lasso的正则化参数𝛼设为 1.0。肯定有些读者会很好奇这个$\alpha$值如何取？

选择合适的正则化强度 $ \alpha $ 是 Lasso 回归中一个关键步骤。过大的 $ \alpha $ 值可能会导致模型过于简单，忽略重要特征；过小的 $ \alpha $ 值则可能导致模型过拟合。常用的方法包括交叉验证和路径算法。

Scikit-learn 提供了 LassoCV 类，可以通过交叉验证来选择最佳的 $ \alpha $ 值。下面是一个示例：

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LassoCV
from sklearn.metrics import mean_squared_error
from tensorflow.keras.datasets import boston_housing

# 加载波士顿房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 使用 LassoCV 进行交叉验证选择最优的 alpha
lasso_cv = LassoCV(cv=5, random_state=0)
lasso_cv.fit(X_train, y_train)

# 输出最优的 alpha 值
print(f'最优的 alpha 值: {lasso_cv.alpha_}')

# 使用最优的 alpha 值进行预测
y_train_pred = lasso_cv.predict(X_train)
y_test_pred = lasso_cv.predict(X_test)

# 计算均方误差
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f'训练集均方误差: {train_mse}')
print(f'测试集均方误差: {test_mse}')
```

输出结果如下：

```
最优的 alpha 值: 0.6859344132928149
训练集均方误差: 25.85262694347411
测试集均方误差: 23.277920133940906
```

#### 5.2.2 最小角回归

下面我们再介绍一下最小角回归Least Angle Regression (LARS)。LARS是一种用于高维数据的回归技术，特别适用于当特征数量大于样本数量时。LARS 在处理稀疏模型（即很多特征的回归系数为零）方面表现出色，并且计算效率高。它的主要优点是在模型构建过程中可以逐步引入和删除特征，从而找到最优的模型参数。

LARS 算法的思想是逐步逼近最优解，每次迭代选择与当前残差最相关的特征，沿着该特征的方向移动，直到另一个特征与残差的相关性相同。具体步骤如下：

1. 初始化：

    - 所有回归系数 $\beta$ 初始化为零。
    - 计算所有特征与响应变量的相关性，选择与响应变量最相关的特征。

2. 向前方向搜索：沿着选择的特征方向向前移动，即增加该特征的回归系数，直到另一个特征与当前残差的相关性相同。此步骤确保同时考虑多个特征对响应变量的贡献。

3. 更新方向：

    - 选择新的方向，使得模型能够同时考虑当前选择的多个特征，并沿着这个新的方向继续移动。
    - 重复上述步骤，每次迭代引入一个新的特征，直到所有特征都被纳入模型或达到某个停止条件（如残差足够小）。

4. 停止规则：当所有特征都已被纳入模型，或达到预设的停止条件时，算法停止。


LARS算法不需要指定参数。我们来看用最小角回归来预测波士顿房价：

```python
import numpy as np
from sklearn.linear_model import Lars
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.datasets import boston_housing

# 加载波士顿房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 使用 LARS 进行模型训练
lars = Lars()
lars.fit(X_train, y_train)

# 进行预测
y_train_pred = lars.predict(X_train)
y_test_pred = lars.predict(X_test)

# 计算均方误差
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f'训练集均方误差: {train_mse}')
print(f'测试集均方误差: {test_mse}')
```

输出结果如下：

```
训练集均方误差: 22.085431828483273
测试集均方误差: 24.25006601308819
```

我们还可以将Lasso和LARS结合在一起，使用LassoLars模型。LassoLars是一种结合了Lasso回归和LARS算法的方法，可以在高维数据集上实现特征选择和模型稀疏化。我们来看如何用LassoLars来预测波士顿房价：

```python
import numpy as np
from sklearn.linear_model import LassoLars
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.datasets import boston_housing

# 加载波士顿房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 使用 LARS Lasso 进行模型训练
lasso_lars = LassoLars()
lasso_lars.fit(X_train, y_train)

# 进行预测
y_train_pred = lasso_lars.predict(X_train)
y_test_pred = lasso_lars.predict(X_test)

# 计算均方误差
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f'训练集均方误差: {train_mse}')
print(f'测试集均方误差: {test_mse}')
```

输出结果如下：

```
训练集均方误差: 27.781703889808558
测试集均方误差: 25.82293972655321
```

#### 5.2.3 LassoLarsCV

我们继续5.2.1节讨论的Lasso回归的$\alpha$值的问题。LassoLarsCV是一种用于选择 Lasso 回归中最优正则化强度 $\alpha$ 的方法，结合了上节介绍的LARS算法与交叉验证技术。它的全称是 Lasso Least Angle Regression with Cross-Validation。

LassoLarsCV 结合了 LARS 算法和交叉验证技术，通过以下步骤找到最优的 $\alpha$ 值：

1. 路径生成：使用 LARS 算法生成一条路径，即不同 $\alpha$ 值下的模型解。这条路径对应于不同的正则化强度，从而得到不同的模型稀疏度。
2. 交叉验证：
    - 对每个 $\alpha$ 值，使用交叉验证技术评估模型性能。具体来说，将数据集划分为 $ k $ 个子集，交替使用 $ k-1 $ 个子集进行训练，剩下的一个子集进行验证。
    - 计算每个 $\alpha$ 值下的交叉验证误差。
3. 选择最优 $\alpha$：选择交叉验证误差最小的 $\alpha$ 值作为最优的正则化强度。

我们来看如何用LassoLarsCV来预测波士顿房价时取最优的$\alpha$值：

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LassoLarsCV
from sklearn.metrics import mean_squared_error
from keras.datasets import boston_housing

# 加载波士顿房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 使用 LassoLarsCV 进行路径算法选择最优的 alpha
lasso_lars_cv = LassoLarsCV(cv=5)
lasso_lars_cv.fit(X_train, y_train)

# 输出最优的 alpha 值
print(f'最优的 alpha 值: {lasso_lars_cv.alpha_}')

# 使用最优的 alpha 值进行预测
y_train_pred = lasso_lars_cv.predict(X_train)
y_test_pred = lasso_lars_cv.predict(X_test)

# 计算均方误差
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f'训练集均方误差: {train_mse}')
print(f'测试集均方误差: {test_mse}')
```

输出结果如下：

```
最优的 alpha 值: 0.0
训练集均方误差: 22.004800838348142
测试集均方误差: 23.195599256423414
```

#### 5.2.4 多任务Lasso

多任务Lasso是一种扩展的 Lasso 回归方法，用于同时处理多个相关回归任务。在这种方法中，我们假设不同的任务共享相似的稀疏模式，即它们具有相似的非零系数位置。通过引入这种假设，多任务Lasso 能够更有效地利用数据的结构信息，提升模型的预测性能。

多任务Lasso 将 Lasso 的思想扩展到多任务场景。假设我们有 $ K $ 个回归任务，每个任务都有自己的响应变量，但它们共享相同的特征矩阵 $X$。多任务Lasso 的目标函数可以表示为：

$\min_{W} \left( \frac{1}{2n} \sum_{k=1}^K \sum_{i=1}^n (y_{ik} - X_i w_k)^2 + \alpha \sum_{j=1}^p \|W_j\|_2 \right)$

其中：
- $W$ 是回归系数矩阵，每列 $w_k$ 对应一个任务的回归系数。
- $\|W_j\|_2$ 是矩阵 $W$ 的第 $j$ 行的 $L2$ 范数，即 $j$ 特征在所有任务中的系数的平方和的平方根。

多任务Lasso 假设所有任务的稀疏模式是共享的，即如果某个特征对一个任务的重要性较大，那么它对其他任务也可能重要。
    
通过共享稀疏模式，Multi-task Lasso 能够更好地利用数据的结构信息，特别是在任务之间存在相关性的情况下，可以提升模型的预测性能。
    
多任务Lasso使用 $ L2 $ 范数对每个特征在所有任务中的系数进行正则化，从而控制模型的复杂性，避免过拟合。

我们来看多任务Lasso的用法：

```python
import numpy as np
from sklearn.linear_model import MultiTaskLasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 生成模拟数据
n_samples, n_features, n_tasks = 100, 20, 3
X = np.random.randn(n_samples, n_features)
W = np.random.randn(n_features, n_tasks)
Y = np.dot(X, W) + np.random.randn(n_samples, n_tasks)

# 分割数据集
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# 使用 Multi-task Lasso 进行模型训练
multi_task_lasso = MultiTaskLasso(alpha=0.1)
multi_task_lasso.fit(X_train, Y_train)

# 进行预测
Y_train_pred = multi_task_lasso.predict(X_train)
Y_test_pred = multi_task_lasso.predict(X_test)

# 计算均方误差
train_mse = mean_squared_error(Y_train, Y_train_pred)
test_mse = mean_squared_error(Y_test, Y_test_pred)

print(f'训练集均方误差: {train_mse}')
print(f'测试集均方误差: {test_mse}')
```

输出结果如下：

```
训练集均方误差: 0.8263506167661774
测试集均方误差: 1.1121606407888283
```

### 5.3 岭回归

岭回归(Ridge Regression)，是一种在普通最小二乘法（Ordinary Least Squares, OLS）回归的基础上引入 $L2$ 正则化的线性回归方法。它通过对回归系数施加惩罚，控制模型复杂性，从而减小过拟合的风险，特别是在存在多重共线性的情况下。上节我们介绍的Lasso回归是$L1$正则化。

岭回归的目标函数如下：$\min_{\beta} \left( \frac{1}{2n} \sum_{i=1}^n (y_i - X_i \beta)^2 + \alpha \|\beta\|_2^2 \right)$

其中：
- $y_i$ 是第 $i$ 个样本的真实值。
- $X_i$ 是第 $i$ 个样本的特征向量。
- $\beta$ 是回归系数向量。
- $n$ 是样本数量。
- $\alpha$ 是正则化参数，控制正则化项的强度。
- $\|\beta\|_2^2$ 是回归系数的 $L2$ 范数的平方，即所有系数的平方和。

岭回归的主要特点有：

- 处理多重共线性：多重共线性是指特征矩阵中的特征彼此高度相关，这会导致 OLS 回归系数估计不稳定。岭回归通过引入$L2$正则化，增加对回归系数的约束，从而稳定估计。
- 减少模型复杂性：正则化项 $\alpha \|\beta\|_2^2$ 使得回归系数更加保守，减少了模型的复杂性，防止过拟合。
- 不会产生稀疏解：Lasso 不同，岭回归不会将回归系数缩减为零。因此，它不会进行特征选择，而是减小所有系数值。

下面我们看看如何用岭回归来预测波士顿房价：

```python
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.datasets import boston_housing

# 加载 Boston 房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 数据标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 使用 Ridge Regression 进行模型训练
ridge = Ridge(alpha=1.0)  # alpha 控制正则化强度
ridge.fit(X_train, y_train)

# 进行预测
y_train_pred = ridge.predict(X_train)
y_test_pred = ridge.predict(X_test)

# 计算均方误差
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f'训练集均方误差: {train_mse}')
print(f'测试集均方误差: {test_mse}')
```

输出结果如下：

```
训练集均方误差: 22.00668625632674
测试集均方误差: 23.105483040456598
```

同样，我们面临如何选择最优的 $\alpha$ 值的问题。这里我们学习一种新的方法，叫做只留交叉验证（Leave-One-Out Cross-Validation, LOOCV）。只留交叉验证是一种特殊的交叉验证技术，在这种方法中，每次只留出一个样本作为验证集，其余的样本用来训练模型。这种方法对数据集中的每个样本都重复一次，最终的模型性能通过所有验证集的结果综合得出。

只留交叉验证的迭代过程为：

- 对于数据集中的每个样本：
    1. 将该样本作为验证集。
    2. 其余的样本作为训练集。
    3. 使用训练集训练模型。
    4. 用验证集评估模型性能，记录误差或评分。

假设数据集有 $n$ 个样本 $( X_1, y_1 ), ( X_2, y_2 ), \ldots, ( X_n, y_n )$。

1. 对于第$i$ 个样本：
    - 训练集：$\{( X_1, y_1 ), \ldots, ( X_{i-1}, y_{i-1} ), ( X_{i+1}, y_{i+1} ), \ldots, ( X_n, y_n )\}$
    - 验证集：$( X_i, y_i )$
2. 训练模型并计算在验证集上的误差 $e_i$。
3. 最终误差为所有 $e_i$ 的平均值：$\text{LOOCV Error} = \frac{1}{n} \sum_{i=1}^n e_i$

scikit learn框架为提供了 LeaveOneOut 类来实现只留交叉验证：

```python
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.model_selection import LeaveOneOut
from sklearn.metrics import mean_squared_error
from keras.datasets import boston_housing
from sklearn.preprocessing import StandardScaler

# 加载 Boston 房价数据集
(X, y), _ = boston_housing.load_data()

# 数据标准化
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 定义 Ridge 回归模型
model = Ridge(alpha=1.0)

# 定义 LOOCV
loo = LeaveOneOut()

mse_list = []

# 进行 LOOCV
for train_index, test_index in loo.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    mse = mean_squared_error(y_test, y_pred)
    mse_list.append(mse)

# 计算平均均方误差
average_mse = np.mean(mse_list)

print(f'LOOCV 平均均方误差: {average_mse}')
```

输出的结果为：

```
LOOCV 平均均方误差: 24.249383742216512
```

同样，scikit-learn也提供了`RidgeCV`类，通过交叉验证自动选择最佳的正则化参数 $\alpha$。用户可以提供一组候选的 $\alpha$ 值，`RidgeCV` 会在这些值中选择一个使得交叉验证误差最小的 $\alpha$。

我们来看一个例子：

```python
import numpy as np
from sklearn.linear_model import RidgeCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.datasets import boston_housing

# 加载 Boston 房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 数据标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 定义 RidgeCV 模型，并指定候选的 alpha 值
alphas = [0.1, 1.0, 10.0, 100.0]
ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)

# 训练模型
ridge_cv.fit(X_train, y_train)

# 获取最佳的 alpha 值
best_alpha = ridge_cv.alpha_
print(f'最佳的 alpha 值: {best_alpha}')

# 进行预测
y_train_pred = ridge_cv.predict(X_train)
y_test_pred = ridge_cv.predict(X_test)

# 计算均方误差
train_mse = mean_squared_error(y_train, y_train_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print(f'训练集均方误差: {train_mse}')
print(f'测试集均方误差: {test_mse}')
```

### 5.4 核岭回归

Kernel Ridge Regression（核岭回归）是一种结合了核方法和岭回归（Ridge Regression）的回归技术。它在处理非线性数据和高维数据时非常有效。

核岭回归结合了核方法和岭回归，通过在核函数计算的高维特征空间中进行岭回归。其目标函数如下：$\text{Loss} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{i=1}^{n} \alpha_i^2$

其中，$\alpha_i$ 是模型参数，$\lambda$ 是正则化参数。

我们来看一个核岭回归的例子：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# 生成一些非线性数据
np.random.seed(0)
X = 5 * np.random.rand(100, 1)
y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建核岭回归模型
model = KernelRidge(kernel='rbf', alpha=1.0, gamma=0.1)

# 训练模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 可视化结果
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='red', label='Actual')
plt.scatter(X_test, y_pred, color='blue', label='Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Kernel Ridge Regression')
plt.legend()
plt.show()

# 打印模型的性能
print(f'Mean squared error: {mean_squared_error(y_test, y_pred):.2f}')
print(f'R^2 score: {r2_score(y_test, y_pred):.2f}')
```

运行结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/kernel_ridge_regression.png)

下面我们用核岭回归来预测波士顿房价：

```python
import numpy as np
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.datasets import boston_housing
import matplotlib.pyplot as plt

# 加载波士顿房价数据集
(X_train_full, y_train_full), (X_test, y_test) = boston_housing.load_data()

# 分割训练集为训练集和验证集
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)

# 特征标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# 创建核岭回归模型
model = KernelRidge(kernel='rbf', alpha=1.0, gamma=0.1)

# 训练模型
model.fit(X_train, y_train)

# 预测验证集
y_val_pred = model.predict(X_val)

# 预测测试集
y_test_pred = model.predict(X_test)

# 打印验证集性能
print(f'Validation Mean Squared Error: {mean_squared_error(y_val, y_val_pred):.2f}')
print(f'Validation R^2 Score: {r2_score(y_val, y_val_pred):.2f}')

# 打印测试集性能
print(f'Test Mean Squared Error: {mean_squared_error(y_test, y_test_pred):.2f}')
print(f'Test R^2 Score: {r2_score(y_test, y_test_pred):.2f}')

# 可视化结果
plt.figure(figsize=(10, 6))

# 验证集结果
plt.subplot(1, 2, 1)
plt.scatter(y_val, y_val_pred, color='blue', label='Predicted vs Actual')
plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], color='red', linestyle='--', label='Ideal')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Validation Set')
plt.legend()

# 测试集结果
plt.subplot(1, 2, 2)
plt.scatter(y_test, y_test_pred, color='green', label='Predicted vs Actual')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Ideal')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Test Set')
plt.legend()

plt.tight_layout()
plt.show()
```

简单解释下上面的代码：

- 加载数据：我们使用Keras中的boston_housing数据集，并将其分为训练集和测试集。
- 分割训练集：将训练集进一步分割为训练集和验证集。
- 特征标准化：使用StandardScaler对特征进行标准化处理，以提高模型性能。
- 创建模型：使用scikit-learn的KernelRidge类创建核岭回归模型，并指定核函数（RBF核）、正则化参数 α 和核函数参数 γ。
- 训练模型：使用训练集数据训练模型。
- 预测：对验证集和测试集进行预测。
- 打印性能指标：输出验证集和测试集的均方误差（Mean Squared Error）和R²得分（R^2 Score）。
- 可视化结果：绘制实际值和预测值的散点图，以便可视化模型的效果。

输出的结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/kernel_boston.png)

### 5.5 决策树回归

决策树通过将特征空间划分为多个区域来拟合数据。决策树回归的目标是最小化每个区域内的平方误差。

其过程跟决策树分类类似，不过在叶节点上的预测值是区域内所有样本的平均值。

- 分裂节点：从根节点开始，选择一个特征及其阈值，将数据集划分成两个子集。选择的标准是使得划分后的子集在目标变量上的误差最小化（如均方误差）。
- 递归划分：对每个子集重复上述过程，继续选择特征和阈值进行划分，直到满足停止条件（如达到最大深度或叶节点样本数小于某个阈值）。
- 预测：对于新的输入数据，从根节点开始，根据决策树的分裂规则，沿着树的路径向下遍历，直到到达叶节点。叶节点的值即为预测值。

DecisionTreeRegressor的主要参数有：

- criterion：用于衡量分裂质量的函数，常用的是 `mse`（均方误差）和 `friedman_mse`。
- max_depth：树的最大深度，防止过拟合。
- min_samples_split：每个内部节点（非叶节点）划分所需的最小样本数。
- min_samples_leaf：叶节点所需的最小样本数。
- max_features：用于最佳分裂的特征数量。
- random_state：控制树分裂时的随机性。

我们来看一下决策树回归的用法：

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

import matplotlib as mpl

zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/truetype/liberation/simhei.ttf')
plt.rcParams['axes.unicode_minus'] = False


# 生成示例数据
np.random.seed(0)
X = np.sort(5 * np.random.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 1 * (0.5 - np.random.rand(16))  # 添加噪声

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# 创建并拟合决策树回归模型
regr = DecisionTreeRegressor(max_depth=3, random_state=0)
regr.fit(X_train, y_train)

# 进行预测
y_pred = regr.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print(f'均方误差: {mse}')

# 可视化决策树回归效果
import matplotlib.pyplot as plt

X_grid = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_grid_pred = regr.predict(X_grid)

plt.figure()
plt.scatter(X, y, s=20, edgecolor="black", c="darkorange", label="Data")
plt.plot(X_grid, y_grid_pred, color="cornflowerblue", label="Pred", linewidth=2)
plt.xlabel("数据",fontproperties=zhfont)
plt.ylabel("目标",fontproperties=zhfont)
plt.title("决策树回归",fontproperties=zhfont)
plt.legend()
plt.show()
```

运行结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/decison_tree_reg.png)

下面我们就用决策树回归来预测波士顿房价：

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from keras.datasets import boston_housing

import matplotlib as mpl

zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/truetype/liberation/simhei.ttf')
plt.rcParams['axes.unicode_minus'] = False


# 加载波士顿房价数据集
(X_train_full, y_train_full), (X_test, y_test) = boston_housing.load_data()

# 划分训练集和验证集
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=0)

# 特征缩放
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# 创建并训练决策树回归模型
regr = DecisionTreeRegressor(max_depth=5, random_state=0)
regr.fit(X_train_scaled, y_train)

# 在验证集上进行预测
y_val_pred = regr.predict(X_val_scaled)
val_mse = mean_squared_error(y_val, y_val_pred)
print(f'验证集均方误差: {val_mse}')

# 在测试集上进行预测
y_test_pred = regr.predict(X_test_scaled)
test_mse = mean_squared_error(y_test, y_test_pred)
print(f'测试集均方误差: {test_mse}')

# 可视化验证集预测结果
plt.figure(figsize=(10, 6))
plt.scatter(y_val, y_val_pred, color='blue', edgecolor='k', alpha=0.6)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'k--', lw=2)
plt.xlabel('真实值',fontproperties=zhfont)
plt.ylabel('预测值',fontproperties=zhfont)
plt.title('决策树回归 - 验证集',fontproperties=zhfont)
plt.show()

# 可视化测试集预测结果
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, color='green', edgecolor='k', alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('真实值',fontproperties=zhfont)
plt.ylabel('预测值',fontproperties=zhfont)
plt.title('决策树回归 - 测试集',fontproperties=zhfont)
plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/decision_tree_boston.png)


### 5.6 k近邻回归

k近邻算法也同样可以用于回归。使用k近邻算法实现回归需要以下三个步骤：

1. 距离度量：
    - 首先，需要定义一个距离度量来计算数据点之间的相似性。常见的距离度量包括欧氏距离（Euclidean Distance）、曼哈顿距离（Manhattan Distance）、闵可夫斯基距离（Minkowski Distance）和余弦相似度（Cosine Similarity）。
    - 对于两个数据点 $ \mathbf{x}_i $ 和 $ \mathbf{x}_j $，欧氏距离计算公式为：
      $d(\mathbf{x}_i, \mathbf{x}_j) = \sqrt{\sum_{k=1}^{n} (x_{ik} - x_{jk})^2}$

2. 寻找最近邻：
    - 给定一个新的数据点 $ \mathbf{x} $，计算它与训练集中所有数据点的距离。
    - 根据这些距离，找到与 $ \mathbf{x} $ 距离最近的 $ k $ 个数据点（即最近邻）。

3. 预测目标值：
    - 对于回归问题，通常使用这 $ k $ 个最近邻的数据点的目标值的平均值来作为预测值。即，如果 $ \mathcal{N}_k(\mathbf{x}) $ 表示 $ \mathbf{x} $ 的 $ k $ 个最近邻的数据点集合，则预测值 $ \hat{y} $ 为：
      $
      \hat{y} = \frac{1}{k} \sum_{i \in \mathcal{N}_k(\mathbf{x})} y_i
      $
    - 也可以使用加权平均的方法，即根据距离的倒数来加权邻居的目标值，距离越近权重越大。

我们来看一个k近邻回归的例子：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score

# 生成一些非线性数据
np.random.seed(0)
X = 5 * np.random.rand(100, 1)
y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 特征标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 创建k-最近邻回归模型
k = 5
model = KNeighborsRegressor(n_neighbors=k)

# 训练模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 可视化结果
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='red', label='Actual')
plt.scatter(X_test, y_pred, color='blue', label='Predicted')
plt.xlabel('X')
plt.ylabel('y')
plt.title(f'k-Nearest Neighbors Regression (k={k})')
plt.legend()
plt.show()

# 打印模型的性能
print(f'Mean squared error: {mean_squared_error(y_test, y_pred):.2f}')
print(f'R^2 score: {r2_score(y_test, y_pred):.2f}')
```

输出结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/knn_regression.png)

好，我们再用k近邻回归来预测波士顿房价：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.datasets import boston_housing

# 加载Keras中的波士顿房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 特征标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 创建k-最近邻回归模型
k = 5
model = KNeighborsRegressor(n_neighbors=k)

# 训练模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 打印模型的性能
print(f'Mean squared error: {mean_squared_error(y_test, y_pred):.2f}')
print(f'R^2 score: {r2_score(y_test, y_pred):.2f}')

# 可视化结果
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title(f'k-Nearest Neighbors Regression (k={k})')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.legend()
plt.show()
```

输出结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/knn_boston.png)


### 5.7 高斯过程回归

高斯过程回归（GPR）是一种非参数的贝叶斯回归方法，其核心思想是利用高斯过程（Gaussian Process, GP）来进行函数拟合和预测。GP 是一个定义在输入空间上的随机过程，每个输入点对应一个高斯分布的输出。GPR 通过这种方式，可以在不明确假设函数形式的前提下，对数据进行回归分析。

高斯过程是指对于任何有限个输入点，其对应的输出值服从多元高斯分布。GP 可以被认为是无限维度的高斯分布，定义为：
$f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))$

其中，$m(\mathbf{x})$ 是均值函数，$k(\mathbf{x}, \mathbf{x}')$ 是协方差函数（或核函数）。

高斯过程回归可以分为训练和预测两个阶段：

1. 训练阶段：
    - 给定训练数据集 $\{\mathbf{X}, \mathbf{y}\}$，其中 $\mathbf{X}$ 是输入矩阵，$\mathbf{y}$ 是输出向量。假设训练数据的输出值服从多元高斯分布：
      $\mathbf{y} \sim \mathcal{N}(\mathbf{0}, \mathbf{K} + \sigma_n^2 \mathbf{I})$
      其中，$\mathbf{K}$ 是由核函数计算得到的协方差矩阵，$\sigma_n^2$ 是噪声的方差。

2. 预测阶段：
    - 给定新的输入点 $\mathbf{x}_*$，构造联合分布：
      $
      \begin{bmatrix}
      \mathbf{y} \\
      f(\mathbf{x}_*)
      \end{bmatrix} \sim \mathcal{N}\left( \mathbf{0}, \begin{bmatrix}
      \mathbf{K} + \sigma_n^2 \mathbf{I} & \mathbf{k}_* \\
      \mathbf{k}_*^\top & k(\mathbf{x}_*, \mathbf{x}_*)
      \end{bmatrix} \right)
      $
      其中，$\mathbf{k}_*$ 是训练数据与新输入点的协方差向量。

    - 根据多元高斯分布的条件分布性质，可以得到预测分布：
      $
      f(\mathbf{x}_*) | \mathbf{X}, \mathbf{y}, \mathbf{x}_* \sim \mathcal{N}(\mu_*, \sigma_*^2)
      $
      其中，均值和方差分别为：
      $
      \mu_* = \mathbf{k}_*^\top (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{y}
      $
      $
      \sigma_*^2 = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^\top (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{k}_*
      $

下面我们来看如何用高斯过程回归来处理波士顿房价数据集：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from keras.datasets import boston_housing

# 加载Keras中的波士顿房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 特征标准化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 定义核函数
# 使用常量核和RBF核的乘积
kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 1e3))

# 创建高斯过程回归模型
gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-2)

# 训练模型
gpr.fit(X_train, y_train)

# 预测
y_pred, y_std = gpr.predict(X_test, return_std=True)

# 打印模型的性能
print(f'Mean squared error: {mean_squared_error(y_test, y_pred):.2f}')
print(f'R^2 score: {r2_score(y_test, y_pred):.2f}')

# 可视化结果
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')
plt.errorbar(y_test, y_pred, yerr=y_std, fmt='o', color='blue', alpha=0.5, label='Uncertainty')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Gaussian Process Regression')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.legend()
plt.show()
```

简单解释一下代码：

- 加载波士顿房价数据集：使用keras.datasets中的boston_housing.load_data()函数加载数据集。
- 数据标准化：使用StandardScaler对特征进行标准化，以确保不同特征在同一尺度上。
- 定义核函数：使用常量核（ConstantKernel）和RBF（径向基函数）核的乘积。常量核用来表示整体的幅度，RBF核用来表示输入点之间的相似性。
- 创建GPR模型：使用GaussianProcessRegressor创建高斯过程回归模型，n_restarts_optimizer参数设置为10，用于多次优化核参数以找到最优值，alpha参数用于添加噪声项。
- 训练模型：使用训练数据训练GPR模型。
- 预测：使用测试数据进行预测，同时返回预测的不确定性（标准差）。
- 性能评估：打印均方误差（MSE）和决定系数（R²）来评估模型性能。
- 可视化结果：绘制预测值与实际值的散点图，并显示预测的不确定性范围。添加一条理想情况下的参考线（y=x）。

输出结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gaussian_process_boston.png)

### 5.8 交叉分解

交叉分解(Cross Decomposition)是用于分析两组变量之间关系的一组技术。通过找到描述它们之间共享信息的潜在结构，这些技术在有两组数据矩阵并希望了解它们之间变量关系的场景中非常有用。

在交叉分解中，常见的方法包括典型相关分析（Canonical Correlation Analysis, CCA）和偏最小二乘法（Partial Least Squares, PLS）。这些方法在化学计量学、基因组学、经济学等领域得到了广泛应用，因为在这些领域中，理解不同类型数据之间的相互作用是至关重要的。

典型相关分析CCA 是一种用于理解两组多维变量之间关系的方法。它找到每个数据集中变量的线性组合，这些组合之间的相关性最大。
典型相关分析目标是找到成对的典型变量 $ (u_i, v_i) $，使得 $ u_i $ 和 $ v_i $ 之间的相关性最大化。

偏最小二乘法PLS 是一种将预测变量和响应变量投影到新空间以最大化它们之间协方差的方法。当预测变量多于观测值且预测变量高度共线时，PLS 特别有用。

我们来看如何使用偏最小二乘法做回归：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib as mpl

zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/truetype/liberation/simhei.ttf')
plt.rcParams['axes.unicode_minus'] = False

# 生成合成数据
np.random.seed(0)
X = np.random.normal(size=(100, 10))
Y = np.random.normal(size=(100, 3))

# 划分训练集和测试集
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# 创建并训练 PLS 模型
pls = PLSRegression(n_components=2)
pls.fit(X_train, Y_train)

# 在测试数据上进行预测
Y_pred = pls.predict(X_test)

# 评估模型
mse = mean_squared_error(Y_test, Y_pred)
print(f'均方误差: {mse:.2f}')

# 绘制结果
plt.figure(figsize=(10, 6))
plt.scatter(Y_test[:, 0], Y_pred[:, 0], label='1')
plt.scatter(Y_test[:, 1], Y_pred[:, 1], label='2')
plt.plot([-3, 3], [-3, 3], '--k', linewidth=2)
plt.xlabel('真实值',fontproperties=zhfont)
plt.ylabel('预测值',fontproperties=zhfont)
plt.title('PLS 回归',fontproperties=zhfont)
plt.legend()
plt.show()
```

偏最小二乘典型相关分析PLSCanonical是偏最小二乘法（Partial Least Squares, PLS）的一种变体，它结合了偏最小二乘回归和典型相关分析（Canonical Correlation Analysis, CCA）的特点。PLSCanonical 尤其适用于处理两组变量之间的多重关系，旨在通过寻找能够最大化两组变量之间协方差的低维表示来揭示它们的潜在结构。

与 PLS 回归类似，PLSCanonical 通过将原始数据投影到一个新的潜在空间中来工作。它试图同时找到两个数据集之间的线性组合，这些组合能够最大限度地解释两个数据集之间的协方差。具体来说，PLSCanonical 寻找一对向量 $u$ 和 $v$，使得 $X$ 和 $Y$ 的投影 $X u$ 和 $Y v$ 之间的协方差最大。

具体做法可以分为下面几步：

1. 数据标准化：对数据进行标准化处理，使得每个变量的均值为 0，方差为 1。
2. 计算协方差矩阵：计算两组变量之间的协方差矩阵。
3. 找到潜在变量：找到一对向量 $u$ 和 $v$，使得投影 $X u$ 和 $Y v$ 的协方差最大。
4. 重复步骤：继续寻找下一对潜在变量，直到达到预定的潜在变量数量。

下面我们来看一个PLSCanonical的例子：

```python
import numpy as np
from sklearn.cross_decomposition import PLSCanonical
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

# 生成合成数据
np.random.seed(0)
X = np.random.normal(size=(100, 10))
Y = np.random.normal(size=(100, 3))

# 数据标准化
scaler_X = StandardScaler()
scaler_Y = StandardScaler()
X = scaler_X.fit_transform(X)
Y = scaler_Y.fit_transform(Y)

# 划分训练集和测试集
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# 创建并训练 PLSCanonical 模型
plsc = PLSCanonical(n_components=2)
plsc.fit(X_train, Y_train)

# 在测试数据上进行预测
Y_pred = plsc.predict(X_test)

# 评估模型
mse = mean_squared_error(Y_test, Y_pred)
print(f'均方误差: {mse:.2f}')
```

另外，PLSSVD（Partial Least Squares Singular Value Decomposition，偏最小二乘奇异值分解）是一种偏最小二乘回归（Partial Least Squares, PLS）的变体，结合了偏最小二乘法和奇异值分解（Singular Value Decomposition, SVD）的特点。PLSSVD 通过同时分解两个数据矩阵，找到它们之间的最大协方差方向，旨在构建更稳健的回归模型。

奇异值分解 (Singular Value Decomposition, SVD)是一种在线性代数中广泛应用的矩阵分解技术。它将一个矩阵分解为三个特定矩阵的乘积，用于揭示原始矩阵的许多重要属性和结构。

对于一个给定的矩阵 $A$（大小为 $ m \times n $），SVD 将其分解为三个矩阵的乘积：
$A = U \Sigma V^T$

其中：

- $U$ 是一个 $m \times m$ 的正交矩阵，称为左奇异向量矩阵。
- $\Sigma$ 是一个 $m \times n$ 的对角矩阵，称为奇异值矩阵，其对角线上的元素是奇异值，按降序排列。
- $V$ 是一个 $n \times n$ 的正交矩阵，称为右奇异向量矩阵，$V^T$ 是 $V$ 的转置矩阵。

PLSSVD 的主要目的是通过奇异值分解技术，找到一对投影矩阵，使得两个数据集之间的投影具有最大的协方差。它通过以下步骤实现：

1. 标准化数据：对数据进行标准化处理，使每个变量的均值为 0，方差为 1。这可以通过StandardScaler类来实现。
2. 计算协方差矩阵：计算两个数据集之间的协方差矩阵。
3. 奇异值分解：对协方差矩阵进行奇异值分解，找到最重要的奇异向量。
4. 投影到低维空间：将原始数据投影到由奇异向量定义的低维空间，生成潜在变量。

我们来看一个PLSSVD的例子：

```python
import numpy as np
from sklearn.cross_decomposition import PLSSVD
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 生成合成数据
np.random.seed(0)
X = np.random.normal(size=(100, 10))
Y = np.random.normal(size=(100, 3))

# 数据标准化
scaler_X = StandardScaler()
scaler_Y = StandardScaler()
X = scaler_X.fit_transform(X)
Y = scaler_Y.fit_transform(Y)

# 划分训练集和测试集
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# 创建并训练 PLSSVD 模型
plssvd = PLSSVD(n_components=2)
plssvd.fit(X_train, Y_train)

# 在训练数据上进行变换以获取潜在变量
X_train_scores, Y_train_scores = plssvd.transform(X_train, Y_train)

# 在测试数据上进行变换以获取潜在变量
X_test_scores, Y_test_scores = plssvd.transform(X_test, Y_test)

# 输出潜在变量
print(f'X 潜在变量 (训练数据投影):\n{X_train_scores}')
print(f'Y 潜在变量 (训练数据投影):\n{Y_train_scores}')
print(f'X 潜在变量 (测试数据投影):\n{X_test_scores}')
print(f'Y 潜在变量 (测试数据投影):\n{Y_test_scores}')
```

小结一下，交叉分解的4个类：
- PLSCanonical：用于寻找两个数据集之间的典型相关关系。
- PLSRegression：用于回归分析，找到预测变量和响应变量之间的线性关系。
- PLSSVD：结合了偏最小二乘和奇异值分解，最大化两个数据集之间的协方差。
- CCA：用于寻找两个多维变量集合之间的最大相关性。

### 5.9 学会使用高级工具

在实际工作中，我们通常会使用一些高级工具来帮助我们更快地完成工作。比如，我们可以使用Pipeline来将多个步骤组合在一起，使用GridSearchCV来进行超参数搜索，使用RandomizedSearchCV来进行随机搜索等。

更进一步，我们可以使用一些更高级的库来更高效完全特定的任务。比如，时间序列预测可以使用Meta开源的Prophet库。

这像调用sklearn一样简单，但是Prophet是专门为时间序列预测而设计的，它可以自动处理节假日效应、趋势变化、季节性等。

比如我们用Prophet来预测波士顿房价数据集：

```python
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.datasets import boston_housing
from prophet import Prophet

# 加载 Boston 房价数据集
(X_train, y_train), (X_test, y_test) = boston_housing.load_data()

# 为了示范，将训练集和测试集的目标变量合并，并创建一个日期索引
y = np.concatenate([y_train, y_test])
dates = pd.date_range(start='2000-01-01', periods=len(y), freq='M')  # 生成日期索引

# 创建 DataFrame，Prophet 需要列名为 'ds' 和 'y'
df = pd.DataFrame({'ds': dates, 'y': y})

# 初始化 Prophet 模型
model = Prophet()

# 训练模型
model.fit(df)

# 生成未来的日期
future = model.make_future_dataframe(periods=12, freq='M')  # 预测未来 12 个月的数据

# 进行预测
forecast = model.predict(future)

# 打印预测结果
print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])

# 可视化预测结果
fig = model.plot(forecast)
```

forecast DataFrame 包含了预测的房价值及其上下置信区间。通过可视化，我们可以直观地看到模型的预测结果及其不确定性范围。

绘制的结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/prophet_boston.png)

下面我们以预测阿里巴巴股票价格为例，看看时间序列如何使用Prophet来预测：

```python
import pandas as pd
import yfinance as yf
from prophet import Prophet
import matplotlib.pyplot as plt

# 获取股价数据
ticker = 'BABA'  # 以阿里巴巴公司为例
data = yf.download(ticker, start='2015-01-01', end='2024-06-16')

# 准备数据
df = data.reset_index()[['Date', 'Close']]
df.columns = ['ds', 'y']  # Prophet 需要列名为 'ds' 和 'y'

# 初始化 Prophet 模型
model = Prophet()

# 训练模型
model.fit(df)

# 生成未来的日期
future = model.make_future_dataframe(periods=365)  # 预测未来 365 天的数据

# 进行预测
forecast = model.predict(future)

# 打印预测结果
print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())

# 可视化预测结果
fig1 = model.plot(forecast)
plt.title(f'{ticker} Stock Price Prediction')
plt.xlabel('Date')
plt.ylabel('Price')
plt.show()

# 可视化预测成分
fig2 = model.plot_components(forecast)
plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/prophet_stock_baba.png)


## 第六章 非监督学习

在监督学习中，我们给算法提供了许多带有标签的例子，这些标签告诉算法每个例子是什么，帮助它学习如何识别新的数据。而在非监督学习中，我们给算法的数据没有标签，算法需要自己找出数据之间的模式和关系。比如，我们有一堆动物的图片，但是我们不知道它们属于哪种动物，我们就可以让非监督学习算法帮我们把相似的动物归为一类。

非监督学习可以帮助我们发现数据中的新信息和模式，从而提供更全面的数据分析和决策支持。它可以帮助我们发现新的见解和发现，而不仅仅是根据已有的标签进行预测。

### 6.1 聚类

我们在日常生活中，经常会给周围的事物进行分类。比如我们会把人群分为高个子的人和矮个子的人，具体标准是多少，完全看这个群体的情况，而不是订一个标准，高于这个标准的算高个子，而低于这个标准的算矮个子。同样，我们说学生成绩好的和成绩不好的，也是根据学生成绩的分布来讨论的。

这样的分类工作，可以由人来做，也可以通过算法来实现。这样的算法就称之为聚类算法。

#### 6.1.1 K均值(K-Means)算法

聚类算法中最著名的是K均值算法，也叫做K-Means算法。KMeans算法可以理解为将数据分成最适合的群组，通过计算数据点与中心之间的距离来确定数据点所属的群组。

形式化地讲，K均值是给定样本集$D={x_1,x_2,...,x_m}$,对于聚类所得簇划分$C={C_1,C_2,...,C_k}$最小化平方误差准则：

$E=\sum_{i=1}^k\sum_{x\in C_i}||x-\mu_i||_2^2$

其中$\mu_i$是簇$C_i$的均值向量。

K均值算法的步骤
- 选择 K 个初始簇中心（质心）：通常随机选择 K 个数据点作为初始簇中心。
- 分配数据点：将每个数据点分配到最近的簇中心，形成 K 个簇。
- 更新簇中心：计算每个簇的质心，即簇内所有数据点的平均值，作为新的簇中心。
- 重复步骤 2 和 3：直到簇中心不再变化或变化小于某个阈值，或达到预设的迭代次数。

K均值算法的优点和缺点
- 优点：

    - 简单易懂，容易实现。
    - 计算速度快，适合大规模数据集。
    - 在许多实际问题中表现良好。
- 缺点：

- 需要预先指定 𝐾的值，不同的k值可能会导致不同的聚类结果。
- 对初始簇中心敏感，不同的初始选择可能导致不同的结果。
- 只能找到凸形簇（即簇的形状为球状），对非凸形簇效果较差。
- 对噪声和异常值敏感。

在编程使用时，我们只要指定分成几个簇就可以了。下面是一个简单的例子，用于分析鸢尾花数据集。

```python
from sklearn import datasets
from sklearn.cluster import KMeans

iris = datasets.load_iris()
X = iris.data
y = iris.target

estimator = KMeans(n_clusters=3)
estimator.fit(X)

labels = estimator.labels_

print(X)
print(y)

print(labels)
```

下面的结果中，第一部分是系统给定的正确值，第二部分是我们聚类所得到的结果：

```
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2
 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2
 2 0]
```

从中可以看到，仅仅是简单聚类所获得的结果已经相当不错了。

那么，k的值如何取呢？常用的方法有肘部法、轮廓系数法、Gap 统计法和知识先验和领域知识4种。

1. 肘部法（Elbow Method）
肘部法通过绘制不同 𝐾值对应的总的簇内平方和（Within-Cluster Sum of Squares, WCSS）来选择 𝐾值。WCSS 是所有数据点到其簇中心距离的平方和。随着 
K 增加，WCSS 会逐渐减小。当𝐾增加到某个值后，WCSS 的减小幅度明显变缓，形成一个“肘部”形状，此时的 𝐾值即为合理选择。


我们来看个例子：

```python

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# 生成示例数据
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# 计算不同 K 值的 WCSS
wcss = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# 绘制肘部法图
plt.plot(K_range, wcss, 'bx-')
plt.xlabel('Number of clusters (K)')
plt.ylabel('WCSS')
plt.title('Elbow Method For Optimal K')
plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/elbow_method.png)

从上图中可以看到，当 𝐾=4 时，WCSS 的减小幅度明显变缓，因此 𝐾=4 是合理的选择。

2. 轮廓系数法（Silhouette Score）
轮廓系数（Silhouette Score）综合了簇内紧密度和簇间分离度，用于评估聚类质量。轮廓系数在 -1 到 1 之间，值越大表示聚类效果越好。通过计算不同𝐾值对应的轮廓系数，选择轮廓系数最大的𝐾值。

```python

from sklearn.metrics import silhouette_score

# 计算不同 K 值的轮廓系数
silhouette_scores = []
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X)
    if k > 1:  # silhouette_score 需要至少两个簇
        score = silhouette_score(X, kmeans.labels_)
        silhouette_scores.append(score)
    else:
        silhouette_scores.append(None)

# 绘制轮廓系数图
plt.plot(K_range, silhouette_scores, 'bx-')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method For Optimal K')
plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/sihouette_method.png)

从上图中可以看到，当 𝐾=4 时，轮廓系数最大，因此 𝐾=4 是合理的选择。

3. Gap 统计法（Gap Statistic）

肘部法虽然直观，但是不能自动化。Gap 统计法通过比较数据实际的聚类结果与随机数据的聚类结果来选择𝐾值。具体步骤如下：

- 对实际数据计算总的簇内平方和（WCSS）。
- 生成多个随机数据，计算其 WCSS。
- 计算两者的差距（Gap），选择 Gap 最大的 𝐾值。

Gap 统计法可以通过 gap_statistic 包来实现。gap_statistic 包提供了一个名为 OptimalK 的类，可以帮助我们选择最优的 𝐾值。

gap_statistic库是一个第三方库，我们先安装一下：

```python
pip install gap-stat
```

然后直接调用OptimalK类即可：

```python

import numpy as np
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from gap_statistic import OptimalK

# 生成示例数据
X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

# 使用 Gap 统计法选择最优 K 值
optimalK = OptimalK(parallel_backend='joblib')
n_clusters = optimalK(X, cluster_array=np.arange(1, 11))

print(f'最佳的簇大小为: {n_clusters}')
```

输出的结果是：

```
最佳的簇大小为: 5
```

下面我们再看看，鸢尾花数据集究竟为分多少个簇：

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from gap_statistic import OptimalK

# 载入数据
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 使用 Gap 统计法选择最优 K 值
optimalK = OptimalK(parallel_backend='joblib')
n_clusters = optimalK(X, cluster_array=np.arange(1, 11))

print(f'最佳的簇大小为: {n_clusters}')
```

输出结果为：

```
最佳的簇大小为: 8
```

4. 知识先验和领域知识

在某些情况下，可能有关于数据结构的先验知识或领域知识，这可以帮助选择合理的𝐾值。例如，如果你知道数据应该分为特定数量的组，这可以作为𝐾值的参考。

除此之外，我们还可以对KMenas算法进行改进，比如KMeans++算法，它是一种改进的KMeans算法，可以更好地选择初始簇中心，从而提高算法的性能。它的目的是提高 KMeans 算法的聚类性能和收敛速度。相比于传统的随机初始化，k-means++ 能够更好地选择初始质心，从而减少算法陷入局部最优的概率。

k-means++ 算法步骤:

- 随机选择第一个质心：从数据集中随机选择一个点作为第一个质心。
- 选择剩余的质心：对于每一个数据点 x，计算其与离它最近的已选择质心之间的距离 D(x)。然后，以 $D(x)^2$作为概率选择下一个质心，即距离越远的点被选择为质心的概率越大。这一步重复进行，直到选择出 K 个质心。
- 运行标准的 KMeans 算法：使用选择好的 K 个初始质心，运行标准的 KMeans 算法进行聚类。

Kmeans++的优点:

- 更好的初始质心：通过这种方法选择的初始质心通常会更接近数据的实际分布，从而提高聚类的性能。
- 减少局部最优：相比于随机初始化，k-means++ 更不容易陷入局部最优解。
- 加快收敛：由于初始质心选择更优，KMeans 算法通常需要更少的迭代次数即可收敛。

在 scikit-learn 中，KMeans 算法默认使用 k-means++ 进行初始化。当然，我们也可以显式指定初始化方法，通过 init 参数指定。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data

# 使用 KMeans 进行聚类，默认使用 k-means++ 初始化
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=0)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

# 使用 PCA 将数据降到二维，以便可视化
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 绘制聚类结果
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_kmeans, s=50, cmap='viridis', marker='o', edgecolor='k')

# 绘制簇中心
centers = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='x')

plt.xlabel('PCA Feature 1')
plt.ylabel('PCA Feature 2')
plt.title('KMeans++ Clustering on Iris Dataset')
plt.show()
```

上面的代码中，我们使用了PCA将数据降到二维，以便可视化。然后我们使用KMeans算法对数据进行聚类，最后绘制聚类结果。效果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/kmeans_pp.png)

#### 6.1.2 DBSCAN算法

DBSCAN是一种基于密度的聚类算法，它能够将密集的数据点群组成一个簇，并识别出噪声点。

比如你在一个大型的户外音乐节，人们聚在一起听音乐，而且通常跟志趣相投的人站得更近。DBSCAN算法就像是一个观察者，试图找出哪些人是一群朋友（即一个簇），哪些人可能只是偶尔走过的路人（即噪声）。

DBSCAN算法有两个参数：邻域大小（eps）和最小点数（MinPts）。

邻域大小（eps）可以看作是你伸出手臂的长度，只有在你手臂范围内的人，你才能轻易与他们交谈。
最小点数（MinPts）是决定一个小组是否足够大以至于被称为“朋友圈”的最少人数。

然后下面我们要做的事情就是从人群中找出所有“核心点”。一个人如果在他的手臂长度内（eps范围内）有至少MinPts个人（包括他自己），那么这个人就是一个核心点。

从一个核心点开始，找到所有能通过核心点直接或间接连接的点（即这些点都可以在连续的手臂长度内互相到达）。将这些点合并成一个簇。

在这个过程中，一些点可能既不是核心点，也不足以与任何核心点连接，这些点被视为噪声点。

最后，你会看到几个大的朋友群聚在一起，它们是通过共同的兴趣或位置紧密相连的，而那些孤立的人就像是噪声点，没有加入任何朋友圈。

我们来看一个例子。我们先随机生成5堆数据：

```python
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

centers = [[0, 4], [-4, -0], [4, 0], [0, -4],[0,0]]
X, labels_true = make_blobs(
    n_samples=2000, centers=centers, cluster_std=0.4, random_state=0
)

X = StandardScaler().fit_transform(X)
```

效果如下面所示：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/5points.png)

然后我们设置一下邻域大小为0.3，最小点数为10：

```python
import numpy as np

from sklearn import metrics
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=0.3, min_samples=10).fit(X)
labels = db.labels_
```

最后分类出来的效果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/5points_final.png)

黑色的点就是噪声点。

如果距离太近的话，有可能会导致误判。我们可以调整下参数来尝试改进。

需要注意的是，邻域的大小对于聚类的效果有很大的影响。我们来看一个例子：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/optics.png)。

我们可以看到，下面的中图邻域大小为0.5，而最右边的图邻域大小为2.0，这导致的聚类的效果完全不同。

为了自动寻找邻域大小，我们可以使用OPTICS算法。

```python
clust = OPTICS(min_samples=50, xi=0.05, min_cluster_size=0.05)

clust.fit(X)
```

#### 6.1.3 亲和传播算法

亲和传播（Affinity Propagation）通过在样本对之间发送消息直到收敛来创建聚类。然后，使用少量的代表样本来描述数据集，这些代表样本被确定为最能代表其他样本的样本。样本对之间发送的消息表示一个样本成为另一个样本的代表的适合程度，这些消息会根据其他样本对的值进行更新。这种更新是迭代进行的，直到收敛为止，此时选择最终的代表样本，从而得到最终的聚类结果。

Affinity Propagation 算法通过在数据点之间传递两种类型的信息来确定簇：责任（Responsibility）和可用性（Availability）。具体步骤如下：

1. 亲和度矩阵（Similarity Matrix）：首先，计算每对数据点之间的相似度（通常使用负欧氏距离）。相似度矩阵 $S$ 的元素 $S(i, j)$ 表示数据点 $i$ 作为数据点 $j$ 的代表点的适合程度（负距离越大，适合度越高）。
2. 责任更新（Responsibility Update）：
   - 责任 $R(i, k)$ 表示数据点 $k$ 作为数据点 $i$ 的代表点的责任大小。更新公式为：
     $R(i, k) = S(i, k) - \max_{k' \neq k} \{ A(i, k') + S(i, k') \}$
   - 这一步计算数据点 $i$ 相对于其他所有可能的代表点 $ k $ 的偏好。
3. 可用性更新（Availability Update）：
   - 可用性 $ A(i, k) $ 表示数据点 $ i $ 选择数据点 $ k $ 作为代表点的可行性大小。更新公式为：
     $A(i, k) = \min \left( 0, R(k, k) + \sum_{i' \notin \{i, k\}} \max(0, R(i', k)) \right) \quad \text{for } i \neq k$
     $A(k, k) = \sum_{i' \neq k} \max(0, R(i', k))$
   - 这一步计算数据点 $k$ 作为代表点的可行性。
4. 聚类决定：
   - 经过多次迭代更新责任和可用性矩阵之后，综合考虑责任和可用性矩阵的值，选择 $R(i, k) + A(i, k)$ 最大的 $k$ 作为数据点 $i$ 的代表点。
   - 最终，数据点 $k$ 作为代表点的那些点形成一个簇。

我们还是以上面的5堆数据为例：

```python
import numpy as np

from sklearn import metrics
from sklearn.cluster import AffinityPropagation
from sklearn.datasets import make_blobs

centers = [[0, 4], [-4, -0], [4, 0], [0, -4],[0,0]]
X, labels_true = make_blobs(
    n_samples=500, centers=centers, cluster_std=0.4, random_state=0
)
```

下面我们调用AffinityPropagation算法：

```python
af = AffinityPropagation(preference=-50, random_state=0).fit(X)
cluster_centers_indices = af.cluster_centers_indices_
labels = af.labels_
```

然后我们将获取的中心点画出来：

```python
import matplotlib.pyplot as plt

plt.close("all")
plt.figure(1)
plt.clf()

colors = plt.cycler("color", plt.cm.viridis(np.linspace(0, 1, 6)))

for k, col in zip(range(n_clusters_), colors):
    class_members = labels == k
    cluster_center = X[cluster_centers_indices[k]]
    plt.scatter(
        X[class_members, 0], X[class_members, 1], color=col["color"], marker="."
    )
    plt.scatter(
        cluster_center[0], cluster_center[1], s=14, color=col["color"], marker="o"
    )
    for x in X[class_members]:
        plt.plot(
            [cluster_center[0], x[0]], [cluster_center[1], x[1]], color=col["color"]
        )

plt.title("Estimated number of clusters: %d" % n_clusters_)
plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/affinity_propagation.png)

亲和传播算法的优点和缺点为：
- 优点

    - 自动确定簇数量：不需要预先指定簇的数量，这对于复杂数据集非常有用。
    - 处理非对称相似度：能够处理非对称的相似度矩阵。
    - 鲁棒性：对噪声和异常值有一定的鲁棒性。

- 缺点

    - 计算复杂度高：对于大规模数据集，计算和内存消耗较高。
    - 敏感性：对相似度矩阵和参数（如偏好参数）的选择较为敏感。


我们再来一个用亲和传播算法来分析鸢尾花数据集的例子：

```python
import numpy as np
from sklearn.cluster import AffinityPropagation
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data

# 进行 Affinity Propagation 聚类
affinity_propagation = AffinityPropagation(random_state=0)
affinity_propagation.fit(X)
cluster_centers_indices = affinity_propagation.cluster_centers_indices_
labels = affinity_propagation.labels_

# 使用 PCA 将数据降到二维，以便可视化
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 绘制聚类结果
plt.figure(figsize=(8, 6))
colors = plt.cm.rainbow(np.linspace(0, 1, len(cluster_centers_indices)))

for k, col in zip(range(len(cluster_centers_indices)), colors):
    class_members = (labels == k)
    cluster_center = X_pca[cluster_centers_indices[k]]
    plt.plot(X_pca[class_members, 0], X_pca[class_members, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=8)
    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=14)
    for x in X_pca[class_members]:
        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], 'k--', linewidth=0.5)

plt.title('Affinity Propagation Clustering of Iris Dataset')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()
```

输出结果如下：
![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/affinity_iris.png)

#### 6.1.4 聚类算法的局限性

在sklearn库的加持下，我们调用聚类算法非常简单。但是，我们不能把聚类算法当成强人工智能。我们来看一下主要的聚类算法在同样的几个数据集上的表现：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/cluster_all.png)

我们可以看到，不同的聚类算法在不同的数据集上表现不同。这说明，聚类算法并不是万能的，我们需要根据具体的数据集来选择合适的算法。

### 6.2 高斯混合模型

高斯混合模型(Gaussian Mixture Models,GMM)是一种广泛用于统计建模和数据聚类的概率模型。GMM 假设所有数据点是由多个高斯分布（也称为正态分布）组成的混合分布生成的。每个高斯分布代表数据中的一个簇。GMM 通过最大化数据点属于这些高斯分布的可能性来进行聚类和密度估计。

高斯分布是一个连续概率分布，它由均值（μ）和方差（σ²）参数化，其概率密度函数（PDF）为：

$ f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right) $

GMM 是多个高斯分布的加权和，每个分布都有自己的均值和方差。GMM 的概率密度函数为：$p(x|\lambda) = \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(x|\mu_k, \Sigma_k) $

其中：
- $ K $ 是高斯分布的数量。
- $ \pi_k $ 是第 $ k $ 个高斯分布的混合权重，满足 $ \sum_{k=1}^{K} \pi_k = 1 $。
- $ \mathcal{N}(x|\mu_k, \Sigma_k) $ 是第 $ k $ 个高斯分布，其均值为 $ \mu_k $，协方差矩阵为 $ \Sigma_k $。

期望最大化（EM）算法是用于估计 GMM 参数的一种常用方法，包括每个高斯分布的均值、方差和混合权重。EM 算法通过以下两个步骤迭代进行：

1. 期望步骤（E-step）：计算每个数据点属于每个高斯分布的概率（即责任）。
2. 最大化步骤（M-step）：根据责任重新估计 GMM 参数。


高斯混合模型的优缺点为：

- 优点
    - 灵活性：能够拟合任意形状的分布，因为多个高斯分布的组合可以近似任何连续分布。
    - 概率解释：提供了每个数据点属于每个簇的概率，这比硬聚类方法（如 K-Means）更具信息性。

- 缺点
    - 复杂性：参数估计和模型选择（例如，选择合适的高斯分布数量）较为复杂。
    - 计算成本：对于大规模数据，EM 算法的收敛速度可能较慢。


#### 6.2.1 用GaussianMixture类来实现高斯混合模型

在SciKit-Learn中，我们可以通过调用mixture.GaussianMixture类来实现高斯混合模型。

我们还是以鸢尾花数据集为例，来看如何使用高斯混合模型进行聚类：

```python
import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data

# 进行 GMM 聚类
gmm = GaussianMixture(n_components=3, random_state=0)
gmm.fit(X)
labels = gmm.predict(X)

# 使用 PCA 将数据降到二维，以便可视化
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 绘制聚类结果
plt.figure(figsize=(8, 6))
colors = plt.cm.rainbow(np.linspace(0, 1, 3))

for k, col in zip(range(3), colors):
    class_members = (labels == k)
    plt.plot(X_pca[class_members, 0], X_pca[class_members, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=8)

plt.title('GMM Clustering of Iris Dataset')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()
```

结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gmm_iris.png)


#### 6.2.2 用JAX实现高斯混合模型

GMM 使用期望最大化 (EM) 算法来估计模型参数，包括均值、方差和混合系数。

实现步骤为：

- 初始化模型参数。
- 实现 E 步骤：计算每个数据点属于每个高斯成分的后验概率。
- 实现 M 步骤：根据后验概率更新模型参数。
- 定义训练循环，反复执行 E 步骤和 M 步骤，直到收敛。

我们还是以鸢尾花为例，看看如何用JAX实现高斯混合模型.

首先我们来写E步骤:

```python
# E步骤：计算后验概率
def e_step(params, data):
    means, covs, weights = params
    responsibilities = jnp.array([
        weights[i] * multivariate_normal.pdf(data, means[i], covs[i])
        for i in range(num_components)
    ]).T
    responsibilities /= responsibilities.sum(axis=1, keepdims=True)
    return responsibilities
```

在高斯混合模型中，数据被假设为由多个高斯分布混合而成，每个高斯分布称为一个组件（component）。E步骤的目的是为了计算每个数据点属于各个组件的后验概率，也就是责任度（responsibilities）。

具体来看这段代码：

- params是一个包含所有模型参数的元组，其中means是每个组件的均值，covs是每个组件的协方差矩阵，weights是每个组件的权重。
- data是观测到的数据点集合。
- 代码首先将参数解构为means, covs, 和 weights。
- 接着，使用列表推导式和multivariate_normal.pdf函数计算每个数据点在每个组件下的概率密度值。multivariate_normal.pdf函数计算的是多元正态分布的概率密度函数值。
- 这些概率密度值乘以对应的组件权重weights[i]，得到初步的责任度。
- 最后，为了得到正规化的责任度，需要将每一行的责任度除以其总和，确保每行的和为1。这是通过responsibilities.sum(axis=1, keepdims=True)实现的，它计算了每一行的和，并通过设置keepdims=True保留了结果的二维性质以便进行广播除法。
- 函数返回正规化后的责任度矩阵，这个矩阵的每一行对应一个数据点，每一列对应一个组件，值表示该数据点属于对应组件的后验概率。


下面我们再写M步骤：

```python
# M步骤：更新参数
def m_step(responsibilities, data):
    N_k = responsibilities.sum(axis=0)
    weights = N_k / N_k.sum()
    means = (responsibilities.T @ data) / N_k[:, None]
    covs = jnp.array([
        (responsibilities[:, i][:, None] * (data - means[i])).T @ (data - means[i])
        / N_k[i]
        for i in range(num_components)
    ])
    return means, covs, weights
```

在GMM中，M步骤负责根据E步骤计算出的责任度来更新模型参数，包括每个组件的均值、协方差矩阵和权重。

具体来看这段代码：

responsibilities是由E步骤计算得到的每个数据点属于各个组件的后验概率矩阵。

data是观测到的数据点集合。

首先，计算每个组件的样本数量N_k，即每个组件的责任度之和。

然后，更新组件的权重weights，它是每个组件的样本数量除以总样本数量，以确保所有权重的和为1。

接下来，更新每个组件的均值means。对于每个组件，其均值是该组件的所有数据点的加权平均，权重就是相应的责任度。

最后，更新每个组件的协方差矩阵covs。对于每个组件，其协方差矩阵的计算考虑了数据点与组件均值的偏差，以及相应的责任度。具体来说，对于每个组件，计算所有数据点与该组件均值的偏差向量，然后乘以相应的责任度，最后求这些向量的加权平均并除以该组件的样本数量。

函数返回更新后的均值、协方差矩阵和权重。

训练循环就是将E步骤和M步骤交替执行，直到收敛。

```python
# 训练循环
def train_gmm(data, params, num_iters=100):
    for _ in range(num_iters):
        responsibilities = e_step(params, data)
        params = m_step(responsibilities, data)
    return params
```

最后我们把代码串起来：

```python
import jax
import jax.numpy as jnp
from jax.scipy.stats import multivariate_normal
from jax import jit
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

# 加载和预处理鸢尾花数据集
iris = datasets.load_iris()
data = iris.data
scaler = StandardScaler()
data = scaler.fit_transform(data)

# 设置随机种子
key = jax.random.PRNGKey(0)

# 初始化参数
num_components = 3
dim = data.shape[1]

def init_params(key, num_components, dim):
    key, subkey = jax.random.split(key)
    means = jax.random.normal(subkey, (num_components, dim))
    key, subkey = jax.random.split(key)
    covs = jnp.stack([jnp.eye(dim) for _ in range(num_components)])
    weights = jnp.ones(num_components) / num_components
    return means, covs, weights

params = init_params(key, num_components, dim)

# E步骤：计算后验概率
def e_step(params, data):
    means, covs, weights = params
    responsibilities = jnp.array([
        weights[i] * multivariate_normal.pdf(data, means[i], covs[i])
        for i in range(num_components)
    ]).T
    responsibilities /= responsibilities.sum(axis=1, keepdims=True)
    return responsibilities

# M步骤：更新参数
def m_step(responsibilities, data):
    N_k = responsibilities.sum(axis=0)
    weights = N_k / N_k.sum()
    means = (responsibilities.T @ data) / N_k[:, None]
    covs = jnp.array([
        (responsibilities[:, i][:, None] * (data - means[i])).T @ (data - means[i])
        / N_k[i]
        for i in range(num_components)
    ])
    return means, covs, weights

# 训练循环
def train_gmm(data, params, num_iters=100):
    for _ in range(num_iters):
        responsibilities = e_step(params, data)
        params = m_step(responsibilities, data)
    return params

# 使用 JIT 编译加速训练过程
train_gmm_jit = jit(train_gmm)

# 训练模型
params = train_gmm_jit(data, params)
means, covs, weights = params

print("Estimated means:", means)
print("Estimated covariances:", covs)
print("Estimated weights:", weights)

# 计算每个数据点的类别
responsibilities = e_step(params, data)
predicted_classes = jnp.argmax(responsibilities, axis=1)
print("Predicted classes:", predicted_classes)
```

运行结果如下：

```
Estimated means: [[-1.0153617   0.8529423  -1.3056443  -1.2539248 ]
 [ 0.35022977 -0.5207465   0.5594692   0.4952543 ]
 [ 1.3369247   0.06840447  1.1437105   1.3245293 ]]
Estimated covariances: [[[ 0.17859381  0.27103153  0.01103067  0.01617385]
  [ 0.27103153  0.7455524   0.01495313  0.02759757]
  [ 0.01103067  0.01495313  0.00955066  0.00445562]
  [ 0.01617983  0.0276229   0.00445651  0.01889936]]

 [[ 0.5475562   0.34272918  0.25402954  0.19825369]
  [ 0.3427626   0.60489476  0.17605983  0.1934371 ]
  [ 0.25404882  0.1760487   0.19194616  0.1836516 ]
  [ 0.19822091  0.19330631  0.1835907   0.24718682]]

 [[ 0.3029113  -0.17866114  0.11537373 -0.07810485]
  [-0.1792758   0.16232526 -0.057357    0.07562156]
  [ 0.11569946 -0.05736061  0.06861536 -0.03457049]
  [-0.07810654  0.07544895 -0.03448259  0.05892238]]]
Estimated weights: [0.3333284  0.5605666  0.10610501]
Predicted classes: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 2 1 1 1
 1 2 1 1 2 1 1 1 1 2 1 2 1 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 2 2 2 1 2 2 2 1 1
 2 1]
```



### 6.3 降维学习

在非监督学习中，降维学习是和聚类一样应用广泛的技术。顾名思义，它是用于减少数据特征的维度的技术。它通过保留一些比较重要的特征，去除一些冗余的特征，减少数据特征的维度。特征的重要性取决于该特征能够表达多少数据集的信息，以及使用什么方法进行降维。

降维在以下情况下会被广泛应用：
- 特征维度过大，可能会导致过拟合。
- 某些样本数据缺失值较多。
- 特征之间的相关性较大。

降维具有以下好处：
- 节省存储空间。
- 加速计算速度，降低计算复杂度。
- 去除冗余特征，提高模型的泛化能力。
- 便于观察和挖掘信息，通过降维可以将数据可视化为2维或3维，更容易理解和分析。
- 避免特征过多或过复杂导致模型过拟合的问题。

在Sklearn中，decomposition包用来实现降维学习。

#### 6.3.1 主成分分析

主成分分析（Principal Component Analysis，简称PCA）是一种常用的降维方法，用于将高维数据集转换为低维表示，同时保留数据的主要信息。它通过线性变换将原始特征空间转换为新的特征空间，新的特征空间中的特征被称为主成分。主成分是原始特征的线性组合，具有以下特点：
- 第一个主成分包含了原始数据中的最大方差，第二个主成分包含了次大方差，以此类推。这意味着主成分能够捕捉到数据中的最重要的信息。
- 主成分之间是正交的，即它们彼此之间是不相关的。

PCA 的步骤

- 数据标准化：将数据标准化，使每个特征的均值为 0，方差为 1。
- 计算协方差矩阵：计算数据集的协方差矩阵。
- 特征值分解：对协方差矩阵进行特征值分解，得到特征值和特征向量。
- 选择主成分：选择前 𝑘个最大特征值对应的特征向量，作为新的坐标轴。
- 转换数据：将原始数据投影到选定的主成分上，得到降维后的数据。

在Sklearn中，我们可以很方便地通过调用decomposition.PCA类来实现主成分分析。比如我们要将4维的Iris数据集降维到3维：

```python
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)
```

我们来看完整的代码：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 数据标准化
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 使用 PCA 进行降维
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 绘制降维结果
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.viridis)
plt.colorbar()
plt.title('PCA on Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
```

结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/pca_iris.png)

#### 6.3.2 独立成分分析

与主成分分析类似的还有独立成分分析。独立成分分析（Independent Component Analysis，简称ICA）是一种盲源分离技术，用于从混合信号中恢复原始信号。通常，ICA不用于降维，而是用于分离叠加的信号。
ICA假设原始信号是相互独立的，即信号之间是不相关的。ICA通过线性变换将混合信号转换为原始信号，使得原始信号在新的特征空间中是相互独立的。

ICA 的步骤

- 中心化：将数据中心化，使其均值为零。
- 白化：将数据白化，使其协方差矩阵为单位矩阵。
- 独立成分提取：通过最大化非高斯性（如使用负熵）来分离独立成分。

在Sklearn中，我们可以通过调用decomposition.FastICA类来实现独立成分分析。其调用方式与PCA类似：

```python
from sklearn.decomposition import FastICA

ica = FastICA(n_components=3, whiten="arbitrary-variance")
S_ = ica.fit_transform(X)  
A_ = ica.mixing_  
```

我们来看一个完整的处理随机生成的独立信号的示例：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import FastICA

# 生成随机混合信号
np.random.seed(0)
n_samples = 2000
time = np.linspace(0, 8, n_samples)

s1 = np.sin(2 * time)  # 正弦波
s2 = np.sign(np.sin(3 * time))  # 方波
s3 = np.random.rand(n_samples)  # 随机噪声

S = np.c_[s1, s2, s3]
S += 0.2 * np.random.normal(size=S.shape)  # 添加噪声
S /= S.std(axis=0)  # 标准化

# 混合信号
A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # 混合矩阵
X = np.dot(S, A.T)  # 观测信号

# 使用 ICA 分离信号
ica = FastICA(n_components=3)
S_ = ica.fit_transform(X)  # 重构信号
A_ = ica.mixing_  # 估计混合矩阵

# 绘制结果
plt.figure(figsize=(10, 8))

plt.subplot(3, 1, 1)
plt.title('Original Signals')
plt.plot(S)

plt.subplot(3, 1, 2)
plt.title('Mixed Signals')
plt.plot(X)

plt.subplot(3, 1, 3)
plt.title('ICA Recovered Signals')
plt.plot(S_)

plt.tight_layout()
plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/ica_random.png)

### 6.4 流形学习

流形学习（Manifold Learning）是一类非线性降维技术，用于在高维数据中发现低维流形结构。流形学习假设高维数据实际上存在于一个低维的嵌入空间中，并试图找到这一嵌入。它特别适用于处理那些通过线性方法（如PCA）难以有效降维的数据集。

流形是一个局部类似于欧几里得空间的拓扑空间。直观上，可以认为流形是嵌入在高维空间中的低维曲面。例如，地球表面是嵌入在三维空间中的二维流形。

流形学习的目标是找到一个低维的表示，使得原始高维数据在该低维空间中的结构尽可能保持不变。通常，这涉及到保持数据点之间的局部邻域关系。

常见的流形学习算法有：
- 局部线性嵌入（LLE）：局部线性嵌入（Locally Linear Embedding, LLE）通过假设每个数据点及其邻居可以用线性组合表示，来找到数据的低维表示。步骤如下：
    - 对每个数据点，找到其最近的 k 个邻居。
    - 计算每个数据点在其邻居中的线性重构权重。
    - 在低维空间中重新表示数据点，使得这些重构权重尽可能保持不变。
- Isomap（Isometric Mapping）：扩展了多维缩放（MDS），通过在高维空间中保持全局几何距离。步骤如下：
    - 构建数据点的邻接图，连接每个点与其最近的 k 个邻居。
    - 计算邻接图中所有点对之间的最短路径距离。
    - 使用经典的多维缩放（MDS）将距离矩阵嵌入到低维空间中。
- t-SNE：t-SNE（t-Distributed Stochastic Neighbor Embedding）通过在高维和低维空间中分别计算点对之间的概率分布，使高维数据在低维空间中聚类。步骤如下：
    - 在高维空间中计算点对之间的相似度，使用高斯分布。
    - 在低维空间中计算点对之间的相似度，使用 t 分布。
    - 最小化两种分布之间的 Kullback-Leibler 散度。
- UMAP：UMAP（Uniform Manifold Approximation and Projection）基于流形理论和代数拓扑，旨在更好地保持全局和局部结构。步骤如下：
    - 构建高维空间中的近邻图。
    - 使用代数拓扑方法在低维空间中优化表示。

我们来看下使用局部线性嵌入和IsoMap算法对鸢尾花数据集进行降维：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.manifold import LocallyLinearEmbedding, Isomap

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data

# 使用 LLE 进行降维
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_lle = lle.fit_transform(X)

# 使用 Isomap 进行降维
isomap = Isomap(n_components=2, n_neighbors=10)
X_isomap = isomap.fit_transform(X)

# 绘制降维结果
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(X_lle[:, 0], X_lle[:, 1], c=iris.target, cmap=plt.cm.viridis)
plt.title('LLE')

plt.subplot(1, 2, 2)
plt.scatter(X_isomap[:, 0], X_isomap[:, 1], c=iris.target, cmap=plt.cm.viridis)
plt.title('Isomap')

plt.show()
```

降维结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/lle_isomap.png)

我们继续详细讨论下t-SNE. t-SNE 通过在高维和低维空间中计算数据点之间的相似度来实现降维。具体步骤如下：

- 高维空间中的相似度：
   - 对于高维空间中的每对数据点 $ x_i $ 和 $ x_j $，t-SNE 使用高斯分布计算相似度：
   $p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}$
   - 这里，$\sigma_i$ 是与数据点 $ x_i $ 相关的标准差，表示高斯核的宽度。相似度是条件概率，表示在点 $ x_i $ 选择点 $ x_j $ 的概率。
- 对称化：$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$. 这里 $ n $ 是数据点的数量。
- 低维空间中的相似度：
    - 在低维空间中，t-SNE 使用 t 分布（通常是自由度为 1 的学生 t 分布，即柯西分布）计算相似度：
   $q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}$
   - 这里 $ y_i $ 和 $ y_j $ 是低维空间中的数据点。

t-SNE 通过最小化高维空间和低维空间中相似度分布的 Kullback-Leibler 散度（KL 散度）来优化低维表示：
$KL(P \| Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$
这里 $ P $ 和 $ Q $ 分别表示高维和低维空间中的相似度分布。


我们来看用t-SNE对鸢尾花数据集进行降维：

```python

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.manifold import TSNE

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 使用 t-SNE 进行降维
tsne = TSNE(n_components=2, random_state=0)
X_tsne = tsne.fit_transform(X)

# 绘制降维结果
plt.figure(figsize=(8, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=plt.cm.viridis)
plt.colorbar()
plt.title('t-SNE on Iris Dataset')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()
```

结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/t_sne_iris.png)

### 6.5 异常点检测

许多应用程序需要能够判断一个新的观察值是否与现有的观察值属于同一分布（它是正常值），或者应该被视为不同（它是异常值）。通常，这种能力被用来清理真实的数据集。必须做出两个重要的区别：

- 异常值检测：训练数据包含异常值，这些异常值被定义为与其他观察值相距甚远的观察值。因此，异常值检测估计器试图拟合训练数据最集中的区域，忽略偏离的观察值。
- 新颖性检测：训练数据没有被异常值污染，我们对检测新的观察值是否是异常值感兴趣。在这种情况下，异常值也称为新颖值。

异常值检测和新颖性检测都用于异常检测，其中人们有兴趣检测异常或不寻常的观察值。因此，异常值检测也被称为无监督异常检测，新颖性检测被称为半监督异常检测。在异常值检测的背景下，异常值/异常不能形成一个密集的集群，因为可用的估计器假设异常值/异常位于低密度区域。相反，在新颖性检测的背景下，新颖性/异常可以形成一个密集的集群，只要它们位于训练数据的低密度区域，在这个背景下被认为是正常的。

```python
# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
#         Albert Thomas <albert.thomas@telecom-paristech.fr>
# License: BSD 3 clause

import time

import matplotlib
import matplotlib.pyplot as plt
import numpy as np

from sklearn import svm
from sklearn.covariance import EllipticEnvelope
from sklearn.datasets import make_blobs, make_moons
from sklearn.ensemble import IsolationForest
from sklearn.kernel_approximation import Nystroem
from sklearn.linear_model import SGDOneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.pipeline import make_pipeline

matplotlib.rcParams["contour.negative_linestyle"] = "solid"

# Example settings
n_samples = 300
outliers_fraction = 0.15
n_outliers = int(outliers_fraction * n_samples)
n_inliers = n_samples - n_outliers

# define outlier/anomaly detection methods to be compared.
# the SGDOneClassSVM must be used in a pipeline with a kernel approximation
# to give similar results to the OneClassSVM
anomaly_algorithms = [
    (
        "Robust covariance",
        EllipticEnvelope(contamination=outliers_fraction, random_state=42),
    ),
    ("One-Class SVM", svm.OneClassSVM(nu=outliers_fraction, kernel="rbf", gamma=0.1)),
    (
        "One-Class SVM (SGD)",
        make_pipeline(
            Nystroem(gamma=0.1, random_state=42, n_components=150),
            SGDOneClassSVM(
                nu=outliers_fraction,
                shuffle=True,
                fit_intercept=True,
                random_state=42,
                tol=1e-6,
            ),
        ),
    ),
    (
        "Isolation Forest",
        IsolationForest(contamination=outliers_fraction, random_state=42),
    ),
    (
        "Local Outlier Factor",
        LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction),
    ),
]

# Define datasets
blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)
datasets = [
    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0],
    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0],
    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0],
    4.0
    * (
        make_moons(n_samples=n_samples, noise=0.05, random_state=0)[0]
        - np.array([0.5, 0.25])
    ),
    14.0 * (np.random.RandomState(42).rand(n_samples, 2) - 0.5),
]

# Compare given classifiers under given settings
xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))

plt.figure(figsize=(len(anomaly_algorithms) * 2 + 4, 12.5))
plt.subplots_adjust(
    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01
)

plot_num = 1
rng = np.random.RandomState(42)

for i_dataset, X in enumerate(datasets):
    # Add outliers
    X = np.concatenate([X, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)

    for name, algorithm in anomaly_algorithms:
        t0 = time.time()
        algorithm.fit(X)
        t1 = time.time()
        plt.subplot(len(datasets), len(anomaly_algorithms), plot_num)
        if i_dataset == 0:
            plt.title(name, size=18)

        # fit the data and tag outliers
        if name == "Local Outlier Factor":
            y_pred = algorithm.fit_predict(X)
        else:
            y_pred = algorithm.fit(X).predict(X)

        # plot the levels lines and the points
        if name != "Local Outlier Factor":  # LOF does not implement predict
            Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)
            plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors="black")

        colors = np.array(["#377eb8", "#ff7f00"])
        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])

        plt.xlim(-7, 7)
        plt.ylim(-7, 7)
        plt.xticks(())
        plt.yticks(())
        plt.text(
            0.99,
            0.01,
            ("%.2fs" % (t1 - t0)).lstrip("0"),
            transform=plt.gca().transAxes,
            size=15,
            horizontalalignment="right",
        )
        plot_num += 1

plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/outlier.png)

### 6.6 受限玻尔兹曼机

受限玻尔兹曼机（Restricted Boltzmann Machine，简称 RBM）是一种生成随机网络，用于对数据进行无监督学习。RBM 是一种能量模型，最早由 Geoffrey Hinton 提出，广泛应用于深度学习和概率图模型中。以下是 RBM 的一些关键特点和结构：

RBM 由两层组成：

1. 可见层（Visible Layer）：包含可见单元，表示输入数据。
2. 隐藏层（Hidden Layer）：包含隐藏单元，捕捉数据的潜在特征。

这两层之间的单元是完全连接的，但同一层内的单元之间不存在连接，这就是“受限”的含义。

受限玻尔兹曼机定义了一个能量函数 $ E(v, h) $，用于描述可见单元 $ v $ 和隐藏单元 $ h $ 的联合状态：

$ E(v, h) = -\sum_{i} v_i b_i - \sum_{j} h_j c_j - \sum_{i,j} v_i h_j w_{ij} $

其中：
- $ v_i $ 和 $ h_j $ 分别表示可见单元和隐藏单元的状态。
- $ b_i $ 和 $ c_j $ 是可见单元和隐藏单元的偏置。
- $ w_{ij} $ 是连接可见单元 $ v_i $ 和隐藏单元 $ h_j $ 的权重。

受限玻尔兹曼机定义了数据的联合概率分布：$ p(v, h) = \frac{e^{-E(v, h)}}{Z} $

其中 $ Z $ 是配分函数，用于归一化概率分布：$ Z = \sum_{v, h} e^{-E(v, h)} $

训练受限玻尔兹曼机通常使用对比散度（Contrastive Divergence, CD）算法。该算法通过以下步骤来更新权重和偏置：

1. 正向传播：给定输入数据，计算隐藏单元的激活值。
2. 重构数据：根据隐藏单元的激活值重构输入数据。
3. 计算梯度：根据原始数据和重构数据的差异更新权重和偏置。

BernoulliRBM 是 scikit-learn 中用于实现受限玻尔兹曼机（Restricted Boltzmann Machine, RBM）的类。它假设可见单元是伯努利分布的，这意味着输入数据应该是二进制的或者经过适当的预处理。

BernoulliRBM的用法为：

```python
from sklearn.neural_network import BernoulliRBM

# 创建一个 BernoulliRBM 实例
rbm = BernoulliRBM(n_components=256, learning_rate=0.01, n_iter=10, random_state=42)

# 训练 RBM
rbm.fit(X_train)

# 转换数据
X_transformed = rbm.transform(X_train)
```

参数说明
- n_components: 隐藏单元的数量。
- learning_rate: 学习率。
- batch_size: 每次迭代的样本数量。
- n_iter: 训练的迭代次数。
- random_state: 随机数种子，保证结果可重复。
- verbose: 是否输出训练过程信息。

下面我们来看如何使用受限波尔兹曼机对糖尿病数据集进行预测。

```python
import numpy as np
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import BernoulliRBM
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 加载糖尿病数据集
diabetes = datasets.load_diabetes()
X = diabetes.data
y = (diabetes.target > diabetes.target.mean()).astype(int)  # 将目标二值化（高于平均值为1，低于等于平均值为0）

# 数据标准化
scaler = StandardScaler()

# 定义 RBM 和 Logistic 回归模型
rbm = BernoulliRBM(random_state=0, n_iter=100, verbose=True)
logistic = LogisticRegression(max_iter=1000)

# 创建一个流水线，将标准化、RBM 和 Logistic 回归结合在一起
classifier = Pipeline(steps=[('scaler', scaler), ('rbm', rbm), ('logistic', logistic)])

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练模型
classifier.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = classifier.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f'Classification accuracy: {accuracy}')
```

## 第七章 集成学习

集成学习（Ensemble Learning）是一种机器学习技术，通过结合多个基学习器（Base Learners）来构建一个更强大的模型，从而提高预测性能和泛化能力。集成学习的核心思想是“集体智慧”，即多个模型的组合往往比单个模型的表现更好。

集成学习的基本类型

- Bagging（Bootstrap Aggregating）：
    - 原理：通过有放回抽样从训练数据集中生成多个子集，在每个子集上训练一个基学习器，然后将这些基学习器的预测结果进行平均（回归任务）或投票（分类任务）。
    - 优点：减少模型的方差，提高稳定性和泛化能力。
    - 示例：随机森林（Random Forest）。
- Boosting：
    - 原理：通过顺序训练多个基学习器，每个基学习器在前一个基学习器的基础上进行改进，重点关注前一个模型错误分类的样本。最终的预测结果是所有基学习器的加权和。
    - 优点：逐步减少偏差，显著提高模型性能。
    - 示例：AdaBoost、梯度提升（Gradient Boosting）。
- Stacking（Stacked Generalization）：
    - 原理：通过训练多个不同类型的基学习器，然后将它们的预测结果作为新的特征输入到一个元学习器（Meta-Learner）中进行最终预测。
    - 优点：利用不同模型的优势，进一步提高预测性能。
    - 示例：各种 stacking 实现通常结合多种不同的基学习器和元学习器。
- Voting：
    - 原理：通过结合多个基学习器的预测结果来确定最终的预测类别，对于硬投票（hard voting），选择多数基学习器投票的类别作为最终预测结果；对于软投票（soft voting），选择具有最高平均预测概率的类别作为最终预测结果。
    - 优点：简单易实现，且能有效提高分类精度。
    - 示例：Voting Classifier。

### 7.1 梯度提升树

梯度提升树（Gradient-Boosted Trees，简称GBT）是一种集成学习方法，通过结合多个弱学习者（通常是决策树）来创建一个强大的预测模型。该方法逐步构建决策树，每一步都优化前一步的误差，使模型逐渐逼近目标函数。

梯度提升树的基本概念
- 弱学习者：通常是浅层决策树（即树的深度较小），这些树单独的预测能力有限。
- 集成方法：将多个弱学习者的预测结果结合起来，形成一个强大的预测模型。
- 加法模型：GBTs 使用逐步加法模型，逐步向模型中添加新的弱学习者，每个新模型都是对前面模型误差的调整。
- 梯度下降：GBTs 使用梯度下降算法来最小化损失函数，每棵新树都是沿着梯度方向来减少前一步的预测误差。

梯度提升树的工作原理
1. 初始化模型：从一个简单的模型开始，通常是使用所有训练样本的均值或中位数作为初始预测。
2. 计算残差：对于每个训练样本，计算当前模型的预测误差（残差）。
3. 拟合残差：训练一个新的决策树来拟合这些残差。
4. 更新模型：将新树的预测结果加到现有模型中，更新后的模型更接近实际值。
5. 重复步骤 2-4：不断重复上述步骤，逐步减少模型的误差，直到达到预定的迭代次数或误差达到满意的水平。

以下是一个使用 scikit-learn 库实现梯度提升树回归的示例代码：

```python
import numpy as np
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# 生成样本数据
X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建 GradientBoostingRegressor 模型
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# 训练模型
gbr.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = gbr.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")
```

代码说明

- 生成数据: 使用 make_regression 生成回归问题的样本数据。
- 划分数据: 将数据集划分为训练集和测试集。
- 创建模型: 使用 GradientBoostingRegressor 创建梯度提升树回归模型。
- 训练模型: 在训练集上训练模型。
- 预测与评估: 在测试集上进行预测并计算均方误差（MSE）。

参数说明

- n_estimators: 决策树的数量，即弱学习者的数量。
- learning_rate: 学习率，控制每棵树对最终模型的贡献，通常设置为较小的值。
- max_depth: 决策树的最大深度，控制每棵树的复杂度。


梯度提升树优点和缺点

- 优点:
    - 高精度：能够处理复杂的非线性关系。
    - 灵活性：可以应用于回归和分类任务。
    - 可解释性：每棵树的贡献可以通过特征重要性进行解释。
- 缺点:
    - 训练时间长：由于逐步添加树，训练时间较长。
    - 过拟合：如果树的数量过多或树的深度过大，容易过拟合。
    - 参数调优复杂：需要调整多个参数（如学习率、树的数量和深度）以获得最佳性能。

### 7.2 随机森林

随机森林（Random Forests）是一种集成学习方法，通过组合多个决策树来提高模型的预测性能和鲁棒性。它在分类和回归任务中都表现出色。随机森林通过引入随机性来生成各个决策树，从而降低了过拟合风险，并提高了模型的泛化能力。

随机森林的基本概念
决策树：单个决策树是一个弱学习者，容易过拟合训练数据。
集成方法：将多个决策树的预测结果结合起来，形成一个更强大的模型。
随机性：在训练每棵树时引入随机性，包括对数据和特征的随机选择，从而使每棵树有所不同。
随机森林的工作原理
数据集划分：从原始训练数据中通过自助法（Bootstrap）随机抽取多个子集（有放回抽样）。
决策树训练：对于每个子集，训练一棵决策树。在树的每个节点分裂时，随机选择特定数量的特征来决定最佳分裂点，而不是使用所有特征。
集成预测：对于分类任务，随机森林通过对所有树的预测结果进行投票来确定最终类别；对于回归任务，取所有树预测值的平均值。
使用 scikit-learn 实现随机森林

下面我们用随机森林算法来处理下鸢尾花数据集：

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# 载入数据集
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建 RandomForestClassifier 模型
rf = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)

# 训练模型
rf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = rf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
```

代码说明

- 载入数据：使用 load_iris 载入鸢尾花数据集。
- 划分数据：将数据集划分为训练集和测试集。
- 创建模型：使用 RandomForestClassifier 创建随机森林分类器。
- 训练模型：在训练集上训练模型。
- 预测与评估：在测试集上进行预测并计算准确率。

参数说明

- n_estimators: 决策树的数量，即森林中树的数量。
- max_features: 决策树分裂时考虑的最大特征数，可以是整数、浮点数或特定的字符串（如 'sqrt' 表示特征总数的平方根）。
- random_state: 随机种子，用于结果复现。

我们再来用随机森林处理下糖尿病数据集：

```python
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# 载入糖尿病数据集
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建 RandomForestRegressor 模型
rf = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=42)

# 训练模型
rf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = rf.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")
```

随机森林的优点和缺点
- 优点:
    - 高准确性：通过集成多个决策树，显著提高了模型的预测性能。
    - 抗过拟合：引入随机性后，每棵树的预测误差互相抵消，整体模型更稳定。
    - 处理高维数据：适用于具有大量特征的数据集。
    - 特征重要性：提供特征重要性评估，有助于特征选择和数据理解。
- 缺点:
    - 训练时间长：包含许多决策树，训练时间较长。
    - 资源消耗大：需要较大的内存和计算资源，特别是当树的数量和数据规模较大时。
    - 不易解释：单个决策树容易解释，但随机森林作为一个整体难以解释其决策过程。

### 7.3 Bagging

Bagging（Bootstrap Aggregating）元估计器（Bagging meta-estimator）通过将多个基学习器（通常是相同类型的模型，如决策树）的预测结果结合起来，从而提高整体预测性能并降低模型的方差。Bagging 的核心思想是通过对数据集进行有放回抽样（bootstrap sampling）生成多个训练子集，并在这些子集上分别训练多个基学习器。

Bagging 的基本概念

- 有放回抽样：从原始训练数据集中随机抽取多个子集，每个子集的样本数与原始数据集相同，但由于有放回抽样，子集中可能包含重复的样本。
- 基学习器：通常选择同一种类型的学习器，如决策树。在这些子集上训练多个基学习器。
- 集成预测：对于分类任务，Bagging 通过对所有基学习器的预测结果进行投票来确定最终类别；对于回归任务，取所有基学习器预测值的平均值。

Bagging 的优点和缺点
- 优点:

    - 减少方差：通过对多个基学习器的预测结果进行平均或投票，Bagging 可以显著减少模型的方差，从而提高模型的稳定性和泛化能力。
    - 防止过拟合：由于每个基学习器在不同的子集上训练，Bagging 有助于减少过拟合现象。
    - 并行计算：各个基学习器可以独立训练，易于并行化，提升计算效率。
- 缺点:

    - 增加计算开销：训练多个基学习器需要更多的计算资源和时间。
    - 模型解释性差：由于最终预测是多个基学习器的综合结果，难以解释每个单独预测的贡献。
    - 使用 scikit-learn 实现 Bagging meta-estimator

下面我们用Bagging meta-estimator来处理糖尿病数据集：

```python
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.metrics import mean_squared_error

# 载入糖尿病数据集
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建基学习器：决策树回归模型
base_estimator = DecisionTreeRegressor(random_state=42)

# 创建 BaggingRegressor 模型
bagging = BaggingRegressor(base_estimator=base_estimator, n_estimators=100, random_state=42)

# 训练模型
bagging.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = bagging.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")
```

代码说明

- 载入数据：使用 load_diabetes 载入糖尿病数据集。
- 划分数据：将数据集划分为训练集和测试集，测试集占总数据的 20%。
- 创建基学习器：使用 DecisionTreeRegressor 作为基学习器。
- 创建 BaggingRegressor 模型：将基学习器 DecisionTreeRegressor 和其他参数传递给 BaggingRegressor。
- 训练模型：在训练集上训练 Bagging 模型。
- 预测与评估：在测试集上进行预测并计算均方误差（MSE）。

参数说明

- base_estimator: 基学习器，在此示例中为决策树回归模型。
- n_estimators: 基学习器的数量，即 Bagging 中的模型数量。在此示例中设置为 100。
- random_state: 随机种子，用于结果复现。在此示例中设置为 42。

### 7.4 投票法

投票法通过结合多个不同类型的基学习器的预测结果来提高整体模型的性能。它通常用于分类任务，通过投票机制来确定最终的预测类别。Voting Classifier 有两种主要类型：硬投票（hard voting）和软投票（soft voting）。

- 硬投票（Hard Voting）:在硬投票中，Voting Classifier 汇总各个基学习器的预测结果，然后选择多数基学习器投票的类别作为最终预测结果。即使某个基学习器的预测错误，只要大多数基学习器的预测是正确的，最终结果仍然可能是正确的。

- 软投票（Soft Voting）:在软投票中，Voting Classifier 结合各个基学习器的预测概率，然后选择具有最高平均预测概率的类别作为最终预测结果。这种方法利用了每个基学习器的概率输出，通常能获得更好的性能。


下面我们使用软投票来处理鸢尾花数据集，使用结合逻辑回归、支持向量机和随机森林分类器来构建 Voting Classifier。
集成学习可以将之前的知识都结合起来，这个感觉是不是很爽？

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score

# 载入鸢尾花数据集
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建基学习器
clf1 = LogisticRegression(random_state=42)
clf2 = SVC(probability=True, random_state=42)
clf3 = RandomForestClassifier(n_estimators=100, random_state=42)

# 创建 VotingClassifier 模型（软投票）
voting_clf = VotingClassifier(estimators=[('lr', clf1), ('svc', clf2), ('rf', clf3)], voting='soft')

# 训练模型
voting_clf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = voting_clf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
```

代码说明

- 载入数据：使用 load_iris 载入鸢尾花数据集。
- 划分数据：将数据集划分为训练集和测试集，测试集占总数据的 20%。
- 创建基学习器：使用逻辑回归（Logistic Regression）、支持向量机（SVC）和随机森林（RandomForestClassifier）作为基学习器。
- 创建 VotingClassifier 模型：
- estimators: 包含基学习器的列表，格式为 (名称, 模型实例) 的元组。
- voting='soft': 使用软投票机制。
- 训练模型：在训练集上训练 Voting Classifier 模型。
- 预测与评估：在测试集上进行预测并计算准确率。

参数说明

- estimators: 基学习器的列表，每个基学习器由一个名称和实际的模型实例组成。
- voting: 投票机制，可选 'hard'（硬投票）或 'soft'（软投票）。

### 7.5 Stacking

Stacked Generalization（简称 Stacking）通过组合多个不同类型的基学习器（Base Learners）的预测结果来提高整体模型的性能。与 Bagging 和 Boosting 不同，Stacking 不是简单地对多个模型的预测结果进行平均或加权，而是通过训练一个元学习器（Meta-Learner）来学习如何最佳地组合这些基学习器的预测结果。

Stacking 的基本原理

- 基学习器（Base Learners）：
    - 多个基学习器可以是不同类型的模型，例如决策树、支持向量机、逻辑回归等。
    - 这些基学习器在原始训练数据上独立训练，并生成初级预测（First-Level Predictions）。
- 元学习器（Meta-Learner）：
    - 元学习器是一个新的模型，它以基学习器的初级预测作为输入特征，训练一个最终的模型。
    - 元学习器的目标是学习如何最佳地组合基学习器的预测，以提高整体预测性能。

Stacking 的实现步骤

- 训练基学习器：
    - 将训练数据集划分为 K 折（K-Folds），在每一折上训练基学习器，并在剩余数据上进行预测，生成初级预测。
    - 将所有折的初级预测结果组合起来，形成一个新的训练集。
- 训练元学习器：使用基学习器的初级预测结果作为特征，原始的目标变量作为标签，训练元学习器。
- 预测阶段：
    - 在测试数据上，首先用基学习器生成初级预测。
    - 将这些初级预测输入到元学习器中，得到最终的预测结果。

下面我们把来逻辑回归、支持向量机和随机森林分类器来构建 Stacking 模型去处理鸢尾花数据集：

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.metrics import accuracy_score

# 载入鸢尾花数据集
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建基学习器
estimators = [
    ('lr', LogisticRegression(random_state=42)),
    ('svc', SVC(probability=True, random_state=42)),
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))
]

# 创建 StackingClassifier 模型
stacking_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())

# 训练模型
stacking_clf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = stacking_clf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"Stacking Classifier Accuracy: {accuracy:.4f}")
```

代码说明

- 载入数据：使用 load_iris 载入鸢尾花数据集。
- 划分数据：将数据集划分为训练集和测试集，测试集占总数据的 20%。
- 创建基学习器：使用逻辑回归（LogisticRegression）、支持向量机（SVC）和随机森林（RandomForestClassifier）作为基学习器。
- 创建 StackingClassifier 模型：
    - estimators: 包含基学习器的列表，格式为 (名称, 模型实例) 的元组。
    - final_estimator: 元学习器，在此示例中为逻辑回归（LogisticRegression）。
- 训练模型：在训练集上训练 Stacking 模型。
- 预测与评估：在测试集上进行预测并计算准确率。

参数说明

- estimators: 基学习器的列表，每个基学习器由一个名称和实际的模型实例组成。
- final_estimator: 元学习器，用于组合基学习器的预测结果。

### 7.6 AdaBoost

AdaBoost（Adaptive Boosting）是一种提升方法（Boosting），通过结合多个弱分类器（通常是决策树桩，即只有一个分裂节点的决策树）来构建一个强分类器。AdaBoost 是由 Freund 和 Schapire 在 1995 年提出的，旨在提高分类器的性能。

AdaBoost 的工作原理

1. 初始化样本权重：初始时，每个训练样本的权重是相等的，设为 $\frac{1}{N}$，其中 $N$ 是训练样本的数量。

2. 迭代过程：
   - 对于每一轮迭代 $t$，执行以下步骤：
     1. **训练弱分类器**：根据当前样本权重分布，训练一个弱分类器 $h_t$。
     2. **计算分类误差**：计算弱分类器在训练集上的误差率 $\epsilon_t$，即错分样本的权重之和。
     3. **计算分类器权重**：根据分类误差计算弱分类器的权重 $\alpha_t$，公式为：
       $
       \alpha_t = \frac{1}{2} \ln\left(\frac{1 - \epsilon_t}{\epsilon_t}\right)
       $
     4. **更新样本权重**：更新样本权重，公式为：
       $
       w_{i}^{(t+1)} = \frac{w_{i}^{(t)} \exp(-\alpha_t y_i h_t(x_i))}{Z_t}
       $
       其中，$y_i$ 是样本 $i$ 的真实标签（取值为 $\pm 1$），$x_i$ 是样本特征，$Z_t$ 是规范化因子，确保权重和为 1。

3. 构建最终分类器：
   - 最终分类器是所有弱分类器加权投票的结果，公式为：$H(x) = \text{sign}\left(\sum_{t=1}^{T} \alpha_t h_t(x)\right)$
     其中，$T$ 是迭代次数。

AdaBoost 的优点和缺点

- 优点：
    - 提高准确性：通过结合多个弱分类器，显著提高分类器的准确性。
    - 鲁棒性：对一些常见的噪声和数据异常具有一定的鲁棒性。
    - 无需调参：相对简单，只需指定迭代次数，通常不需要复杂的超参数调优。

- 缺点：
    - 对噪声敏感：在存在大量噪声数据时，可能会导致过拟合。
    - 计算开销：随着迭代次数的增加，计算开销和时间开销也会增加。

下面我们使用AdaBoost处理鸢尾花数据集，并使用决策树作为弱分类器。

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score

# 载入数据
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建决策树桩（弱分类器）
weak_classifier = DecisionTreeClassifier(max_depth=1, random_state=42)

# 创建 AdaBoost 模型
ada_clf = AdaBoostClassifier(estimator=weak_classifier, n_estimators=50, random_state=42)

# 训练模型
ada_clf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = ada_clf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"AdaBoost Classifier Accuracy: {accuracy:.4f}")
```

### 7.7 极端随机树

极端随机树与随机森林类似，但在构建决策树时有一些不同的策略。主要区别在于，极端随机树在分割节点时不仅随机选择特征，还随机选择分割阈值。

极端随机树的特点
- 极端随机性：在每个节点分裂时，不仅随机选择特征，还在特征的取值范围内随机选择切分点。这种极端的随机性有助于降低过拟合。
- 高效性：由于节点分裂的随机性，训练过程通常比随机森林更快。不需要在每个节点执行复杂的优化步骤。
- 集成方法：与随机森林类似，通过构建多棵决策树并集成它们的结果来提高模型的稳定性和准确性。


下面我们使用极端随机树处理鸢尾花数据集：

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import accuracy_score

# 载入数据
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建 ExtraTreesClassifier 模型
extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)

# 训练模型
extra_trees_clf.fit(X_train, y_train)

# 在测试集上进行预测
y_pred = extra_trees_clf.predict(X_test)

# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print(f"ExtraTreesClassifier Accuracy: {accuracy:.4f}")
```

参数说明

- n_estimators：要构建的树的数量。
- criterion：用于分裂节点的标准（如 "gini" 或 "entropy"）。
- max_features：在每个分裂点考虑的最大特征数量。
- max_depth：每棵树的最大深度。
- min_samples_split：分裂内部节点所需的最小样本数。
- min_samples_leaf：叶节点所需的最小样本数。

极端随机树的优点和缺点
- 优点：

    - 处理高维数据：适用于高维数据集。
    - 抗过拟合：比单一决策树更不容易过拟合。
    - 高效：训练速度快，适用于大规模数据集。
缺点：

    - 解释性较差：由于集成了多棵树，模型的解释性不如单一决策树。
    - 内存消耗大：需要存储多棵树，内存消耗较大。

### 7.8 孤立森林

孤立森林是一种基于树结构的无监督异常检测算法，它通过随机选择特征并在特征的随机值上分割数据来隔离数据点。异常点更容易被隔离，因为它们与其他点的差异更大，因此需要更少的分裂步骤。

孤立森林的工作原理
- 随机分割数据：
通过随机选择一个特征以及该特征上的一个随机分割值，将数据集分割成两个子集。这个过程在每个子集上递归进行，形成一棵树。
- 构建多棵树：
重复上述过程，构建多棵树（称为森林）。每棵树是通过随机子采样数据集构建的。
- 计算路径长度：对于每个数据点，计算其在树中的路径长度（即从根节点到达叶节点的步数）。异常点通常会在较少的分裂步骤后被孤立，因此其路径长度较短。
- 异常分数：通过路径长度计算异常分数。路径长度越短，数据点越可能是异常点。

我们来看一个例子：

```python
from sklearn.ensemble import IsolationForest
import numpy as np

# 生成示例数据
rng = np.random.RandomState(42)
X = 0.3 * rng.randn(100, 2)
X_train = np.r_[X + 2, X - 2]

# 生成一些异常点
X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))

# 训练 IsolationForest 模型
iso_forest = IsolationForest(contamination=0.1, random_state=42)
iso_forest.fit(X_train)

# 预测训练数据和异常点
y_pred_train = iso_forest.predict(X_train)
y_pred_outliers = iso_forest.predict(X_outliers)

print("Training Data Predictions:", y_pred_train)
print("Outlier Predictions:", y_pred_outliers)
```

参数说明

- n_estimators：森林中树的数量。
- max_samples：构建每棵树时用于训练的数据点数，可以是整数或浮点数（表示比例）。
- contamination：数据集中异常点的比例，用于自动设置阈值。
- max_features：用于分裂节点时的最大特征数。
- bootstrap：是否对样本进行重采样。

孤立森林的优点和缺点：
- 优点：

    - 无监督学习：不需要标签即可检测异常。
    - 高效：适用于大规模数据集，计算复杂度较低。
    - 多维数据：适用于多维数据集。
- 缺点：

    - 参数敏感：需要调整参数（如 contamination）来获得最佳效果。
    - 解释性较弱：树结构的随机性使得模型解释性较弱。

### 7.9 XGBoost

XGBoost（Extreme Gradient Boosting）是一个优化的分布式梯度提升库，旨在实现高效、灵活和可扩展的机器学习模型。它基于梯度提升决策树（GBDT），但在多个方面进行了改进，使其在处理大规模、高维数据集时表现出色。XGBoost 由 Tianqi Chen 开发，并且在许多数据科学竞赛中表现优异，成为了数据科学家和机器学习工程师的常用工具。

XGBoost 的特点
- 高效性：XGBoost 通过硬件优化和算法改进，提高了训练速度和效率。它利用了多线程并行计算。
- 正则化：通过添加正则化项（如 L1 和 L2）来防止过拟合，使得模型更具泛化能力。
- 灵活性：支持多种目标函数和自定义目标函数，适用于回归、分类、排序等任务。
- 分布式计算：支持在分布式环境下运行，适用于大规模数据集。
- 缺失值处理：内置了对缺失值的处理机制，在训练过程中能够自动处理缺失数据。
- 树剪枝：通过最大深度和最小子节点权重等参数进行树剪枝，进一步防止过拟合。

下面我们调用XGBoost库来处理鸢尾花数据集。注意，XGBoost需要安装额外的库，可以通过`pip install xgboost`安装。

```python
import xgboost as xgb
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 载入数据
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建 DMatrix 数据结构
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# 设置参数
params = {
    'booster': 'gbtree',
    'objective': 'multi:softmax',  # 多分类问题
    'num_class': 3,                # 类别数
    'eta': 0.3,                    # 学习率
    'max_depth': 6,                # 树的最大深度
    'eval_metric': 'mlogloss'      # 评价指标
}

# 训练模型
num_round = 100
bst = xgb.train(params, dtrain, num_round)

# 预测
y_pred = bst.predict(dtest)
accuracy = accuracy_score(y_test, y_pred)
print(f"XGBoost Accuracy: {accuracy:.4f}")
```

参数说明

- booster：选择基学习器类型（如 'gbtree'、'gblinear'、'dart'）。
- objective：定义优化目标（如 'reg:squarederror'、'binary:logistic'、'multi:softmax'）。
- eta：学习率（步长缩减），控制每一步的权重。
- max_depth：树的最大深度，控制模型的复杂度。
- eval_metric：评价指标（如 'rmse'、'mae'、'logloss'、'error'）。


XGBoost的优点和缺点
- 优点：

    - 高效性：训练速度快，适用于大数据集。
    - 灵活性：支持多种任务和自定义目标函数。
    - 正则化：通过正则化项有效防止过拟合。
    - 分布式计算：支持在分布式环境下运行。
- 缺点：

    - 复杂性：参数众多，调参过程复杂。
    - 计算资源：在大数据集上训练时，可能消耗大量内存和计算资源。

### 7.10 小结

通过上面的学习，我们对于集成学习有了更深入的了解。集成学习是一种通过结合多个基学习器来提高整体模型性能的方法。常见的集成学习方法包括 Bagging、Boosting、Voting 和 Stacking 等。集成学习可以有效提高模型的泛化能力，降低过拟合风险。


## 第九章 前向神经网络

### 9.1 人工神经网络

在介绍人工神经网络之前，我们先说说人体的神经网络。

在人体中，神经元是神经系统中的基本单位，负责传递信号和信息。神经元通过电化学信号的传递来实现信息的传递和处理。

如下图所示：神经元由细胞体、树突、轴突等部分组成。

- 细胞体：神经元的主体，包含细胞核和细胞质。
- 树突：从细胞体伸出的分支，用于接收来自其他神经元的信号。
- 轴突：神经元的长且细长的延伸部分，用于传递信号。
- 髓鞘：轴突被髓鞘包裹着，将电信号和外界隔离开来，加快信号传递的速度。髓鞘是由施万细胞构成的。
- 施万细胞：包裹和形成神经纤维的髓鞘，起到保护和促进神经信号传导的作用。施万细胞在神经再生方面具有重要作用，它们构建神经再生通道，帮助神经修复。
- 郎飞氏结：nodes of Ranvier，是两个施万细胞之间的神经轴。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/cellbody.png)

当神经元受到刺激时，细胞膜的透性发生急剧变化，这叫做神经冲动。神经冲动是伴随着钠离子大量流入和钾离子的大量流出而发生的。神经冲动的传导过程是电化学的过程，是在神经纤维上顺序发生的电化学变化，其传导速度非常快。

在19世纪末，意大利科学家卡米洛·夕尔基（Camillo Golgi）使用硝酸鸭染色技术观察到了神经元的存在。
1891年，德国科学家瓦尔特·冯·瓦尔德耶（Wilhelm von Waldyer）提出了“神经元”这个概念，并建立了神经元学说。
1943年，美国神经生理学家沃伦·麦库洛克（Warren McCulloch）和数学家沃尔特·皮茨（Walter Pitts）提出了第一个人工神经元模型，称为“M-P模型”。
1950年代，美国心理学家弗兰克·罗森布拉特（Frank Rosenblatt）发明了感知机模型，这是第一个能够学习和识别模式的神经网络模型。
1960年代至1980年代，神经网络的研究进入了第一个繁荣期，出现了许多新的神经网络模型和算法。
1969年: 马文·明斯基（Marvin Minsky）和西摩·帕普特（Seymour Papert）在《感知器》一书中指出了感知器的局限性，特别是它无法解决非线性问题。这导致了对神经网络的研究热情大幅下降，进入所谓的“人工智能冬天”。
1986年: 大卫·鲁梅尔哈特（David Rumelhart）、杰弗里·辛顿（Geoffrey Hinton）和罗纳德·威廉姆斯（Ronald Williams）提出了反向传播算法。这一算法解决了多层神经网络的训练问题，使得神经网络能够解决更复杂的任务，重新激发了研究热情。
2006年: 杰弗里·辛顿及其同事提出了深度信念网络（Deep Belief Networks），标志着深度学习的兴起。深度学习通过多层神经网络处理大量数据，显著提升了在图像识别、语音识别等领域的性能。
2012年: AlexNet在ImageNet图像识别比赛中取得了突破性胜利，展示了卷积神经网络（Convolutional Neural Networks, CNNs）的强大性能。深度学习时代正式到来。

感知机是一种基本的机器学习算法，它可以用来将数据分为两个类别。感知机是由两部分组成的：输入层和输出层。输入层接收输入数据，输出层输出预测结果。感知机的输出是一个二进制值，表示输入数据属于哪个类别。


感知机的工作原理如下：

- 输入层接收输入数据，并将数据传递给输出层。
- 输出层计算输入数据的加权和，并将结果传递给激活函数。
- 激活函数将加权和转换为输出值，输出值表示输入数据属于哪个类别。

下面是感知器原理的示意图：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/perception.png)

#### 9.1.1 激活函数

神经网络中的激活函数就像是一个开关，它决定了每个神经元（像大脑中的神经细胞一样）是否应该“激活”或“放电”。这个“开关”可以根据输入信号的强度来做出决定。

如果没有激活函数，神经网络就只是一堆简单的数学运算，不能处理复杂的问题。我们需要激活函数来引入非线性，使得神经网络能够处理复杂的数据和任务，比如识别图片中的猫和狗。

别看激活函数只是深度学习中最简单的一个计算，但是随着深度学习的发展，激活函数也在不断地演进。最早的激活函数是阶跃函数，然后是Sigmoid函数，再到ReLU函数，再到现在的GELU函数。如果是几年前学习深度学习，可能只知道ReLU函数及基变种，而现在GELU函数及其变种早已成为主流。

在神经网络早期发展的过程中，最简单的激活函数是阶跃函数（也称为Heaviside函数）。它的输出只有两种状态：0或1，类似于一个简单的二进制开关。

```python
def step_function(x):
    return 1 if x > 0 else 0
```

阶跃函数的主要缺点是它不连续，无法通过梯度下降法进行有效训练。

20世纪80年代，在早期的多层感知器（MLP）中，引入了sigmoid函数。Sigmoid函数引入了平滑的非线性，其输出在0到1之间。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/sigmoid.png)

sigmoid函数的优点是平滑，输出范围在0到1之间。其缺点为容易导致梯度消失问题，尤其在深层网络中。

同时广泛使用的还有tanh函数，它是sigmoid函数的变种，输出范围在-1到1之间。它在某些方面比Sigmoid函数更好，因为它的输出均值为0，减少了梯度消失问题。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/tanh.png)

sigmoid 和 tanh 激活函数会将输入压缩到一个有限的范围内（如 -1 到 1），而这些输出在反向传播过程中经过多层的链式相乘后，梯度会迅速衰减。

假设我们有一个深度为 $L$ 的神经网络，网络的前向传播计算为：$a^{[l]} = g(z^{[l]})$

其中，$g$ 是激活函数，$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$。

在反向传播过程中，梯度的计算涉及到激活函数的导数：$\frac{\partial \mathcal{L}}{\partial W^{[l]}} = \delta^{[l]} a^{[l-1]}$

其中，$\delta^{[l]}$ 是第 $l$ 层的误差项，并且：$\delta^{[l]} = \frac{\partial \mathcal{L}}{\partial z^{[l]}} = \left( W^{[l+1]} \right)^T \delta^{[l+1]} \circ g'(z^{[l]})$

如果激活函数的导数 $ g'(z) $ 很小，例如 sigmoid 函数在靠近 0 和 1 的区域内导数接近于零，乘积 $ \delta^{[l]} $ 会变得越来越小，导致梯度消失。

ReLU（Rectified Linear Unit）及其变种（如 Leaky ReLU、Parametric ReLU）能够有效减轻梯度消失问题，因为它们在正半轴上具有恒定的梯度。

ReLU 函数的定义非常简单：

对于任何负输入，输出都是 0。
对于任何非负输入，输出都等于输入本身。

我们来画一下ReLU函数：

```python
import numpy as np
import matplotlib.pyplot as plt

# 定义 ReLU 函数
def relu(x):
    return np.maximum(0, x)

# 创建输入数据
x = np.linspace(-10, 10, 400)

# 计算 ReLU 输出
y = relu(x)

# 绘制图形
plt.figure(figsize=(8, 6))
plt.plot(x, y, label='ReLU(x)', color='blue')
plt.title('ReLU Activation Function')
plt.xlabel('x')
plt.ylabel('ReLU(x)')
plt.grid(True)
plt.legend()
plt.show()
```

画出来结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/relu.png)

在BERT，GPT等大型模型中，GELU函数已经成为主流。GELU（Gaussian Error Linear Unit）是一种激活函数，用于深度学习中的神经网络。它结合了高斯误差函数和线性变换的优点，提供了平滑的非线性变换。

GELU 激活函数的数学表达式如下：$\text{GELU}(x) = x \cdot \Phi(x)$

其中，$\Phi(x)$ 是标准正态分布的累积分布函数：$\Phi(x) = \frac{1}{2} \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]$

这里，$\text{erf}(x)$ 是误差函数（error function）。

我们来绘制一下GELU函数：

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import erf

# 定义 GELU 函数
def gelu(x):
    return 0.5 * x * (1 + erf(x / np.sqrt(2)))

# 创建输入数据
x = np.linspace(-10, 10, 400)

# 计算 GELU 输出
y = gelu(x)

# 绘制图形
plt.figure(figsize=(8, 6))
plt.plot(x, y, label='GELU(x)', color='blue')
plt.title('GELU Activation Function')
plt.xlabel('x')
plt.ylabel('GELU(x)')
plt.grid(True)
plt.legend()
plt.show()
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gelu.png)


#### 9.1.2 优化方法

深度学习中的优化方法是用于调整神经网络权重，以最小化损失函数，从而提高模型的性能和准确性。以下是一些常见的优化方法：

- 梯度下降算法: 这部分我们在学习监督学习时已经介绍过了
    - 批量梯度下降（Batch Gradient Descent）:批量梯度下降使用整个训练集来计算损失函数的梯度，并更新模型参数。虽然它能提供稳定的收敛路径，但计算代价大，尤其是对于大型数据集。
    - 随机梯度下降（Stochastic Gradient Descent, SGD）:随机梯度下降每次使用一个样本来更新模型参数。虽然它的更新频率高，但收敛路径相对不稳定，可以帮助摆脱局部最优解。
    - 小批量梯度下降（Mini-batch Gradient Descent）：小批量梯度下降结合了批量梯度下降和随机梯度下降的优点。它在每次迭代中使用一小部分数据（小批量）来计算梯度，从而在计算效率和收敛稳定性之间取得平衡。
- 动量方法
    - 动量梯度下降（Momentum）：动量方法通过加速梯度下降的方向来加快收敛速度。它在每次更新时考虑了前一步的梯度更新，从而在较平坦的区域内加快收敛。
        - $v_t = \gamma v_{t-1} + \eta \nabla L(\theta_t)$
        - $\theta_{t+1} = \theta_t - v_t$
        - 其中，$\gamma$ 是动量因子，$\eta$ 是学习率。
    - Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）:NAG 是动量的一种改进方法，它在计算梯度时考虑了动量的影响，从而在梯度更新时更加精确。
        - $v_t = \gamma v_{t-1} + \eta \nabla L(\theta_t - \gamma v_{t-1})$
        - $\theta_{t+1} = \theta_t - v_t$
- 自适应学习率方法
    - AdaGrad: AdaGrad 是一种自适应学习率方法，它根据每个参数的历史梯度来调整学习率。它适用于稀疏数据集，但可能会导致学习率过早衰减。
        - $g_{t,i} = \nabla L(\theta_{t,i})$
        - $G_{t} = G_{t-1} + g_{t,i}^2$
        - $\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G_t + \epsilon}} g_{t,i}$
    - RMSProp: RMSProp 是一种改进的自适应学习率方法，它使用指数加权移动平均来调整学习率。它通过对梯度的平方进行指数加权移动平均来调整学习率，从而减少学习率的波动。
        - $g_{t,i} = \nabla L(\theta_{t,i})$
        - $G_{t} = \beta G_{t-1} + (1 - \beta) g_{t,i}^2$
        - $\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G_t + \epsilon}} g_{t,i}$
    - Adam: Adam 是一种结合了动量方法和 RMSProp 的自适应学习率方法。它使用指数加权移动平均来调整学习率和动量，从而提高收敛速度和稳定性。
        - $m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$
        - $v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$
        - $\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$
        - $\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

PyTorch中封装了大量的优化器，包括 SGD、Adam、RMSProp 等。我们可以通过调用 torch.optim 模块来使用这些优化器。
我们看两个例子：

```python
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)
```

下面我们来看个真实的例子，看看实际应用中的大模型使用的是哪种优化方法。

以LLaMA 2大模型为例，它使用的是AdamW优化器，这是一种改进的 Adam 优化算法，结合了权重衰减（weight decay）技术。AdamW 通过分离 L2 正则化和梯度更新，能更有效地防止过拟合。

AdamW 的更新公式如下：

$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$

$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$

$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$

$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

$\theta_{t+1} = \theta_t - \eta \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t\right)$

其中，$\lambda$ 是权重衰减因子。

LLaMA 2 使用了余弦退火调度（cosine annealing scheduler）来调整学习率。这种调度策略在训练初期会保持较高的学习率，然后逐渐降低，帮助模型更好地收敛到全局最优解。

余弦退火调度的公式如下：

$\eta_t = \eta_{\text{min}} + \frac{1}{2} (\eta_{\text{max}} - \eta_{\text{min}}) \left(1 + \cos\left(\frac{T_{\text{cur}}}{T_{\text{max}}} \pi\right)\right)$

其中：
- $\eta_t$ 是当前学习率。
- $\eta_{\text{min}}$ 和 $\eta_{\text{max}}$ 分别是最小和最大学习率。
- $T_{\text{cur}}$ 是当前迭代次数。
- $T_{\text{max}}$ 是总的迭代次数。

我们可以调用torch.optim.lr_scheduler.CosineAnnealingLR来使用余弦退火调度。这种调度器在训练初期保持较高的学习率，然后逐渐降低，到达最低点后再周期性地回升，模拟余弦函数的波动。这种方法在某些训练任务中可以帮助模型更好地收敛。

以下是 CosineAnnealingLR 的基本用法及其参数说明：

- optimizer：所需调整学习率的优化器。
- T_max：更新周期的最大迭代次数。
- eta_min：最低学习率，默认为 0。
- last_epoch：上一个 epoch 的索引，默认为 -1。如果设置为 -1，学习率将从初始值开始。

以下是一个完整的示例，包括模型定义、优化器设置、学习率调度器设置以及训练循环：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR

# 定义一个简单的神经网络模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 1)

    def forward(self, x):
        return self.fc(x)

# 实例化模型
model = SimpleModel()

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.AdamW(model.parameters(), lr=0.1)  # 初始学习率为 0.1

# 设置学习率调度器
scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.01)

# 模拟数据
inputs = torch.randn(32, 10)  # 32 个样本，每个样本 10 个特征
targets = torch.randn(32, 1)  # 32 个目标值

# 训练循环
num_epochs = 200
for epoch in range(num_epochs):
    model.train()
    
    # 前向传播
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # 更新学习率
    scheduler.step()
    
    # 打印学习率和损失值
    current_lr = optimizer.param_groups[0]['lr']
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Learning Rate: {current_lr:.6f}")
```

在这个示例中，我们完成了以下操作：

- 模型定义：我们定义了一个简单的线性回归模型。
- 优化器设置：使用 AdamW 优化器，并将初始学习率设为 0.1。
- 学习率调度器设置：使用 CosineAnnealingLR，设置 T_max 为 100，表示每 100 个 epoch 完成一个余弦周期，最低学习率 eta_min 设为 0.01。
- 训练循环：在每个 epoch 中，我们前向传播计算损失，反向传播更新参数，并调用 scheduler.step() 更新学习率。每个 epoch 后打印当前的损失值和学习率。

注意事项

- T_max 的选择很重要。它决定了学习率的更新周期。如果训练时间较长，T_max 的值应相应较大。
- eta_min 可以控制学习率的下限，避免学习率过低导致训练停滞。
- 调度器的学习率更新是在调用 scheduler.step() 方法时进行的，因此确保在每个 epoch 或指定的间隔内调用该方法。

#### 9.1.3 损失函数

在深度学习中，损失函数（或称为代价函数、目标函数）用于衡量模型预测输出与实际目标之间的差异。不同类型的任务（如回归、分类、生成等）通常使用不同的损失函数。以下是一些常用的损失函数：

- 回归任务中的损失函数
    - 均方误差（Mean Squared Error, MSE）均方误差用于衡量预测值与真实值之间的平方差。其定义为：

$$
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$
其中，$y_i$ 是真实值，$\hat{y}_i$ 是预测值，$N$ 是样本数量。

在 PyTorch 中可以通过 nn.MSELoss 实现：

```python
import torch.nn as nn
mse_loss = nn.MSELoss()
```


平均绝对误差（Mean Absolute Error, MAE）平均绝对误差用于衡量预测值与真实值之间的绝对差。

其定义为：

$$ \text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i| $$

在 PyTorch 中可以通过 nn.L1Loss 实现：


```python
import torch.nn as nn
mae_loss = nn.L1Loss()
```

- 分类任务中的损失函数

交叉熵损失（Cross Entropy Loss）交叉熵损失用于多分类问题，结合了 softmax 和负对数似然损失。其定义为：


$$
\text{CrossEntropyLoss} = -\sum_{i} y_i \log(\hat{y}_i)
$$

在 PyTorch 中可以通过 nn.CrossEntropyLoss 实现：

```python

import torch.nn as nn
cross_entropy_loss = nn.CrossEntropyLoss()
```

二元交叉熵损失（Binary Cross Entropy Loss）: 二元交叉熵损失用于二分类问题。其定义为：

$$
\text{BCELoss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
$$
在 PyTorch 中可以通过 nn.BCELoss 实现：

```python
import torch.nn as nn
bce_loss = nn.BCELoss()
```

选择合适的损失函数是深度学习模型训练中的关键步骤。不同的任务和数据分布可能需要不同的损失函数。了解和选择合适的损失函数可以帮助模型更好地学习和泛化。

#### 9.1.4 Dropout

Dropout 是一种在训练神经网络时用于减少过拟合的正则化技术。它的主要思想是在每次训练过程中，随机忽略（即“丢弃”）一部分神经元，使得网络不依赖于某些特定的神经元，从而提高模型的泛化能力。

在每一次训练迭代中，Dropout 会以一定的概率 $p$（通常是 0.5）随机将一些神经元的输出设置为 0。换句话说，对于给定的神经元，它有 $p$ 的概率被“丢弃”，有 
$1−p$ 的概率被保留。被丢弃的神经元在当前迭代中不会参与前向传播和反向传播。

在测试阶段，为了保证输出的期望值不变，所有神经元的输出会乘以一个因子 $1−p$。

Dropout 的作用主要有：
- 减少过拟合：Dropout 通过随机忽略部分神经元，防止模型对某些特定的神经元产生过度依赖，从而减少过拟合现象。
- 增强模型的泛化能力：因为 Dropout 使得每次迭代的模型结构不同，可以看作是对多个不同的子模型进行训练。这种“集成学习”的效果能够提高模型的泛化能力。
- 防止神经元共适应：神经元共适应指的是一些神经元依赖于其他特定的神经元来纠正错误。Dropout 通过打破这种依赖关系，使得每个神经元更具鲁棒性和独立性。

在 PyTorch 中，Dropout 可以通过 nn.Dropout 层来实现。在训练模型时，Dropout 会自动应用于前向传播，并在测试阶段关闭。以下是一个简单示例，展示了如何在网络中使用 Dropout：

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 定义一个简单的全连接网络，包含 Dropout 层
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```

#### 9.1.5 自动求导

PyTorch 的自动微分功能依赖于一个称为自动求导（Autograd）的系统。Autograd 是 PyTorch 中的一个核心功能，它允许用户在张量操作过程中自动计算梯度。

Autograd 使用了反向传播（backpropagation）算法来计算梯度。其核心思想是通过记录张量的操作图来跟踪计算过程，并在需要计算梯度时反向遍历这个图。

在 PyTorch 中，每个张量都可能有一个关联的计算图。这个计算图是由有向无环图（DAG）构成的，其中节点表示张量，边表示张量之间的操作。每次对张量进行操作（例如加法、乘法等），PyTorch 会在计算图中记录这个操作。

每个张量对象都有一个 requires_grad 属性，如果设置为 True，则 PyTorch 会开始跟踪所有对该张量的操作，以便在反向传播时计算梯度。计算图中的每个张量对象还包含一个 grad 属性，用于存储计算得到的梯度。

当调用 backward() 方法时，PyTorch 会从计算图的终端节点开始，使用链式法则（Chain Rule）反向计算各个张量的梯度。这种方法可以有效地计算复杂函数的梯度。

以下是一个简单的示例代码，展示了 Autograd 的工作原理：

```python
import torch

# 创建一个张量并设置 requires_grad=True 以启用自动求导
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 定义一个简单的函数 y = 3 * x + 2
y = 3 * x + 2

# 定义一个更复杂的函数 z = y^2
z = y.pow(2).sum()

# 反向传播以计算梯度
z.backward()

# 输出 x 的梯度
print(x.grad)
```

在上面的代码中,我们首先创建了一个张量 x 并将 requires_grad 设置为 True。
然后，我们定义了两个函数 y 和 z。这里，y 是 x 的线性变换，z 是 y 的平方和。
调用 z.backward() 将触发反向传播，计算 z 相对于 x 的梯度。
最后，我们输出 x 的梯度，存储在 x.grad 中。

#### 9.1.6 前向神经网络的基本结构

前向神经网络（Feedforward Neural Network, FNN）前向神经网络是最基础的神经网络结构，其中数据从输入层经过一个或多个隐藏层到达输出层，每层的神经元之间全连接。FNN 的训练过程主要通过前向传播和反向传播来优化模型参数。

一个典型的前向神经网络由以下几部分组成：

- 输入层（Input Layer）：接受输入数据，每个神经元代表一个输入特征。
- 隐藏层（Hidden Layers）：位于输入层和输出层之间，可以有一个或多个隐藏层，每层包含若干神经元。隐藏层的神经元通过激活函数（如 ReLU, Sigmoid, Tanh 等）引入非线性。
- 输出层（Output Layer）：生成最终的输出，每个神经元对应一个输出。

前向传播是指数据从输入层经由隐藏层传递到输出层的过程。每一层的输出作为下一层的输入。数学上，每层的线性变换和激活函数可以表示为：

$h^{(l)} = f(W^{(l)} h^{(l-1)} + b^{(l)})$

其中：
- $h^{(l)}$ 是第 $l$ 层的输出。
- $W^{(l)}$ 是第 $l$ 层的权重矩阵。
- $b^{(l)}$ 是第 $l$ 层的偏置向量。
- $f$ 是激活函数（如 ReLU, Sigmoid 等）。

反向传播是指通过计算损失函数相对于每个参数的梯度，从输出层向输入层传播误差，并更新权重和偏置以最小化损失函数。主要步骤包括：

- 计算损失函数（Loss Function）：常用的损失函数有均方误差（MSE）和交叉熵损失（Cross-Entropy Loss）。
- 计算梯度：使用链式法则计算损失函数相对于每个参数的梯度。
- 更新参数：根据梯度下降算法更新参数（权重和偏置），公式如下：

$W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}$

$b^{(l)} \leftarrow b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}$

其中，$\eta$ 是学习率。

我们先看如何定义一个前向网络：

```python
class IrisNet(nn.Module):
    def __init__(self):
        super(IrisNet, self).__init__()
        self.fc1 = nn.Linear(4, 16)
        self.fc2 = nn.Linear(16, 3)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = IrisNet()
```

这段代码定义了一个名为IrisNet的神经网络模型。该模型继承自nn.Module类，是PyTorch中的一个基类，用于定义神经网络模型。

在IrisNet的构造函数中，有两个全连接层(nn.Linear)，分别是self.fc1和self.fc2。self.fc1将输入维度为4的数据映射到维度为16的特征空间，而self.fc2将维度为16的特征空间映射到维度为3的输出空间。

forward方法定义了模型的前向传播过程。在前向传播中，输入数据x首先通过self.fc1进行线性变换，并经过ReLU激活函数(torch.relu)进行非线性变换。然后，变换后的结果再通过self.fc2进行线性变换，得到最终的输出。

最后，通过model = IrisNet()创建了一个IrisNet的实例，即创建了一个名为model的神经网络模型对象。

接下来，我们还需要定义损失函数和优化器，以便训练模型。

```python
# 3. 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
```

在这段代码中，我们定义了一个交叉熵损失函数(nn.CrossEntropyLoss)和一个Adam优化器(optim.Adam)。交叉熵损失函数适用于多分类问题，Adam优化器是一种自适应学习率方法，结合了动量方法和RMSProp方法。

CrossEntropyLoss 是深度学习中常用的损失函数，特别适用于多分类任务。它结合了 softmax 激活函数和负对数似然损失（negative log-likelihood loss），可以有效地衡量模型输出的概率分布与目标分布之间的差异。

在多分类问题中，softmax 函数用于将模型的原始输出（logits）转换为概率分布。给定一个样本的 logits 向量 z，softmax 函数定义如下：

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
$$

交叉熵损失用于衡量两个概率分布之间的差异。在分类问题中，它衡量的是模型输出的预测概率分布与真实标签分布之间的差异。给定一个样本的真实标签（用 one-hot 编码表示） ，交叉熵损失定义如下：


$$
\text{CrossEntropyLoss}(y, \hat{y}) = -\sum_{i} y_i \log(\hat{y}_i)
$$

针对独热编码，损失可以简化为：

$$
\text{CrossEntropyLoss} = -\log(\hat{y}_i)
$$
在 PyTorch 中，nn.CrossEntropyLoss 将 softmax 和交叉熵损失结合在一起。给定模型的原始输出（logits）和真实标签（不需要 one-hot 编码），它会先对 logits 应用 softmax 函数，然后计算交叉熵损失。

然后我们可以开始训练模型：

```python
num_epochs = 100
for epoch in range(num_epochs):
    # 前向传播
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

这里的关键代码我们需要介绍下：

- `optimizer.zero_grad()`: 这个操作用于将模型参数的梯度归零。在每次进行反向传播之前，我们需要清除之前计算的梯度值，以避免梯度累积的影响。这样做是因为PyTorch默认会将每次计算的梯度值累加到之前的梯度上，而不是替换之前的梯度值[2]。
- `loss.backward()`: 这个操作用于计算损失函数关于模型参数的梯度。在前向传播计算损失函数之后，我们可以调用`loss.backward()`来自动计算梯度。这个操作会根据链式法则自动计算出每个参数的梯度，并将其存储在参数的.grad属性中[1]。
- `optimizer.step()`: 这个操作用于更新模型参数。在计算完梯度之后，我们可以调用`optimizer.step()`来根据优化算法更新模型参数。这个操作会根据优化算法的规则，使用之前计算的梯度值来更新模型参数，从而使模型向更优的方向前进[1]。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/pic_iris.png)

下面我们看来完整的代码：
```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# 1. 加载并预处理数据
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 数据标准化
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 转换为 PyTorch 张量
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)

# 2. 构建全连接神经网络模型
class IrisNet(nn.Module):
    def __init__(self):
        super(IrisNet, self).__init__()
        self.fc1 = nn.Linear(4, 16)
        self.fc2 = nn.Linear(16, 3)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = IrisNet()

# 3. 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 4. 训练模型
num_epochs = 100
for epoch in range(num_epochs):
    # 前向传播
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 5. 在测试集上评估模型
model.eval()  # 将模型设置为评估模式
with torch.no_grad():
    outputs = model(X_test)
    _, predicted = torch.max(outputs, 1)
    accuracy = accuracy_score(y_test, predicted)
    print(f'Accuracy on test set: {accuracy:.4f}')
```

运行结果如下：

```
Epoch [10/100], Loss: 0.6799
Epoch [20/100], Loss: 0.4794
Epoch [30/100], Loss: 0.3776
Epoch [40/100], Loss: 0.3168
Epoch [50/100], Loss: 0.2647
Epoch [60/100], Loss: 0.2135
Epoch [70/100], Loss: 0.1692
Epoch [80/100], Loss: 0.1331
Epoch [90/100], Loss: 0.1072
Epoch [100/100], Loss: 0.0899
Accuracy on test set: 1.0000
```

我们看下分类的效果：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/mlp_iris.png)

下面我们再用深度学习来处理下波士顿房价数据集：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
from keras.datasets import boston_housing
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

# 加载波士顿房价数据集
(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()

# 数据标准化
scaler = StandardScaler()
train_data = scaler.fit_transform(train_data)
test_data = scaler.transform(test_data)

# 转换为 PyTorch 张量
train_data = torch.tensor(train_data, dtype=torch.float32)
train_targets = torch.tensor(train_targets, dtype=torch.float32).view(-1, 1)
test_data = torch.tensor(test_data, dtype=torch.float32)
test_targets = torch.tensor(test_targets, dtype=torch.float32).view(-1, 1)

# 创建数据集和数据加载器
train_dataset = TensorDataset(train_data, train_targets)
test_dataset = TensorDataset(test_data, test_targets)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# 定义全连接网络
class BostonHousingModel(nn.Module):
    def __init__(self):
        super(BostonHousingModel, self).__init__()
        self.fc1 = nn.Linear(13, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# 创建模型实例
model = BostonHousingModel()

# 定义损失函数和优化器
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 100
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)
    
    train_loss = running_loss / len(train_loader.dataset)
    train_losses.append(train_loss)
    
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for inputs, targets in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item() * inputs.size(0)
    
    val_loss = val_loss / len(test_loader.dataset)
    val_losses.append(val_loss)
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')

# 评估模型
model.eval()
test_loss = 0.0
with torch.no_grad():
    for inputs, targets in test_loader:
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        test_loss += loss.item() * inputs.size(0)
test_loss = test_loss / len(test_loader.dataset)
print(f'Test MSE: {test_loss:.4f}')

# 可视化训练过程
epochs = range(1, num_epochs + 1)
plt.figure(figsize=(12, 6))
plt.plot(epochs, train_losses, 'bo-', label='Training loss')
plt.plot(epochs, val_losses, 'ro-', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

运行结果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/mlp_boston.png)

### 9.2 计算图

在 PyTorch 中，计算图（Computational Graph）是一个用于描述张量之间操作关系的有向无环图（Directed Acyclic Graph, DAG）。计算图的节点表示张量，边表示张量之间的操作。计算图是 PyTorch 实现自动微分（Autograd）的基础。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/TorchScript.png)

#### 9.2.1 计算图的构建

计算图的构建是动态的，即在每次前向传播（forward pass）时，根据你所执行的操作动态创建。这种动态计算图的特性使得 PyTorch 非常灵活，尤其是在处理变长输入或需要在每个前向传播过程中改变模型结构的情况下。

计算图的主要作用是跟踪张量之间的操作，以便在反向传播（backward pass）时，能够使用链式法则（Chain Rule）高效地计算梯度。

当你对张量进行操作时，PyTorch 会在后台构建计算图。例如：

```python
import torch

# 创建张量并设置 requires_grad=True 以启用自动求导
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)

# 对张量进行操作
y = x + 2
z = y * y * 3
out = z.mean()

# 查看计算图的输出
print(out)
```

在上面的代码中，PyTorch 通过以下步骤构建计算图：

创建张量 x 并设置 requires_grad=True，表示希望计算该张量的梯度。
进行操作 y = x + 2，在计算图中记录这一步操作。
进行操作 z = y * y * 3，在计算图中记录这一步操作。
进行操作 out = z.mean()，在计算图中记录这一步操作。

我们看下输出的结果：

```
tensor(50., grad_fn=<MeanBackward0>)
```

可以看到，grad_fn 的值为 <MeanBackward0>，表示 out 张量是通过 mean 操作得到的。

当我们调用 backward() 方法时，PyTorch 会从计算图的终端节点（如上例中的 out）开始，反向遍历计算图，并使用链式法则计算每个张量的梯度。

```python
# 反向传播以计算梯度
out.backward()

# 输出 x 的梯度
print(x.grad)
```

在上面的代码中，out.backward() 会计算 out 相对于 x 的梯度，并将梯度存储在 x.grad 中。

#### 9.2.2 通过Trace机制生成计算图

Tracing 通过运行一遍模型的前向传播，记录下计算图。适用于大多数纯前向计算的模型，但对控制流（如 if 语句和循环）的支持有限。

我们可以通过 torch.jit.trace 方法来生成计算图：

```python
import torch
import torch.nn as nn

# 定义一个简单的模型
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc = nn.Linear(10, 5)

    def forward(self, x):
        return torch.relu(self.fc(x))

# 创建模型实例
model = MyModel()

# 创建一个输入张量
input_tensor = torch.randn(3, 10)

# 使用 tracing 将模型转换为 TorchScript
traced_model = torch.jit.trace(model, input_tensor)

# 保存 TorchScript 模型
traced_model.save("traced_model.pt")

# 加载 TorchScript 模型
loaded_model = torch.jit.load("traced_model.pt")

# 进行推理
output = loaded_model(input_tensor)
print(output)
```

在上面的代码中，我们首先定义了一个简单的模型 MyModel，然后创建了一个输入张量 input_tensor。接着，我们使用 torch.jit.trace 方法将模型转换为 TorchScript，并保存为 traced_model.pt 文件。最后，我们通过 torch.jit.load 方法加载模型，并进行推理。

TorchScript 是 PyTorch 的一种中间表示（Intermediate Representation, IR），它是一种高效的序列化和优化格式，可以提高模型的性能和便于部署。TorchScript 模型具有以下特点：

- 可序列化和可优化：TorchScript 模型可以保存为文件，然后加载到其他环境中进行推理。这使得模型的部署更加便捷和高效。
- 跨平台和跨语言支持：TorchScript 模型可以在 C++ 环境中运行，这对于嵌入式系统或移动设备上的部署非常有帮助。
- 静态图：与动态的 PyTorch 计算图不同，TorchScript 使用静态计算图，这在某些情况下可以带来性能优化。

#### 9.2.3 通过Script机制生成计算图

Scripting 通过直接解析 Python 代码，将其转换为 TorchScript。适用于包含复杂控制流的模型。

```python
import torch
import torch.nn as nn
import torch.jit as jit

# 定义一个包含控制流的模型
class MyScriptModel(nn.Module):
    def __init__(self):
        super(MyScriptModel, self).__init__()
        self.fc = nn.Linear(10, 5)

    def forward(self, x):
        if x.mean() > 0:
            return torch.relu(self.fc(x))
        else:
            return -torch.relu(self.fc(x))

# 创建模型实例
script_model = MyScriptModel()

# 使用 scripting 将模型转换为 TorchScript
scripted_model = jit.script(script_model)

# 保存 TorchScript 模型
scripted_model.save("scripted_model.pt")

# 加载 TorchScript 模型
loaded_scripted_model = torch.jit.load("scripted_model.pt")

# 创建一个输入张量
input_tensor = torch.randn(3, 10)

# 进行推理
output = loaded_scripted_model(input_tensor)
print(output)
```

在上面的代码中，我们定义了一个包含控制流的模型 MyScriptModel，然后使用 torch.jit.script 方法将模型转换为 TorchScript，并保存为 scripted_model.pt 文件。最后，我们通过 torch.jit.load 方法加载模型，并进行推理。

#### 9.2.4 TorchScript IR

TorchScript IR（Intermediate Representation，中间表示）是 PyTorch 中用于表示和操作 TorchScript 程序的一种低级表示形式。TorchScript IR 是一种图结构，它描述了计算过程中的操作和数据流，类似于计算图。它使得模型可以在没有 Python 解释器的生产环境中运行，并且可以进行各种优化和转换。

TorchScript IR 的特点

- 图结构：TorchScript IR 使用图结构来表示计算过程，每个节点（Node）对应一个操作（Operation），每条边（Edge）对应数据流（Tensor）。
- 操作符（Operators）：操作符定义了节点执行的具体操作，如加法、矩阵乘法等。
- 类型系统：TorchScript IR 具有静态类型系统，定义了每个操作的输入和输出类型。
- 优化和转换：由于是静态图，TorchScript IR 可以进行各种优化和转换，如常量折叠、内联、死代码消除等。

TorchScript IR 的组成部分
- 图（Graph）:整个计算过程表示为一个图：
    - 节点（Node）：图中的一个操作。
    - 边（Edge）：表示数据流，连接不同节点的数据依赖关系。
- 节点（Node）每个节点表示一个操作，包括：
    - 操作符（Operator）：定义节点执行的具体操作。
    - 输入和输出（Inputs and Outputs）：节点所需的输入和产生的输出。
- 类型（Type）:每个节点的输入和输出都有明确的类型定义，常见的类型包括：
    - Tensor：张量类型。
    - Scalar：标量类型。
    - List：列表类型。
    - Tuple：元组类型。
    - Dict：字典类型。

TorchScript IR 的工作流程

- 前端（Frontend）：将 Python 代码解析为 TorchScript IR。
- 中端（Midend）：对 TorchScript IR 进行各种优化和转换。
- 后端（Backend）：将优化后的 TorchScript IR 生成可执行代码。

前端负责将 Python 代码转换为 TorchScript IR。这包括解析 Python 代码，生成抽象语法树（AST），并将其转换为 TorchScript 图。

中端对生成的 TorchScript 图进行优化，包括：

- 常量折叠：将常量表达式计算并替换为常量值。
- 内联：将函数调用展开为实际的函数体。
- 死代码消除：移除不会被执行的代码。

后端将优化后的 TorchScript 图转换为实际的可执行代码，这可以是 CPU 代码、GPU 代码或其他硬件平台上的代码。

说了这么多，那么，我们如何查看TorchScript IR呢？我们可以通过下面的代码来查看：

```python
import torch
import torch.nn as nn
import torch.jit as jit

# 定义一个简单的模型
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc = nn.Linear(10, 5)

    def forward(self, x):
        return torch.relu(self.fc(x))

# 创建模型实例
model = MyModel()

# 使用 scripting 将模型转换为 TorchScript
scripted_model = jit.script(model)

# 打印 TorchScript IR
print(scripted_model.graph)
```

在上面的代码中，我们定义了一个简单的模型 MyModel，并使用 torch.jit.script 方法将模型转换为 TorchScript。然后，我们通过打印 scripted_model.graph 来查看 TorchScript IR。

打印出来的结果如下：

```
graph(%self : __torch__.___torch_mangle_3.MyModel,
      %x.1 : Tensor):
  %fc : __torch__.torch.nn.modules.linear.___torch_mangle_2.Linear = prim::GetAttr[name="fc"](%self)
  %4 : Tensor = prim::CallMethod[name="forward"](%fc, %x.1) # <ipython-input-6-5a8b04b374f6>:12:26
  %5 : Tensor = aten::relu(%4) # <ipython-input-6-5a8b04b374f6>:12:15
  return (%5)
```

我们可以看到，打印出来的结果是一个图结构，每个节点表示一个操作，如 prim::GetAttr、prim::CallMethod 和 aten::relu。每个节点包含操作的输入和输出，以及操作的具体实现。

#### 9.2.5 torch.fx

torch.fx 是 PyTorch 的一个模块，用于对神经网络模型进行静态图形表示、变换和编译。torch.fx 提供了一种方法，可以将 PyTorch 的动态图模型转换为更容易分析和优化的静态图形表示。以下是如何使用 torch.fx 的基本步骤和示例。

基本步骤
- 定义 PyTorch 模型：定义一个常规的 PyTorch 模型。
- 使用 torch.fx.symbolic_trace 创建模型的 GraphModule：将模型转换为 torch.fx.GraphModule。
- 分析和变换图形：可以对生成的图形进行分析、优化或改写。
- 生成新的模型：使用变换后的图形生成新的模型。

我们来看一个例子：

```python
import torch
import torch.nn as nn
import torch.fx as fx

# 定义一个简单的 PyTorch 模型
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(10, 10)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(10, 5)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# 实例化模型
model = MyModel()

# 使用 torch.fx.symbolic_trace 创建模型的 GraphModule
traced_model = fx.symbolic_trace(model)

# 打印 GraphModule 的图形表示
print(traced_model.graph)

# 定义一个简单的变换 Pass
class MyPass(fx.Transformer):
    def call_function(self, target, args, kwargs):
        # 在调用函数前插入一个 print 语句
        if target == torch.relu:
            print(f"ReLU is called with args: {args}, kwargs: {kwargs}")
        return super().call_function(target, args, kwargs)

# 应用变换 Pass
transformed_model = MyPass(traced_model).transform()

# 打印变换后的 GraphModule 的图形表示
print(transformed_model.graph)

# 使用变换后的模型进行推理
input_tensor = torch.randn(1, 10)
output = transformed_model(input_tensor)
print(output)
```

输出如下：

```
graph():
    %x : [num_users=1] = placeholder[target=x]
    %fc1 : [num_users=1] = call_module[target=fc1](args = (%x,), kwargs = {})
    %relu : [num_users=1] = call_module[target=relu](args = (%fc1,), kwargs = {})
    %fc2 : [num_users=1] = call_module[target=fc2](args = (%relu,), kwargs = {})
    return fc2
graph():
    %x : [num_users=1] = placeholder[target=x]
    %fc1 : [num_users=1] = call_module[target=fc1](args = (%x,), kwargs = {})
    %relu : [num_users=1] = call_module[target=relu](args = (%fc1,), kwargs = {})
    %fc2 : [num_users=1] = call_module[target=fc2](args = (%relu,), kwargs = {})
    return fc2
tensor([[ 0.3579,  0.0094, -0.9394,  1.0753,  0.9414]],
       grad_fn=<AddmmBackward0>)
```

### 9.3 优化方法

优化方法的公式比较多，前面已经简单介绍过了，这里我们结合编程方法继续详细讲解下。

#### 9.3.1 动量方法

动量梯度下降（Momentum Gradient Descent）是一种优化算法，它在标准梯度下降算法的基础上引入了动量的概念，以加速收敛和减少振荡。动量的引入可以帮助优化器在梯度方向上积累动量，从而在平坦区域或山谷中更快地前进，并在陡峭区域减少振荡。

在标准梯度下降算法中，参数更新的公式如下：

$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$

其中：
- $\theta_t$ 是第 $t$ 次迭代时的参数。
- $\eta$ 是学习率。
- $\nabla_\theta L(\theta_t)$ 是损失函数 $L$ 对参数 $\theta$ 的梯度。

动量梯度下降在参数更新时引入了一个动量项，更新公式如下：

$$ v_{t+1} = \gamma v_t + \eta \nabla_\theta L(\theta_t) $$

$$ \theta_{t+1} = \theta_t - v_{t+1} $$

其中：
- $v_t$ 是动量项，表示之前梯度的指数加权平均。
- $\gamma$ 是动量系数，通常取值在 $0.9$ 左右，表示动量的衰减因子。
- 其他符号与标准梯度下降中的含义相同。

动量的引入相当于在参数更新时增加了一个惯性，使得优化过程能够更平滑地进行。具体来说：

- 加速收敛：在梯度方向一致的情况下（例如在山谷中），动量项会积累梯度，使得参数更新的步伐加大，从而加速收敛。
- 减少振荡：在梯度方向频繁变化的情况下（例如在陡峭区域），动量项会平滑这些变化，减少振荡。


动量梯度下降没有自己单独的优化器类，而是作为随机梯度下降的一个参数。

```python
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

momentum参数的值就是动量系数$\gamma$.

Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）是对动量梯度下降的一种改进，由俄罗斯数学家 Yurii Nesterov 提出。这种方法在动量梯度下降的基础上，通过对未来位置的梯度进行估计，从而进一步加速收敛。

具体来说，它在计算梯度时，不是基于当前参数位置，而是基于当前参数位置加上动量项的一个估计值。这一估计值是对未来位置的预测，从而使得更新更为准确，收敛更快。

Nesterov 加速梯度的更新公式如下：

$$ v_{t+1} = \gamma v_t + \eta \nabla_\theta L(\theta_t - \gamma v_t) $$

$$ \theta_{t+1} = \theta_t - v_{t+1} $$

其中：

- $\theta_t - \gamma v_t$ 是对未来位置的预测。
- 其他符号与动量梯度下降中的含义相同。

Nesterov 加速梯度的核心思想是：在计算梯度时，不是基于当前的参数位置，而是基于当前参数位置加上动量项的一个估计值。这样可以避免动量带来的“惯性”导致的方向偏移，使得梯度更新更为准确。

在 PyTorch 中，Nesterov 加速梯度同样只是一个参数，可以通过设置 nesterov=True 来实现：

```python
# 使用 Nesterov 加速梯度的 SGD 优化器
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)
```

#### 9.3.2 自适应方法 AdaGrad

大家在编程的时候会发现，即使是带了动量的随机梯度下降，也有可能会出现学习率过大或者过小的情况，这时候就需要自适应学习率优化算法。
自适应学习率优化算法是一类根据参数梯度自适应调整学习率的优化算法。这类算法通常能够更快地收敛，并且对于不同参数具有不同的学习率，从而更好地适应不同参数的特性。

AdaGrad（Adaptive Gradient Algorithm）是一种自适应学习率优化算法。其核心思想是根据参数的历史梯度信息来调整每个参数的学习率，使得经常更新的参数学习率降低，而不常更新的参数学习率升高。

AdaGrad 的关键在于对每个参数维护一个累积梯度平方和，并使用这个累积量来调整学习率。具体来说，对于每个参数 $\theta_i$，学习率的调整公式如下：

$$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, i} + \epsilon}} g_{t, i} $$

其中：
- $\eta$ 是全局学习率（初始学习率）。
- $G_{t, i}$ 是第 $i$ 个参数的梯度平方和，即 $G_{t, i} = \sum_{k=1}^t g_{k, i}^2$。
- $g_{t, i}$ 是损失函数对第 $i$ 个参数在第 $t$ 步的梯度。
- $\epsilon$ 是一个小常数，用于防止除零。

AdaGrad的更新公式为：

$ g_{t, i} = \nabla_{\theta_i} L(\theta_t) $ (1-梯度计算公式) 

$ G_{t, i} = \sum_{k=1}^t g_{k, i}^2 $ (2-累积梯度平方和公式)

$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{G_{t, i} + \epsilon}} g_{t, i} $ (3-参数更新公式)


我们先来看看这个公式(1)-梯度计算公式:

$g_{t, i}$：表示在第 $t$ 次迭代中，损失函数 $L(\theta_t)$ 对参数 $\theta_i$ 的梯度。

$\nabla_{\theta_i} L(\theta_t)$：表示损失函数 $L(\theta_t)$ 对参数 $\theta_i$ 的偏导数，即梯度。

这个公式表示在第 $t$ 次迭代时，计算损失函数 $L(\theta_t)$ 对当前参数 $\theta_i$ 的梯度。这个梯度 $g_{t, i}$ 表明参数 $\theta_i$ 在当前迭代中的变化方向和变化率。

接下来我们看看公式(2)-累积梯度平方和公式：

$G_{t, i}$：表示从第 1 次迭代到第 $t$ 次迭代，参数 $\theta_i$的梯度平方的累积和。

$\sum_{k=1}^t g_{k, i}^2$：表示从第 1 次迭代到第 $t$ 次迭代，所有 $g_{k, i}$ 的平方和。

公式（2）表示在第 $t$ 次迭代时，计算参数 $\theta_i$ 从第 1 次迭代到第 $t$ 次迭代的梯度平方和。这一累积量 $G_{t, i}$ 被用来调整学习率，使得学习率能够根据梯度的历史变化情况进行自适应调整。

最后我们看看公式(3)：

- $\theta_{t+1, i}$：表示在第 $t+1$次迭代中参数$\theta_i$的新值。
- $\theta_{t, i}$：表示在第 $t$ 次迭代中参数 $\theta_i$的当前值。
- $\eta$：表示全局学习率（初始学习率）。
- $G_{t, i}$：表示第 $t$次迭代时参数 $\theta_i$的累积梯度平方和（如上公式所示）。
- $\epsilon$：是一个小常数，用于防止除零。
- $g_{t, i}$：表示在第$t$次迭代中，损失函数 $L(\theta_t)$ 对参数 $\theta_i$的梯度。

AdaGrad的优点和缺点为：

- 优点
    - 自适应学习率：不同参数有不同的学习率，适应性好，尤其是在稀疏数据情境下。
    - 无需手动调整学习率：由于学习率是自适应的，减少了超参数调整的工作量。
- 缺点
    - 学习率单调递减：累积的梯度平方和不断增大，导致学习率不断减小，可能会导致学习率过小，进而导致训练过程过早停止。
    - 不适用非凸问题：在非凸优化问题中，可能导致收敛到次优解。

在 PyTorch 中，我们可以通过 optim.Adagrad 创建一个 AdaGrad 优化器，lr 参数表示初始学习率：

```python
# 使用 AdaGrad 优化器
optimizer = optim.Adagrad(model.parameters(), lr=0.01)
```

#### 9.3.3 RMSProp

RMSprop（Root Mean Square Propagation）是一种自适应学习率优化算法，由 Geoffrey Hinton 提出。它旨在解决 AdaGrad 算法在深度学习应用中的一些局限性，特别是学习率不断减小的问题。

RMSprop 通过对梯度平方的指数加权移动平均来调整学习率，从而避免了 AdaGrad 中累积梯度平方和不断增大导致学习率过小的问题。其核心公式如下：

梯度平方的指数加权移动平均:$E[g^2]_{t, i} = \gamma E[g^2]_{t-1, i} + (1 - \gamma) g_{t, i}^2$

参数更新:$\theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{E[g^2]_{t, i} + \epsilon}} g_{t, i}$

其中：
- $\eta$ 是学习率（通常较小）。
- $\gamma$ 是衰减率，通常取值在 0.9 到 0.99 之间。
- $E[g^2]_{t, i}$ 是第 $i$ 个参数的梯度平方的指数加权移动平均。
- $g_{t, i}$ 是损失函数对第 $i$ 个参数在第 $t$ 步的梯度。
- $\epsilon$ 是一个小常数，用于防止除零。

RMSProp优点

- 解决学习率不断减小的问题：通过指数加权移动平均，RMSprop 能够保持稳定的学习率，从而避免 AdaGrad 中学习率过小的问题。
- 更快的收敛速度：RMSprop 通过自适应调整学习率，可以更快地收敛到最优解。
- 适用于非凸优化问题：RMSprop 在处理复杂、非凸的损失函数时表现良好。

我们顺带介绍一下非凸优化问题。

非凸优化问题是指目标函数或约束条件中的至少一个是非凸的优化问题。在数学中，凸函数具有以下性质：如果任意两点之间的线段上的函数值不大于该线段两端点的函数值，则该函数是凸的。非凸函数则不满足此性质。

对于函数 $f(x)$，如果对任意的 $x_1, x_2$ 和 $\lambda \in [0, 1]$，都有$f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)$, 则称 \( f(x) \) 为凸函数。

如果函数 $f(x)$ 不满足上述凸函数定义，则称其为非凸函数。非凸函数可能具有多个局部极小值和局部极大值，这使得优化问题变得更加复杂。

非凸优化问题的特点:

- 多局部最优解：非凸优化问题可能存在多个局部最优解（局部极小值或极大值）。找到全局最优解变得更加困难，因为优化算法可能会陷入局部最优解。
- 复杂地形：非凸目标函数的图形可能非常复杂，具有许多凹陷和凸起，这使得优化算法难以导航。
- 全局收敛性差：许多优化算法在处理非凸优化问题时，可能无法保证收敛到全局最优解，尤其是梯度下降类方法容易陷入局部最优。

在 PyTorch 中，我们可以通过 optim.RMSprop 创建一个 RMSprop 优化器，lr 参数表示初始学习率：

```python
# 使用 RMSprop 优化器
optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
```

我们来介绍一下参数：

- lr=0.01:学习率。这是每次参数更新时的步长。学习率决定了模型参数更新的速度。过大的学习率可能导致训练过程不稳定，过小的学习率可能导致收敛速度过慢。这里设置为 0.01。
- alpha=0.99:衰减率。这是梯度平方的指数加权移动平均的系数。它决定了历史梯度对当前梯度的影响程度。值越接近 1，历史梯度的影响越大。这里设置为 0.99，意味着给历史梯度较高的权重。
- eps=1e-08: epsilon。用于防止除零操作的小常数。
- weight_decay=0:权重衰减（L2 正则化）。这是用于防止过拟合的一种方法，通过在损失函数中添加参数值的平方和来实现。0 表示不进行权重衰减。
momentum=0:
- 动量。这是用于加速梯度下降算法的一种方法，通过在梯度更新中加入之前梯度的某个比例来实现。0 表示不使用动量。
- centered=False: 中心化。如果为 True，则梯度的均值将被从梯度平方的移动平均中减去。这种方法可以在某些情况下提高性能。这里设置为 False。

#### 9.3.4 Adam算法

Adam（Adaptive Moment Estimation）是一种结合了动量和 RMSprop 优点的自适应学习率优化算法。Adam 算法在处理稀疏梯度和噪声较大的梯度时效果显著，因此在深度学习领域中被广泛使用。

Adam 算法通过计算梯度的一阶矩（动量）和二阶矩（加速度）的指数加权移动平均来更新参数。它的核心思想是结合了动量优化和 RMSprop 优化的优点。

梯度的一阶矩估计（动量）:$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$

梯度的二阶矩估计（加速度）:$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$

由于在初始阶段， $m_t$ 和 $v_t$ 都偏向于零，需要进行偏差校正：

$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$

$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

参数更新：

$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$

其中：
- $\theta_t$是第 $t$ 步的参数。
- $g_t$ 是损失函数对参数的梯度。
- $\eta$ 是学习率。
- $\beta_1$ 和 $\beta_2$ 是分别控制一阶矩和二阶矩衰减率的参数，通常取值为 0.9 和 0.999。
- $\epsilon$ 是一个小常数，用于防止除零，通常取值为 $10^{-8}$。

调用Adam优化器的方法很简单：

```python
# 使用 Adam 优化器
optimizer = optim.Adam(model.parameters(), lr=0.01)
```

#### 9.3.5 AdamW算法


AdamW 是 Adam（Adaptive Moment Estimation）优化算法的变体，改进了权重衰减（weight decay）机制。AdamW 由 Ilya Loshchilov 和 Frank Hutter 在他们的论文《Decoupled Weight Decay Regularization》中提出。与传统的 Adam 不同，AdamW 将权重衰减从梯度更新过程中分离出来，从而提高了模型的正则化效果和训练性能。

在传统的 Adam 优化算法中，权重衰减（L2 正则化）作为损失函数的一部分被引入到梯度计算中。这种方式的权重衰减实际上改变了梯度的计算方式，可能导致不理想的正则化效果。

AdamW 将权重衰减从梯度更新过程中分离出来，使得权重衰减只影响参数本身，而不影响梯度的计算。这种方法能够更好地控制参数的缩减，从而提高正则化效果。

AdamW的前三个公式跟Adam一样：

1. 梯度的一阶矩估计（动量）：$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$

2. 梯度的二阶矩估计（加速度）：$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$

3. 偏差校正：
   $\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$

   $\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

注意，权重衰减项$\eta \lambda \theta_t$被明确地从梯度更新中分离出来。

4. 参数更新：$\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \eta \lambda \theta_t$

AdamW算法的优点：

- 更好的正则化效果：通过将权重衰减与梯度更新分离，AdamW 能够更好地控制参数的缩减，提高模型的泛化能力。
- 更稳定的训练过程：AdamW 改进了传统 Adam 在处理权重衰减时的不稳定性，特别是在训练深度神经网络时表现尤为显著。
- 易于实现：AdamW 只需要对参数更新公式进行简单的修改，易于在现有的深度学习框架中实现。

在 PyTorch 中，我们可以通过 optim.AdamW 创建一个 AdamW 优化器，lr 参数表示初始学习率, weight_decay 参数表示权重衰减系数：

```python
# 使用 AdamW 优化器
optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)
```

### 9.4 PyTorch前馈神经网络编程

#### 9.4.1 占位符

`torch.nn.Identity` 是 PyTorch 中的一个神经网络模块，它不对输入进行任何操作，直接返回输入。这在需要占位符层或在模型中进行动态调整时特别有用。

语法为：

```python
torch.nn.Identity(*args, **kwargs)
```

我们来看一个例子：

```python
import torch
import torch.nn as nn

# 定义一个简单的模型，包含一个占位符层
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.placeholder = nn.Identity()
        self.fc2 = nn.Linear(5, 2)

    def forward(self, x):
        x = self.fc1(x)
        x = self.placeholder(x)  # 占位符层，不做任何操作
        x = self.fc2(x)
        return x

# 创建模型实例并进行前向传播
model = SimpleModel()
input_data = torch.randn(1, 10)
output = model(input_data)
print(output)
```

在上面的代码中，我们定义了一个简单的模型 SimpleModel，包含一个占位符层 nn.Identity。在模型的 forward 方法中，我们先通过全连接层 fc1 对输入进行线性变换，然后通过占位符层 placeholder 返回输入，最后再通过全连接层 fc2 进行线性变换。

#### 9.4.2 全连接层

`torch.nn.Linear` 是 PyTorch 中的一个模块，用于实现一个线性变换 $y = xA^T + b$，其中 $x$ 是输入，$A$ 是权重矩阵，$b$ 是偏置向量。

```python
torch.nn.Linear(in_features, out_features, bias=True)
```

参数

- in_features：输入张量的特征数。
- out_features：输出张量的特征数。
- bias（可选）：是否使用偏置。默认值为 True。

我们来看下Linear层的基本用法：

```python
import torch
import torch.nn as nn

# 定义一个线性层
linear = nn.Linear(in_features=3, out_features=2)

# 创建一个输入张量
input_tensor = torch.randn(4, 3)  # 形状为 (batch_size, in_features)

# 进行前向传播
output_tensor = linear(input_tensor)

print("Input Tensor:\n", input_tensor)
print("Output Tensor:\n", output_tensor)
```

在上面的代码中，我们首先定义了一个线性层 linear，输入特征数为 3，输出特征数为 2。然后我们创建了一个形状为 (4, 3) 的输入张量 input_tensor，并通过 linear 进行前向传播，得到输出张量 output_tensor。

然后我们看一下如在模型中使用 Linear 层
在构建神经网络模型时，线性层常用于全连接层。

```python
import torch
import torch.nn as nn

# 定义一个简单的神经网络模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(in_features=10, out_features=5)
        self.fc2 = nn.Linear(in_features=5, out_features=1)

    def forward(self, x):
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x

# 创建模型实例
model = SimpleModel()

# 创建一个输入张量
input_tensor = torch.randn(2, 10)  # 形状为 (batch_size, in_features)

# 进行前向传播
output_tensor = model(input_tensor)

print("Input Tensor:\n", input_tensor)
print("Output Tensor:\n", output_tensor)
```

在上面的代码中，我们定义了一个简单的神经网络模型 SimpleModel，包含两个线性层 fc1 和 fc2。在模型的 forward 方法中，我们先通过 fc1 对输入进行线性变换，然后通过 ReLU 激活函数进行非线性变换，最后再通过 fc2 进行线性变换。

最后我们看一下如何初始化 Linear 层的权重和偏置

```python
import torch
import torch.nn as nn

# 定义一个线性层
linear = nn.Linear(in_features=3, out_features=2)

# 自定义初始化权重和偏置
nn.init.constant_(linear.weight, 0.5)
nn.init.constant_(linear.bias, 0.1)

# 创建一个输入张量
input_tensor = torch.randn(4, 3)

# 进行前向传播
output_tensor = linear(input_tensor)

print("Initialized Weights:\n", linear.weight)
print("Initialized Bias:\n", linear.bias)
print("Output Tensor:\n", output_tensor)
```

在上面的代码中，我们首先定义了一个线性层 linear，输入特征数为 3，输出特征数为 2。然后我们通过 nn.init.constant_ 自定义初始化了权重和偏置，将权重初始化为 0.5，偏置初始化为 0.1。最后我们创建了一个形状为 (4, 3) 的输入张量 input_tensor，并通过 linear 进行前向传播，得到输出张量 output_tensor。

#### 9.4.3 双线性层 Bilinear

`torch.nn.Bilinear` 是一个双线性层，它接受两个输入张量 $x_1$ 和 $x_2$，并计算双线性变换：$y = x_1^T W x_2 + b$

其中 $W$ 是一个权重张量，$b$ 是一个偏置向量。

```python
torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)
```

参数

- in1_features：第一个输入张量的特征数。
- in2_features：第二个输入张量的特征数。
- out_features：输出张量的特征数。
- bias（可选）：是否使用偏置。默认值为 True。

我们来看一个使用双线性层的简单例子：

```python
import torch
import torch.nn as nn

# 定义一个双线性层
bilinear = nn.Bilinear(in1_features=3, in2_features=4, out_features=2)

# 创建两个输入张量
input1 = torch.randn(5, 3)  # 形状为 (batch_size, in1_features)
input2 = torch.randn(5, 4)  # 形状为 (batch_size, in2_features)

# 进行前向传播
output = bilinear(input1, input2)

print("Input1:\n", input1)
print("Input2:\n", input2)
print("Output:\n", output)
```

#### 9.4.4 线性层的延迟初始化

`torch.nn.LazyLinear` 是一种延迟初始化的线性层，允许在创建层时不指定输入特征数。它将在第一次输入数据时自动推断输入特征数，并初始化权重和偏置。

```python
torch.nn.LazyLinear(out_features, bias=True)
```

参数

- out_features：输出张量的特征数。
- bias（可选）：是否使用偏置。默认值为 True。

我们来看一个使用延迟初始化的线性层的例子：

```python
import torch
import torch.nn as nn

# 定义一个 LazyLinear 层，不指定输入特征数
lazy_linear = nn.LazyLinear(out_features=5)

# 创建一个输入张量
input_tensor = torch.randn(3, 10)  # 形状为 (batch_size, in_features)

# 进行前向传播，自动推断输入特征数并初始化权重
output_tensor = lazy_linear(input_tensor)

print("Input Tensor:\n", input_tensor)
print("Output Tensor:\n", output_tensor)
print("Weight Shape:", lazy_linear.weight.shape)
print("Bias Shape:", lazy_linear.bias.shape)
```

在上面的代码中，我们定义了一个 LazyLinear 层 lazy_linear，输出特征数为 5。然后我们创建了一个形状为 (3, 10) 的输入张量 input_tensor，并通过 lazy_linear 进行前向传播。由于 LazyLinear 层是延迟初始化的，不需要指定输入特征数，它会在第一次输入数据时自动推断输入特征数，并初始化权重和偏置。

代码输出如下：
```
3]
0 秒
output_tensor = lazy_linear(input_tensor)

print("Input Tensor:\n", input_tensor)
print("Output Tensor:\n", output_tensor)
print("Weight Shape:", lazy_linear.weight.shape)
print("Bias Shape:", lazy_linear.bias.shape)
Input Tensor:
 tensor([[ 0.9465,  0.2032, -0.9759,  0.2553,  1.0981, -0.6491,  0.7495,  0.3235,
         -1.2605,  0.0932],
        [ 0.9067,  0.7694,  1.9935,  1.1480, -1.8206,  0.4429,  0.6787,  0.9024,
         -1.0095,  0.4598],
        [-1.9434, -0.7695,  1.7412, -0.8570,  2.4540,  0.3592, -1.0942, -0.6389,
         -1.7951, -0.1588]])
Output Tensor:
 tensor([[ 0.4927, -0.2000, -0.4663,  0.1465, -0.0747],
        [-1.5104,  0.4840, -0.3354, -0.0209, -0.9627],
        [ 0.8565,  0.3604,  0.6518,  1.4731, -0.1947]],
       grad_fn=<AddmmBackward0>)
Weight Shape: torch.Size([5, 10])
Bias Shape: torch.Size([5])
```

#### 9.5 JAX实现神经网络

PyTorch的封装做得太好，往往让我们看不清底层的实现。下面我们用JAX写一个不那么封装的，但是其实代码量也没有增加多少。

```python
# 定义神经网络模型
def relu(x):
    return jnp.maximum(0, x)

def predict(params, X):
    hidden = relu(jnp.dot(X, params['W1']) + params['b1'])
    logits = jnp.dot(hidden, params['W2']) + params['b2']
    return logits

# 定义损失函数
def loss_fn(params, X, y):
    logits = predict(params, X)
    log_probs = jax.nn.log_softmax(logits)
    loss = -jnp.mean(jnp.sum(log_probs * y, axis=1))
    return loss

# 梯度下降
grad_fn = jax.jit(jax.grad(loss_fn))
```

relu的本质我们前面介绍过了，所以它就是maximum(0,x). 

下面是计算隐藏层：

`jnp.dot(X, params['W1'])`：计算输入数据 X 与权重矩阵 W1 的矩阵乘法，结果是一个矩阵，其中每一行是一个样本的隐藏层线性组合。

`+ params['b1']`：将偏置 b1 加到每个样本的隐藏层线性组合中。

`relu(...)`：对上述结果应用 ReLU（Rectified Linear Unit）激活函数。ReLU 是一种非线性激活函数，它将所有负值变为零，正值保持不变。


我们再看计算输出层的：

`jnp.dot(hidden, params['W2'])`：计算隐藏层输出 hidden 与权重矩阵 W2 的矩阵乘法，结果是一个矩阵，其中每一行是一个样本的输出层线性组合。
`+ params['b2']`：将偏置 b2 加到每个样本的输出层线性组合中。这一步的结果称为 logits。

网络输出的未归一化的概率分布（logits）。在分类问题中，这些 logits 通常会被进一步处理，例如通过 softmax 函数转换为概率分布。

损失函数就是预测的结果和真实结果的交叉熵。

我们加上用梯度下降进行训练的部分：

```python
# 训练模型
learning_rate = 0.01
epochs = 1000

for epoch in range(epochs):
    grads = grad_fn(params, X_train, y_train)
    params = jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)

    if epoch % 100 == 0:
        loss = loss_fn(params, X_train, y_train)
        print(f"Epoch {epoch}, Loss: {loss}")
```

这里面tree_map需要解释下。

`jax.tree_util.tree_map` 是一个便利函数，它可以对树状结构（如字典或命名元组）中的每个元素应用一个函数。
在这里，`lambda p, g: p - learning_rate * g` 是一个匿名函数，用于更新每个参数 p，减去其对应的梯度 g 乘以学习率。
这个操作会将所有参数更新为新的值，以便在下一次迭代中使用。

看起来唬人，其实就是参数减去学习率乘以梯度。

我们用鸢尾花数据集来将上面的代码串在一起：

```python
import jax
import jax.numpy as jnp
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 数据预处理
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One-hot 编码目标变量
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y.reshape(-1, 1))

# 分割数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 初始化参数
key = jax.random.PRNGKey(0)
input_dim = X_train.shape[1]
hidden_dim = 10
output_dim = y_train.shape[1]

def init_params(key):
    params = {
        'W1': jax.random.normal(key, (input_dim, hidden_dim)),
        'b1': jnp.zeros(hidden_dim),
        'W2': jax.random.normal(key, (hidden_dim, output_dim)),
        'b2': jnp.zeros(output_dim)
    }
    return params

params = init_params(key)

# 定义神经网络模型
def relu(x):
    return jnp.maximum(0, x)

def predict(params, X):
    hidden = relu(jnp.dot(X, params['W1']) + params['b1'])
    logits = jnp.dot(hidden, params['W2']) + params['b2']
    return logits

# 定义损失函数
def loss_fn(params, X, y):
    logits = predict(params, X)
    log_probs = jax.nn.log_softmax(logits)
    loss = -jnp.mean(jnp.sum(log_probs * y, axis=1))
    return loss

# 梯度下降
grad_fn = jax.jit(jax.grad(loss_fn))

# 训练模型
learning_rate = 0.01
epochs = 1000

for epoch in range(epochs):
    grads = grad_fn(params, X_train, y_train)
    params = jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)

    if epoch % 100 == 0:
        loss = loss_fn(params, X_train, y_train)
        print(f"Epoch {epoch}, Loss: {loss}")

# 测试模型
logits_train = predict(params, X_train)
logits_test = predict(params, X_test)

y_pred_train = jnp.argmax(logits_train, axis=1)
y_pred_test = jnp.argmax(logits_test, axis=1)

y_train_labels = jnp.argmax(y_train, axis=1)
y_test_labels = jnp.argmax(y_test, axis=1)

train_accuracy = accuracy_score(y_train_labels, y_pred_train)
test_accuracy = accuracy_score(y_test_labels, y_pred_test)

print(f"Train Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")
```

运行结果如下：
```
Epoch 0, Loss: 1.8908624649047852
Epoch 100, Loss: 0.6700484156608582
Epoch 200, Loss: 0.4600417912006378
Epoch 300, Loss: 0.3555840849876404
Epoch 400, Loss: 0.29059529304504395
Epoch 500, Loss: 0.24826818704605103
Epoch 600, Loss: 0.21808886528015137
Epoch 700, Loss: 0.19502606987953186
Epoch 800, Loss: 0.17778009176254272
Epoch 900, Loss: 0.1645180732011795
Train Accuracy: 0.9333333333333333
Test Accuracy: 1.0
```


## 第十章 卷积神经网络和循环神经网络

### 10.1 卷积神经网络简介

#### 10.1.1 卷积神经网络的基本概念

卷积神经网络（Convolutional Neural Network，CNN）是一种专门用于处理具有类似网格拓扑结构数据的深度学习模型，最常用于图像和视频分析。

卷积神经网络的一些关键组成部分有：

- 输入层：接受原始图像数据，通常是一个多通道（例如 RGB）的矩阵。
- 卷积层（Convolutional Layer）：通过卷积操作提取图像的局部特征。卷积操作使用若干滤波器（或称为卷积核）扫描图像，产生特征图。
- 激活函数（Activation Function）：通常使用非线性函数（如 ReLU）对卷积层的输出进行非线性变换，以引入非线性特征。
- 池化层（Pooling Layer）：通常在卷积层之后，用于下采样（通常是最大池化或平均池化），减少特征图的尺寸，从而降低计算复杂度并引入不变性。
- 全连接层（Fully Connected Layer）：类似于传统的神经网络层，连接所有神经元，用于综合前面的特征，输出分类结果或其他任务的结果。
- 输出层：用于给出最终的预测结果。例如，在图像分类任务中，输出层通常是一个 Softmax 层，用于输出各个类别的概率。


卷积神经网络的主要特点为：

- 局部连接（Local Connectivity）：每个神经元只连接一小部分输入数据，有助于提取局部特征。
- 权重共享（Weight Sharing）：在同一特征图中，卷积核的参数是共享的，这大大减少了参数数量，使模型更易于训练。
- 空间不变性（Spatial Invariance）：通过池化操作和卷积核的平移，对输入图像的平移、旋转、缩放等变换具有一定的不变性。

不过，也不要到了深度学习就神话它。本质上它也不过就是一个分类器。

我们先写一段代码来用CNN处理鸢尾花数据集：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt

# 加载和预处理数据
iris = load_iris()
X, y = iris.data, iris.target

# 标准化特征
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 将数据扩展到适合CNN输入的形状
X = X.reshape(X.shape[0], 1, 2, 2)  # 将4个特征重塑为1x2x2的图像

# 分割数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 转换为PyTorch的Tensor
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)

# 创建数据加载器
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# 定义CNN模型
class IrisCNN(nn.Module):
    def __init__(self):
        super(IrisCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=2)
        self.fc1 = nn.Linear(8, 32)
        self.fc2 = nn.Linear(32, 3)
    
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(x.size(0), -1)  # 扁平化
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 初始化模型、损失函数和优化器
model = IrisCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 50
train_losses = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    
    train_loss = running_loss / len(train_loader)
    train_losses.append(train_loss)
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}')

# 测试模型
model.eval()
correct = 0
total = 0

with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f'Test Accuracy: {accuracy:.2f}%')

# 绘制训练损失
plt.plot(train_losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()
```

#### 10.1.2 卷积核

卷积核（Convolutional Kernel），也被称为滤波器（Filter），是卷积层的核心组件。卷积核通过与输入数据进行卷积操作，提取局部特征。
更具体地说，卷积核是一个小的权重矩阵，通常比输入图像或特征图的尺寸要小。例如，常见的卷积核大小有 $3 \times 3$、$5 \times 5$ 或 $7 \times 7$ 等。卷积核的参数在训练过程中通过反向传播算法进行学习。



卷积操作是指将卷积核在输入数据上滑动，并在每个位置进行点积运算以生成输出特征图。具体步骤如下：

1. **选择区域**：从输入数据中选择一个与卷积核大小相同的局部区域。
2. **点积运算**：计算该区域与卷积核对应元素的乘积之和。
3. **生成特征值**：将计算结果作为输出特征图的一个像素值。
4. **滑动窗口**：按照预定的步幅（stride）移动卷积核，重复上述步骤，直到覆盖整个输入数据。

假设输入图像为 $ I $（大小为 $ H \times W $），卷积核为 $ K $（大小为 $ k_h \times k_w $），则卷积操作可以表示为：

$ (I * K)(i, j) = \sum_{m=0}^{k_h-1} \sum_{n=0}^{k_w-1} I(i+m, j+n) \cdot K(m, n) $

这里，$ (I * K)(i, j) $ 表示输出特征图在位置 $ (i, j) $ 的值。

对于多通道输入（如彩色图像），卷积核的深度与输入数据的通道数相同。假设输入有 $ C $ 个通道，则卷积核的形状为 $ k_h \times k_w \times C $。在这种情况下，卷积操作会对每个通道分别进行点积运算，然后将结果相加，得到最终的输出值。

在同一个卷积层中，卷积核的参数是共享的，这意味着同一个卷积核会在输入的不同区域重复使用。这显著减少了模型的参数数量和计算复杂度。

不同的卷积核可以学习到不同的特征，例如：
- 边缘
- 纹理
- 颜色

通过组合多个卷积核，卷积神经网络能够提取出输入数据的多层次特征。

假设有一个 $3 \times 3$ 的卷积核 $ K $ 和一个 $5 \times 5$ 的输入图像 $ I $，卷积操作过程如下：

```
输入图像 I:
1 1 1 0 0
0 1 1 1 0
0 0 1 1 1
0 0 1 1 0
0 1 1 0 0

卷积核 K:
1 0 1
0 1 0
1 0 1

输出特征图:
2 3 3 2
1 3 4 2
1 2 4 3
1 2 3 2
```

在这个例子中，卷积核在输入图像上滑动，并计算点积以生成输出特征图。

通过卷积核的滑动和计算，卷积神经网络能够有效地提取输入数据的局部特征，进而进行更高层次的特征学习和模式识别。

在PyTorch中，可以使用 `torch.nn.Conv2d` 类创建卷积层。下面是一个简单的示例：

```python
import torch
import torch.nn as nn

# 创建一个简单的卷积层
# 参数说明：
# - in_channels: 输入通道数，例如灰度图像是1，RGB图像是3
# - out_channels: 输出通道数，也就是卷积核的数量
# - kernel_size: 卷积核的大小，可以是单个整数或者一个表示高和宽的元组
# - stride: 卷积的步幅，默认为1
# - padding: 填充，默认为0
conv_layer = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=1)

# 创建一个输入张量（例如一个1通道的28x28图像）
# .unsqueeze(0) 用于添加一个维度，表示批次大小(batch size)，这里是1
input_tensor = torch.randn(1, 1, 28, 28)

# 应用卷积层
output_tensor = conv_layer(input_tensor)

# 输出张量的形状
print(f"Output tensor shape: {output_tensor.shape}")
```

而在JAX中，没有现成的卷积层，我们需要自己实现。下面是一个简单的卷积操作的实现：

```python
import jax
import jax.numpy as jnp
from jax import random

import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

def conv2d(x, W, b, strides=(1, 1), padding='VALID'):
    # 使用 jax.lax.conv_general_dilated 进行卷积操作
    return jax.lax.conv_general_dilated(
        x, W, window_strides=strides, padding=padding,
        dimension_numbers=('NHWC', 'HWIO', 'NHWC')
    ) + b

# 初始化卷积层参数
def initialize_conv_params(rng, input_channels, output_channels, kernel_size):
    k1, k2 = random.split(rng)
    W = random.normal(k1, (kernel_size, kernel_size, input_channels, output_channels))
    b = jnp.zeros(output_channels)
    return W, b

# 设置随机种子
rng = random.PRNGKey(0)

# 初始化卷积层参数
input_channels = 1  # 输入通道数，例如灰度图像是1，RGB图像是3
output_channels = 3  # 输出通道数，也就是卷积核的数量
kernel_size = 3  # 卷积核的大小

W, b = initialize_conv_params(rng, input_channels, output_channels, kernel_size)

# 创建一个输入张量（例如一个1通道的28x28图像）
input_tensor = random.normal(rng, (1, 28, 28, 1))

# 应用卷积层
output_tensor = conv2d(input_tensor, W, b, strides=(1, 1), padding='SAME')

# 输出张量的形状
print(f"Output tensor shape: {output_tensor.shape}")
```

`jax.lax.conv_general_dilated` 是 JAX 提供的一个低级接口，用于执行高度可定制的卷积操作。它非常灵活，可以支持各种卷积变体，包括普通卷积、池化、跨步卷积和空洞卷积。

conv_general_dilated的参数如下：

```python
jax.lax.conv_general_dilated(
    lhs,  # 左侧输入张量
    rhs,  # 右侧卷积核张量
    window_strides,  # 步幅
    padding,  # 填充方式
    lhs_dilation=None,  # 输入张量的扩张
    rhs_dilation=None,  # 卷积核的扩张
    dimension_numbers=None,  # 维度顺序
    feature_group_count=1,  # 特征组的数量（用于分组卷积）
    batch_group_count=1,  # 批次组的数量
    precision=None  # 运算的精度
)
```

参数解释

- lhs：输入张量，通常是图像或特征图。
- rhs：卷积核张量。
- window_strides：卷积的步幅。
- padding：填充方式，可以是 'VALID' 或 'SAME'，也可以是一个具体的填充元组。
- lhs_dilation：输入张量的扩张因子，用于空洞卷积。
- rhs_dilation：卷积核的扩张因子，用于空洞卷积。
- dimension_numbers：指定输入、卷积核和输出的维度顺序。
- feature_group_count：特征组的数量，用于分组卷积。
- batch_group_count：批次组的数量。
- precision：运算的精度。

#### 10.1.3 池化层

池化层（Pooling Layer）是卷积神经网络（Convolutional Neural Network，CNN）中的一种常用层类型，主要用于减小特征图（feature map）的尺寸，同时保留重要的特征信息。池化层通过下采样（subsampling）操作减少计算量，控制过拟合，并提高模型的鲁棒性。

池化层主要有两种类型：

- 最大池化（Max Pooling）：选取池化窗口内的最大值作为池化结果。
- 平均池化（Average Pooling）：计算池化窗口内所有值的平均值作为池化结果。

池化层通常有以下几个参数：

- 池化窗口大小（kernel size）：定义池化操作的窗口大小，如 2×2 或 3×3。
- 步幅（stride）：定义池化窗口在特征图上滑动的步长。
- 填充（padding）：定义池化操作是否在特征图边缘填充额外的值，通常是零填充（zero padding）。

池化层的作用

- 降维和减少计算量：池化层通过下采样操作减小了特征图的尺寸，从而减少了后续层的计算量。
- 控制过拟合：通过降低网络参数的数量，池化层有助于减少过拟合现象。
- 提高鲁棒性：池化操作保留了重要的特征信息，同时忽略了不重要的细节，使得网络对输入数据的微小变动更加鲁棒。
- 提取空间不变性：池化层通过下采样保留了特征图的空间分布信息，有助于提取空间不变性特征。


在PyTorch中，提供了最大池化层、平均池化层和全局平均池化层等池化层。

- 最大池化层：最大池化层在每个池化窗口内选择最大值，从而减小特征图的尺寸。

```python
import torch
import torch.nn as nn

# 创建一个最大池化层
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)

# 创建一个示例输入张量
input_tensor = torch.tensor([[[[1, 2, 3, 4],
                               [5, 6, 7, 8],
                               [9, 10, 11, 12],
                               [13, 14, 15, 16]]]], dtype=torch.float32)

# 应用最大池化层
output_tensor = max_pool(input_tensor)

print(output_tensor)
```

- 平均池化层: 平均池化层在每个池化窗口内计算平均值，从而减少特征图的尺寸。

```python
import torch
import torch.nn as nn

# 创建一个平均池化层
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)

# 创建一个示例输入张量
input_tensor = torch.tensor([[[[1, 2, 3, 4],
                               [5, 6, 7, 8],
                               [9, 10, 11, 12],
                               [13, 14, 15, 16]]]], dtype=torch.float32)

# 应用平均池化层
output_tensor = avg_pool(input_tensor)

print(output_tensor)
```

- 全局平均池化层:全局平均池化层计算特征图的全局平均值，通常用于将二维特征图转换为一维特征向量。

```python

import torch
import torch.nn as nn

# 创建一个示例输入张量
input_tensor = torch.tensor([[[[1, 2, 3, 4],
                               [5, 6, 7, 8],
                               [9, 10, 11, 12],
                               [13, 14, 15, 16]]]], dtype=torch.float32)

# 使用自适应平均池化层实现全局平均池化
global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))

# 应用全局平均池化层
output_tensor = global_avg_pool(input_tensor)

print(output_tensor)
```

在JAX中，池化的操作叫做窗口归约：jax.lax.reduce_window。

其定义如下：

```python
jax.lax.reduce_window(
    operand,  # 输入张量
    init_value,  # 初始值
    computation,  # 归约操作
    window_dimensions,  # 窗口大小
    window_strides,  # 窗口步幅
    padding,  # 填充方式
    base_dilation=None,  # 输入张量的扩张因子
    window_dilation=None  # 窗口的扩张因子
)
```

参数解释

- operand：输入张量。
- init_value：归约操作的初始值，例如，对于最大池化，初始值可以是 -inf。
- computation：归约操作，例如 jax.lax.max、jax.lax.add 等。
- window_dimensions：窗口大小的元组。
- window_strides：窗口移动的步幅。
- padding：填充方式，可以是 'VALID' 或 'SAME'，也可以是一个具体的填充元组。
- base_dilation：输入张量的扩张因子（可选）。
- window_dilation：窗口的扩张因子（可选）。

### 10.2 循环神经网络

#### 10.2.1 循环神经网络的基本概念

循环神经网络（Recurrent Neural Network, RNN）是一类用于处理序列数据的神经网络。与传统的前馈神经网络不同，RNN具有循环连接的特性，使其能够在序列的每个时间步上保持并更新一个隐藏状态，从而捕捉到输入数据的时间依赖性。

一个典型的RNN单元包含以下部分：

1. 输入层：接受当前时间步的输入 $x_t$。
2. 隐藏层：维护一个隐藏状态 $h_t$，该状态根据当前输入 $x_t$ 和前一时间步的隐藏状态 $h_{t-1}$ 更新。
3. 输出层：生成当前时间步的输出 $y_t$。

RNN的核心计算可以表示为以下公式：

1. 隐藏状态更新：
$h_t = \sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$
其中，$\sigma$ 是激活函数（如tanh或ReLU），$W_{xh}$ 是输入到隐藏状态的权重矩阵，$W_{hh}$ 是隐藏状态到隐藏状态的权重矩阵，$b_h$ 是隐藏层的偏置。

2. 输出计算：
$y_t = W_{hy} h_t + b_y$

其中，$W_{hy}$ 是隐藏状态到输出的权重矩阵，$b_y$ 是输出层的偏置。

特点和优势

- 时间依赖性：RNN能够处理序列数据，捕捉到数据的时间依赖性，适用于时间序列预测、自然语言处理等任务。
- 参数共享：在不同时间步上共享相同的参数，使得模型能够有效地处理不同长度的序列。

RNN在实际应用中面临一些挑战，包括：

- 梯度消失和梯度爆炸：在长序列训练过程中，梯度可能会逐渐消失或爆炸，导致训练困难。
- 长距离依赖：标准RNN难以捕捉长序列中的长距离依赖关系。

为了克服这些问题，提出了几种改进的RNN变体，如：

- 长短期记忆网络（LSTM）：通过引入三个门（输入门、遗忘门和输出门）来控制信息的流动，能够更好地捕捉长距离依赖关系。
- 门控循环单元（GRU）：简化了LSTM的结构，但保留了其捕捉长距离依赖的能力。

####  10.2.2 长短期记忆网络（LSTM）

长短期记忆网络（Long Short-Term Memory, LSTM）是一种特殊的循环神经网络（RNN），设计用于解决标准RNN在处理长序列时遇到的梯度消失和梯度爆炸问题。LSTM通过引入门机制来控制信息的流动，能够更好地捕捉长时间依赖关系。

LSTM的特点和优势有：

- 捕捉长时间依赖：LSTM通过门机制能够有效地捕捉序列中长时间的依赖关系。
- 解决梯度消失问题：通过细胞状态的线性变化，LSTM能够避免梯度消失问题，从而在长序列数据上表现良好。

LSTM单元比标准RNN单元复杂得多，包含三个主要的门：输入门、遗忘门和输出门。这些门通过控制细胞状态（cell state）来决定应该记住什么、遗忘什么和输出什么。以下是LSTM的核心组件：

1. 细胞状态（Cell State）：类似于RNN中的隐藏状态，但它可以携带长时间的信息。
2. 遗忘门（Forget Gate）：控制细胞状态中哪些信息应该被遗忘。
3. 输入门（Input Gate）：控制当前输入的信息如何影响细胞状态。
4. 输出门（Output Gate）：控制细胞状态中哪些信息应该输出。

LSTM的核心计算可以表示为以下公式：

1. 遗忘门：
$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
其中，$\sigma$ 是 sigmoid 激活函数，$W_f$ 是权重矩阵，$b_f$ 是偏置。

2. 输入门：

$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$

$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$

其中，$\tanh$ 是双曲正切激活函数，$W_i$ 和 $W_C$ 是权重矩阵，$b_i$ 和 $b_C$ 是偏置。

3. 更新细胞状态：

$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$

4. 输出门：
$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
$h_t = o_t \cdot \tanh(C_t)$

JAX没有现成的LSTM层，我们需要自己实现。正好结合上面的公式我们来实现一下。

```python
def lstm_cell(params, h, c, x):
    w_i, w_f, w_c, w_o, b_i, b_f, b_c, b_o = params
    input_and_h = jnp.concatenate([x, h], axis=-1)
    
    i = jax.nn.sigmoid(jnp.dot(input_and_h, w_i) + b_i)
    f = jax.nn.sigmoid(jnp.dot(input_and_h, w_f) + b_f)
    g = jnp.tanh(jnp.dot(input_and_h, w_c) + b_c)
    o = jax.nn.sigmoid(jnp.dot(input_and_h, w_o) + b_o)
    
    new_c = f * c + i * g
    new_h = o * jnp.tanh(new_c)
    
    return new_h, new_c
```

我们先看遗忘门公式：$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$

对应的代码：

```python
f = jax.nn.sigmoid(jnp.dot(input_and_h, w_f) + b_f)
```

input_and_h就是 $[h_{t-1}, x_t]$，然后通过矩阵乘法和偏置，再通过sigmoid激活函数，就得到了遗忘门的输出。

再看输入门的公式：$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$

对应的代码：

```python
i = jax.nn.sigmoid(jnp.dot(input_and_h, w_i) + b_i)
```

$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$ 对应于代码中的g:

```python
g = jnp.tanh(jnp.dot(input_and_h, w_c) + b_c)
```

输出门：$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$

对应代码：

```python
o = jax.nn.sigmoid(jnp.dot(input_and_h, w_o) + b_o)
```

细胞状态更新：$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$

对应了new_c的计算：

```python
new_c = f * c + i * g
```

历史状态更新：$h_t = o_t \cdot \tanh(C_t)$

对应于new_h的计算：

```python
new_h = o * jnp.tanh(new_c)
```

然后我们写个LSTM结构将LSTM细胞串起来：

```python
def lstm_forward(params, x):
    batch_size, seq_len, input_size = x.shape
    num_layers = len(params) - 2  # 减去全连接层的参数
    hidden_size = params[0][0].shape[1]

    h = jnp.zeros((num_layers, batch_size, hidden_size))
    c = jnp.zeros((num_layers, batch_size, hidden_size))
    
    for t in range(seq_len):
        x_t = x[:, t, :]
        for l in range(num_layers):
            new_h, new_c = lstm_cell(params[l], h[l], c[l], x_t)
            h = h.at[l].set(new_h)
            c = c.at[l].set(new_c)
            x_t = new_h
    
    fc_w, fc_b = params[-2], params[-1]
    output = jnp.dot(h[-1], fc_w) + fc_b
    return output
```

对于序列中的每个时间步 t，以及每一层 l，调用 lstm_cell 函数计算新的隐藏状态和细胞状态，并更新 h 和 c。同时，将新的隐藏状态作为下一层的输入。

我们再看下损失函数：

```python
def mse_loss(params, x, y):
    preds = lstm_forward(params, x)
    return jnp.mean((preds - y) ** 2)
```

这里我们使用均方误差作为损失函数。

下面我们来看下训练过程：

```python
optimizer = optax.adam(learning_rate=0.001)
opt_state = optimizer.init(params)

@jax.jit
def update(params, opt_state, x, y):
    loss, grads = jax.value_and_grad(mse_loss)(params, x, y)
    updates, opt_state = optimizer.update(grads, opt_state)
    new_params = optax.apply_updates(params, updates)
    return new_params, opt_state, loss
```

这里有个新出场的库，optax，它是JAX的优化库，提供了一些常用的优化器的封装。Optax 专为 JAX 设计，充分利用了 JAX 的自动微分和硬件加速特性。

具体地说，optax.apply_updates 的作用是将优化器计算出的 updates 应用到模型的现有参数 params 上，生成新的参数 new_params。这个过程通常包括以下步骤：

- 计算梯度：通过反向传播或其他方法计算损失函数相对于每个参数的梯度。
- 计算更新量：使用优化器（如 Adam、SGD 等）根据梯度计算每个参数的更新量。
- 应用更新量：将更新量应用到现有参数上，得到新的参数。


下面我们把JAX实现LSTM的代码串到一起：

```python
import jax
import jax.numpy as jnp
from jax import random
import optax

import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

def lstm_cell(params, h, c, x):
    w_i, w_f, w_c, w_o, b_i, b_f, b_c, b_o = params
    input_and_h = jnp.concatenate([x, h], axis=-1)
    
    i = jax.nn.sigmoid(jnp.dot(input_and_h, w_i) + b_i)
    f = jax.nn.sigmoid(jnp.dot(input_and_h, w_f) + b_f)
    g = jnp.tanh(jnp.dot(input_and_h, w_c) + b_c)
    o = jax.nn.sigmoid(jnp.dot(input_and_h, w_o) + b_o)
    
    new_c = f * c + i * g
    new_h = o * jnp.tanh(new_c)
    
    return new_h, new_c

def init_lstm_params(key, input_size, hidden_size):
    k1, k2, k3, k4 = random.split(key, 4)
    w_i = random.normal(k1, (input_size + hidden_size, hidden_size))
    w_f = random.normal(k2, (input_size + hidden_size, hidden_size))
    w_c = random.normal(k3, (input_size + hidden_size, hidden_size))
    w_o = random.normal(k4, (input_size + hidden_size, hidden_size))
    b_i = jnp.zeros(hidden_size)
    b_f = jnp.zeros(hidden_size)
    b_c = jnp.zeros(hidden_size)
    b_o = jnp.zeros(hidden_size)
    return (w_i, w_f, w_c, w_o, b_i, b_f, b_c, b_o)

def lstm_forward(params, x):
    batch_size, seq_len, input_size = x.shape
    num_layers = len(params) - 2  # 减去全连接层的参数
    hidden_size = params[0][0].shape[1]

    h = jnp.zeros((num_layers, batch_size, hidden_size))
    c = jnp.zeros((num_layers, batch_size, hidden_size))
    
    for t in range(seq_len):
        x_t = x[:, t, :]
        for l in range(num_layers):
            new_h, new_c = lstm_cell(params[l], h[l], c[l], x_t)
            h = h.at[l].set(new_h)
            c = c.at[l].set(new_c)
            x_t = new_h
    
    fc_w, fc_b = params[-2], params[-1]
    output = jnp.dot(h[-1], fc_w) + fc_b
    return output

key = random.PRNGKey(0)
input_size = 10
hidden_size = 20
output_size = 1
num_layers = 2

keys = random.split(key, num_layers + 1)
params = [init_lstm_params(k, input_size if i == 0 else hidden_size, hidden_size) for i, k in enumerate(keys[:-1])]
fc_w = random.normal(keys[-1], (hidden_size, output_size))
fc_b = jnp.zeros(output_size)
params.append(fc_w)
params.append(fc_b)

def mse_loss(params, x, y):
    preds = lstm_forward(params, x)
    return jnp.mean((preds - y) ** 2)

optimizer = optax.adam(learning_rate=0.001)
opt_state = optimizer.init(params)

@jax.jit
def update(params, opt_state, x, y):
    loss, grads = jax.value_and_grad(mse_loss)(params, x, y)
    updates, opt_state = optimizer.update(grads, opt_state)
    new_params = optax.apply_updates(params, updates)
    return new_params, opt_state, loss

num_epochs = 100
batch_size = 32

# 生成示例数据
key = random.PRNGKey(1)
X_train = random.normal(key, (1000, 5, 10))  # 1000 个样本，每个样本包含 5 个时间步，每个时间步有 10 个特征
y_train = random.normal(key, (1000, 1))  # 1000 个样本的标签

# 将数据分成批次
def get_batches(X, y, batch_size):
    n_batches = len(X) // batch_size
    for i in range(n_batches):
        X_batch = X[i * batch_size:(i + 1) * batch_size]
        y_batch = y[i * batch_size:(i + 1) * batch_size]
        yield X_batch, y_batch

# 初始化参数
params = [init_lstm_params(k, input_size if i == 0 else hidden_size, hidden_size) for i, k in enumerate(keys[:-1])]
params.append(fc_w)
params.append(fc_b)

for epoch in range(num_epochs):
    for X_batch, y_batch in get_batches(X_train, y_train, batch_size):
        params, opt_state, loss = update(params, opt_state, X_batch, y_batch)
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}')

# 生成示例测试数据
X_test = random.normal(key, (200, 5, 10))  # 200 个测试样本
y_test = random.normal(key, (200, 1))

# 评估模型
predictions = lstm_forward(params, X_test)
test_loss = mse_loss(params, X_test, y_test)
print(f'Test Loss: {test_loss:.4f}')
```

在这个示例中，我们首先定义了一个 LSTM 单元 lstm_cell，然后使用 init_lstm_params 函数初始化 LSTM 的参数。接着，我们定义了 lstm_forward 函数，用于前向传播 LSTM。然后，我们定义了损失函数 mse_loss，这里使用均方误差作为损失函数。接下来，我们使用 optax.adam 初始化了 Adam 优化器，并定义了 update 函数，用于更新参数。最后，我们训练模型并评估模型的性能。

输出结果如下：

```
Epoch [10/100], Loss: 0.9156
Epoch [20/100], Loss: 0.5790
Epoch [30/100], Loss: 0.3752
Epoch [40/100], Loss: 0.2221
Epoch [50/100], Loss: 0.1113
Epoch [60/100], Loss: 0.0516
Epoch [70/100], Loss: 0.0227
Epoch [80/100], Loss: 0.0133
Epoch [90/100], Loss: 0.0062
Epoch [100/100], Loss: 0.0049
Test Loss: 2.3987
```

PyTorch中包含了LSTM的实现，可以通过 `torch.nn.LSTM` 类创建LSTM层。以下是一个简单的示例：

```python
import torch
import torch.nn as nn
import torch.optim as optim

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # 定义LSTM层
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        
        # 定义全连接层
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # 初始化 LSTM 的隐藏状态和细胞状态
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # 前向传播 LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # 取出 LSTM 最后一个时间步的输出
        out = out[:, -1, :]
        
        # 通过全连接层
        out = self.fc(out)
        return out

input_size = 10  # 输入特征的维度
hidden_size = 20  # 隐藏层的维度
num_layers = 2  # LSTM 层数
output_size = 1  # 输出特征的维度（例如回归任务中的一个值）

model = LSTMModel(input_size, hidden_size, num_layers, output_size)

criterion = nn.MSELoss()  # 例如用于回归任务的均方误差损失
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 生成一些示例数据
X_train = torch.rand((100, 5, input_size))  # 100 个样本，每个样本包含 5 个时间步，每个时间步有 input_size 个特征
y_train = torch.rand((100, output_size))  # 100 个样本的标签

num_epochs = 100

for epoch in range(num_epochs):
    model.train()
    
    # 前向传播
    outputs = model(X_train)
    loss = criterion(outputs, y_train)
    
    # 反向传播和优化
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

model.eval()
with torch.no_grad():
    X_test = torch.rand((20, 5, input_size))  # 20 个测试样本
    y_test = torch.rand((20, output_size))
    
    predictions = model(X_test)
    test_loss = criterion(predictions, y_test)
    print(f'Test Loss: {test_loss.item():.4f}')
```



#### 10.2.3 门控循环单元（GRU）

门控循环单元（Gated Recurrent Unit, GRU）是一种改进的循环神经网络（RNN），与长短期记忆网络（LSTM）类似，旨在解决普通RNN中的梯度消失和梯度爆炸问题。GRU简化了LSTM的结构，但仍然能够有效地捕捉长时间依赖关系。

GRU单元包含两个主要的门：重置门（Reset Gate）和更新门（Update Gate）。这些门通过控制信息的流动，决定应该记住什么、忘记什么和输出什么。以下是GRU的核心组件：

1. 重置门（Reset Gate）：控制前一时间步的隐藏状态对当前计算的影响。
2. 更新门（Update Gate）：控制当前隐藏状态和前一时间步隐藏状态的混合程度。

GRU的特点和优势有：

- 结构简单：GRU相比LSTM结构更简单，只有两个门，因此计算效率更高。
- 性能优越：在某些任务上，GRU的性能与LSTM相当甚至更优。
- 捕捉长时间依赖：通过门机制，GRU能够有效地捕捉序列中长时间的依赖关系。


GRU的核心计算可以表示为以下公式：

1. 更新门：
$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$

其中，$\sigma$ 是 sigmoid 激活函数，$W_z$ 是权重矩阵，$b_z$ 是偏置。

2. 重置门：
$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$

3. 候选隐藏状态：
$\tilde{h}_t = \tanh(W \cdot [r_t \cdot h_{t-1}, x_t] + b)$

4. 隐藏状态更新：
$h_t = (1 - z_t) \cdot h_{t-1} + z_t \cdot \tilde{h}_t$

```python
def gru_cell(params, h, x):
    # Extract parameters
    wxz, whz, bz = params['wxz'], params['whz'], params['bz']
    wxr, whr, br = params['wxr'], params['whr'], params['br']
    wxh, whh, bh = params['wxh'], params['whh'], params['bh']

    # 更新门
    z = sigmoid(jnp.dot(x, wxz) + jnp.dot(h, whz) + bz)

    # 重置门
    r = sigmoid(jnp.dot(x, wxr) + jnp.dot(h, whr) + br)

    # 候选隐藏状态
    h_tilde = tanh(jnp.dot(x, wxh) + jnp.dot(r * h, whh) + bh)

    # 隐藏状态更新
    h_new = (1 - z) * h + z * h_tilde

    return h_new
```

然后我们把GRU细胞串起来：

```python
def gru_forward(params, h0, xs):
    def scan_fn(h, x):
        h_new = gru_cell(params, h, x)
        return h_new, h_new

    _, hs = jax.lax.scan(scan_fn, h0, xs)
    return hs
```

这里我们又引用新工具了。jax.lax.scan 是 JAX 提供的一个高效的循环工具，用于在 JIT 编译的环境中进行循环操作。它能够在序列上高效地执行循环计算，并自动进行向量化和优化。


我们就不写训练和验证的过程了，把上面的代码串起来，这样代码看起来更简洁易于理解：

```python
import jax
import jax.numpy as jnp
from jax import random
from jax.nn import sigmoid, tanh

import jax.tools.colab_tpu
jax.tools.colab_tpu.setup_tpu()

def gru_cell(params, h, x):
    # Extract parameters
    wxz, whz, bz = params['wxz'], params['whz'], params['bz']
    wxr, whr, br = params['wxr'], params['whr'], params['br']
    wxh, whh, bh = params['wxh'], params['whh'], params['bh']

    # Update gate
    z = sigmoid(jnp.dot(x, wxz) + jnp.dot(h, whz) + bz)

    # Reset gate
    r = sigmoid(jnp.dot(x, wxr) + jnp.dot(h, whr) + br)

    # New hidden state
    h_tilde = tanh(jnp.dot(x, wxh) + jnp.dot(r * h, whh) + bh)

    # Final hidden state
    h_new = (1 - z) * h + z * h_tilde

    return h_new


def init_gru_params(input_dim, hidden_dim, key):
    k1, k2, k3, k4, k5, k6, k7 = random.split(key, 7)
    params = {
        'wxz': random.normal(k1, (input_dim, hidden_dim)),
        'whz': random.normal(k2, (hidden_dim, hidden_dim)),
        'bz': jnp.zeros(hidden_dim),
        'wxr': random.normal(k3, (input_dim, hidden_dim)),
        'whr': random.normal(k4, (hidden_dim, hidden_dim)),
        'br': jnp.zeros(hidden_dim),
        'wxh': random.normal(k5, (input_dim, hidden_dim)),
        'whh': random.normal(k6, (hidden_dim, hidden_dim)),
        'bh': jnp.zeros(hidden_dim)
    }
    return params

def gru_forward(params, h0, xs):
    def scan_fn(h, x):
        h_new = gru_cell(params, h, x)
        return h_new, h_new

    _, hs = jax.lax.scan(scan_fn, h0, xs)
    return hs

# 定义维度
input_dim = 10
hidden_dim = 20
seq_len = 5

# 初始化随机种子
key = random.PRNGKey(0)

# 初始化 GRU 参数
params = init_gru_params(input_dim, hidden_dim, key)

# 生成随机输入序列和初始隐藏状态
xs = random.normal(key, (seq_len, input_dim))
h0 = random.normal(key, (hidden_dim,))

# 计算 GRU 前向传播
hs = gru_forward(params, h0, xs)

print("Output hidden states:", hs)
```

PyTorch对于GRU同样有封装，可以通过 `torch.nn.GRU` 类创建GRU层。以下是一个简单的示例：

```python
class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):
        super(GRUModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # 定义 GRU 层
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)
        # 定义全连接层
        self.fc = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        # 初始化隐藏状态
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        
        # 前向传播 GRU
        out, _ = self.gru(x, h0)
        
        # 取最后一个时间步的输出
        out = out[:, -1, :]
        
        # 全连接层
        out = self.fc(out)
        return out
```

### 10.3 PyTorch Ignite

PyTorch Ignite是一个高级库，旨在简化和扩展PyTorch在训练和评估神经网络方面的功能。它提供了一套工具，包括引擎、事件、处理器、指标等，使得开发者能够更加灵活和透明地构建和运行他们的机器学习模型。

以下是PyTorch Ignite的一些关键特性：

- 引擎（Engines）‌：Ignite提供了一个抽象的Engine类，它可以用来定义训练和评估循环。开发者可以通过定义事件处理程序来控制这些循环的行为。
- 事件（Events）‌：Ignite定义了一系列的事件，如EPOCH_STARTED、ITERATION_STARTED等，可以在这些事件上挂载自定义的处理器（Handlers）。
- 处理器（Handlers）‌：处理器是与特定事件关联的函数，可以用来执行诸如数据加载、模型更新、日志记录等操作。
- 指标（Metrics）‌：Ignite提供了一系列内置的指标，如准确率、损失等，用于评估模型的性能。这些指标可以在线计算，无需存储整个模型的输出历史。
- 分布式支持：Ignite还支持分布式训练，可以与PyTorch的分布式包一起使用，以实现在多个GPU或多个机器上的训练。
- 灵活性：Ignite的设计允许开发者轻松地插入自己的代码，无论是添加新的事件处理器还是自定义指标，都可以无缝集成。
- 文档和社区资源：Ignite有详细的文档和教程，以及活跃的社区支持，帮助开发者快速上手和使用这个库。

我们需要通过pip安装PyTorch Ignite：

```bash
pip install pytorch-ignite
```

下面我们用PyTorch Ignite来处理MNIST手写数字识别任务：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor, Normalize, Compose
from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator
from ignite.metrics import Accuracy, Loss
from ignite.handlers import ModelCheckpoint, EarlyStopping

# 定义数据预处理
transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])

# 下载和加载数据集
train_dataset = MNIST(root='.', train=True, download=True, transform=transform)
test_dataset = MNIST(root='.', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
```

首先，代码导入了必要的库，包括PyTorch的核心模块、优化器、数据加载器，以及用于图像处理的torchvision库。此外，还导入了PyTorch Ignite的相关组件，用于创建训练和评估引擎、定义指标和处理器。

接着，定义了数据预处理流程transform，这里使用了Compose来组合多个预处理步骤。ToTensor()将图像转换为PyTorch张量，Normalize((0.1307,), (0.3081,))则对图像进行标准化处理，其中(0.1307,)和(0.3081,)分别是MNIST数据集的全局平均值和标准差。

然后，使用MNIST类从torchvision.datasets模块下载并加载MNIST数据集。指定train=True和download=True来获取训练集，并自动下载数据（如果尚未下载）。对于测试集，设置train=False。两者都应用了之前定义的transform预处理。

最后，使用DataLoader类创建训练和测试的数据加载器。这里设置了批量大小batch_size=64，并且对训练数据集启用了随机打乱shuffle=True以提高模型的泛化能力，而对测试数据集则不进行打乱shuffle=False。

这样，数据就被准备好并加载到了训练和测试加载器中，可以用于后续的模型训练和评估过程。

下面我们定义一个CNN模型：

```python
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = SimpleCNN()
```

然后，还是老规矩，定义损失函数和优化器：

```python
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

下面我们使用PyTorch Ignite来训练和评估模型：

```python
trainer = create_supervised_trainer(model, optimizer, criterion, device='cuda' if torch.cuda.is_available() else 'cpu')
evaluator = create_supervised_evaluator(model, metrics={'accuracy': Accuracy(), 'loss': Loss(criterion)}, device='cuda' if torch.cuda.is_available() else 'cpu')
```

上面代码使用了PyTorch Ignite库来创建一个训练器（trainer）和一个评估器（evaluator），这两个对象分别用于训练和评估一个神经网络模型。

首先，create_supervised_trainer函数用于创建一个训练器。它接受四个参数：

- model：要训练的神经网络模型。
- optimizer：用于模型参数优化的优化器。
- criterion：用于计算损失的损失函数。
- device：指定训练应该在哪个设备上进行，可以是CPU或GPU。这里通过torch.cuda.is_available()检查CUDA是否可用，如果可用，则使用GPU（'cuda'），否则使用CPU（'cpu'）。

训练器负责管理训练过程，包括前向传播、计算损失、反向传播和参数更新。

然后，create_supervised_evaluator函数用于创建一个评估器。它接受三个参数：

- model：要评估的神经网络模型。
- metrics：一个字典，指定要在评估过程中计算的指标。在这个例子中，计算准确率和损失。Accuracy()用于计算准确率，Loss(criterion)用于计算损失，其中criterion是之前定义的损失函数。
- device：指定评估应该在哪个设备上进行，与训练器的device参数相同。

评估器负责在验证集或测试集上评估模型的性能，计算指定的指标。

这两个对象创建后，可以使用PyTorch Ignite的事件和处理器来控制训练和评估的过程，例如在每个epoch结束时进行评估，或者在满足某些条件时保存模型的检查点。

下面，我们添加事件处理器：

```python
@trainer.on(Events.EPOCH_COMPLETED)
def log_training_results(trainer):
    evaluator.run(train_loader)
    metrics = evaluator.state.metrics
    print(f"Training Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}")

@trainer.on(Events.EPOCH_COMPLETED)
def log_validation_results(trainer):
    evaluator.run(test_loader)
    metrics = evaluator.state.metrics
    print(f"Validation Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}")

# 添加模型检查点
checkpointer = ModelCheckpoint('models', 'mnist_cnn', n_saved=2, create_dir=True, save_as_state_dict=True)
trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'model': model})

# 添加早停机制
early_stopping = EarlyStopping(patience=5, score_function=lambda engine: -engine.state.metrics['loss'], trainer=trainer)
evaluator.add_event_handler(Events.COMPLETED, early_stopping)
```

首先，定义了两个事件处理程序，它们分别在每个epoch完成时被触发。

log_training_results函数在每个epoch结束时运行，它使用训练数据加载器train_loader来评估模型，并打印出平均准确率和平均损失。这是通过调用评估器evaluator的run方法实现的，该方法返回一个包含所有指标的字典metrics。
log_validation_results函数也在每个epoch结束时运行，但它使用测试数据加载器test_loader来评估模型，并打印出验证阶段的平均准确率和平均损失。
接下来，代码添加了模型检查点。ModelCheckpoint处理器在每个epoch结束时保存模型的状态字典。这里指定了保存目录为'models'，文件名为'mnist_cnn'，最多保存2个模型副本，并确保创建目录。

最后，代码添加了早停机制。EarlyStopping处理器在验证阶段完成后检查模型的性能，如果在连续5个epoch内没有改善（即损失没有减少），则停止训练。这里的score_function是一个lambda函数，它返回负的损失值，因为早停机制是在损失不再减小时停止训练。

现在，我们可以开始训练模型：

```python
trainer.run(train_loader, max_epochs=20)
```

下面我们把完整的代码串起来：

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor, Normalize, Compose
from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator
from ignite.metrics import Accuracy, Loss
from ignite.handlers import ModelCheckpoint, EarlyStopping

# 定义数据预处理
transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])

# 下载和加载数据集
train_dataset = MNIST(root='.', train=True, download=True, transform=transform)
test_dataset = MNIST(root='.', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 7 * 7)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = SimpleCNN()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

trainer = create_supervised_trainer(model, optimizer, criterion, device='cuda' if torch.cuda.is_available() else 'cpu')
evaluator = create_supervised_evaluator(model, metrics={'accuracy': Accuracy(), 'loss': Loss(criterion)}, device='cuda' if torch.cuda.is_available() else 'cpu')

@trainer.on(Events.EPOCH_COMPLETED)
def log_training_results(trainer):
    evaluator.run(train_loader)
    metrics = evaluator.state.metrics
    print(f"Training Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}")

@trainer.on(Events.EPOCH_COMPLETED)
def log_validation_results(trainer):
    evaluator.run(test_loader)
    metrics = evaluator.state.metrics
    print(f"Validation Results - Epoch: {trainer.state.epoch}  Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}")

# 添加模型检查点
checkpointer = ModelCheckpoint(
    dirname='models', 
    filename_prefix='mnist_cnn', 
    n_saved=2, 
    create_dir=True, 
    require_empty=False  # 如果目录不为空，允许覆盖
)
trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'model': model})

# 添加早停机制
early_stopping = EarlyStopping(patience=5, score_function=lambda engine: -engine.state.metrics['loss'], trainer=trainer)
evaluator.add_event_handler(Events.COMPLETED, early_stopping)

trainer.run(train_loader, max_epochs=20)
```

运行结果如下：

```
Training Results - Epoch: 1  Avg accuracy: 0.98 Avg loss: 0.05
Validation Results - Epoch: 1  Avg accuracy: 0.98 Avg loss: 0.05
Training Results - Epoch: 2  Avg accuracy: 0.99 Avg loss: 0.03
Validation Results - Epoch: 2  Avg accuracy: 0.99 Avg loss: 0.04
Training Results - Epoch: 3  Avg accuracy: 0.99 Avg loss: 0.02
Validation Results - Epoch: 3  Avg accuracy: 0.99 Avg loss: 0.03
Training Results - Epoch: 4  Avg accuracy: 1.00 Avg loss: 0.01
Validation Results - Epoch: 4  Avg accuracy: 0.99 Avg loss: 0.03
Training Results - Epoch: 5  Avg accuracy: 1.00 Avg loss: 0.01
Validation Results - Epoch: 5  Avg accuracy: 0.99 Avg loss: 0.03
Training Results - Epoch: 6  Avg accuracy: 1.00 Avg loss: 0.01
Validation Results - Epoch: 6  Avg accuracy: 0.99 Avg loss: 0.03
Training Results - Epoch: 7  Avg accuracy: 1.00 Avg loss: 0.00
Validation Results - Epoch: 7  Avg accuracy: 0.99 Avg loss: 0.03
Training Results - Epoch: 8  Avg accuracy: 1.00 Avg loss: 0.01
Validation Results - Epoch: 8  Avg accuracy: 0.99 Avg loss: 0.04
Training Results - Epoch: 9  Avg accuracy: 1.00 Avg loss: 0.01
2024-06-22 14:18:12,841 ignite.handlers.early_stopping.EarlyStopping INFO: EarlyStopping: Stop training
Validation Results - Epoch: 9  Avg accuracy: 0.99 Avg loss: 0.04
State:
	iteration: 8442
	epoch: 9
	epoch_length: 938
	max_epochs: 20
	output: 1.5570223695249297e-05
	batch: <class 'list'>
	metrics: <class 'dict'>
	dataloader: <class 'torch.utils.data.dataloader.DataLoader'>
	seed: <class 'NoneType'>
	times: <class 'dict'>
```

### 10.4 TorchInductor

TorchInductor 是 PyTorch 的一个动态编译后端，旨在通过自动代码生成和优化来提高深度学习模型的性能。TorchInductor 可以将高层次的 PyTorch 操作转换为高效的底层代码，从而充分利用现代硬件的计算能力。

TorchInductor是默认的torch.compile后端，可以通过设置环境变量`TORCH_USE_TORCHINDUCTOR=1`来启用。

我们来看一个使用TorchInductor的例子：

```python
import torch
import torch.nn as nn
import torch.optim as optim

import torch._dynamo
torch._dynamo.config.suppress_errors = True

# 定义一个简单的模型
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.fc1 = nn.Linear(32 * 26 * 26, 128)  # 修正后的输入尺寸
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = torch.relu(x)
        x = torch.flatten(x, 1)  # 保证展平后的尺寸正确
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x

# 创建模型实例
model = SimpleModel()

# 使用 TorchInductor 编译模型
compiled_model = torch.compile(model)

# 定义损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(compiled_model.parameters(), lr=0.01, momentum=0.9)

# 创建一些伪造的输入数据
inputs = torch.randn(32, 1, 28, 28)  # batch_size=32, channels=1, height=28, width=28
targets = torch.randint(0, 10, (32,))  # batch_size=32, num_classes=10

# 训练循环
for epoch in range(10):
    optimizer.zero_grad()
    outputs = compiled_model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}")

print(compiled_model)
```

我们可以看到打印出的优化后的模型：

```
OptimizedModule(
  (_orig_mod): SimpleModel(
    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))
    (fc1): Linear(in_features=21632, out_features=128, bias=True)
    (fc2): Linear(in_features=128, out_features=10, bias=True)
  )
)
```

我们还可以打印更多的参数信息：

```python
# 查看模型参数
for name, param in compiled_model.named_parameters():
    print(f"Parameter name: {name}")
    print(f"Parameter shape: {param.shape}")
    print(f"Parameter requires_grad: {param.requires_grad}")
    print()
```

运行结果如下：

```
Parameter name: _orig_mod.conv1.weight
Parameter shape: torch.Size([32, 1, 3, 3])
Parameter requires_grad: True

Parameter name: _orig_mod.conv1.bias
Parameter shape: torch.Size([32])
Parameter requires_grad: True

Parameter name: _orig_mod.fc1.weight
Parameter shape: torch.Size([128, 21632])
Parameter requires_grad: True

Parameter name: _orig_mod.fc1.bias
Parameter shape: torch.Size([128])
Parameter requires_grad: True

Parameter name: _orig_mod.fc2.weight
Parameter shape: torch.Size([10, 128])
Parameter requires_grad: True

Parameter name: _orig_mod.fc2.bias
Parameter shape: torch.Size([10])
Parameter requires_grad: True
```

通过安装torchviz，我们可以将编译后的模型输出成图形：

```python
outputs = compiled_model(inputs)

# 生成计算图
dot = make_dot(outputs, params=dict(compiled_model.named_parameters()))

# 保存计算图为 png 文件
dot.render("compiled_model_graph", format="png")
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/compiled_model_graph.png)

## 第十一章 注意力机制与预训练模型

### 11.1 注意力机制

注意力是我们日常常用的词。人类的注意力就是从海量的信息中抓住重点。在人工神经网络中，为了模仿人类的机制，当前的词与其他每个词的关系，我们都设置一个值，这就构成了算法中的注意力机制。

有了注意力机制之后，我们通过训练，就可以让跟当前词相关的词的权重变高，而不相关的词的权重变低。

比如在机器翻译中，我们可以通过注意力机制，让源语言与目标语言中相对应的词的注意力权重变高。常见的有加法注意力和乘法注意力等实现方式。

如果我们不管与别的语句的注意力，只关注一句话或一段话中某个词与其他词之间的注意力关系，这被称为自注意力。

我们看一个例子：
![Attention](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/attention_vis.png)

通过训练，我们发现，句子中的making一词，跟后面的more和difficult之间的关系更紧密，所以它们之间的权重更高。

那么，如何去实现句子中的自注意力呢？

为了减少计算量，我们采用点积做为主要的算法，这种结构叫做缩放点积注意力模块：

![缩放点积注意力](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/scale_dot_attention.png)

如图所示，我们把一个词变换成三种东西：Q, K, V. 其中Q代表Query查询，K代表Key键，它们先进行矩阵乘，然后进行缩放，再进行Softmask，所取得的结果再与V既Value之间再进行矩阵乘。

对于矩阵乘和Softmax不了解的同学不用急，我在后面会补充基础知识，大家先理解自注意力模块的实现逻辑。

我们用PyTorch可以这样写：

```python
        # 计算注意力权重，使用缩放点积注意力
        A = torch.matmul(Q, K) / np.sqrt(self.head_dim) # (batch_size, num_heads, seq_len_q ,seq_len_k)
        A = torch.softmax(A ,dim=-1) # (batch_size,num_heads ,seq_len_q ,seq_len_k)
        # 计算注意力输出
        O = torch.matmul(A ,V) # (batch_size,num_heads ,seq_len_q ,head_dim)
```

我们再用TensorFlow.js来看看如何写：

```javascript
    // 计算注意力分数
    const scores = tf.matMul(qHeads, kHeads, false, true).div(tf.sqrt(this.depth));

    // 应用softmax函数进行归一化
    const attentionWeights = tf.softmax(scores, -1);

    // 计算加权的Value向量
    const weightedValues = tf.matMul(attentionWeights, vHeads);

```

缩放点积注意力只是一个词对另一个词的注意力，要计算每一个词，我们还得将其组合起来，变成一个更大的自注意力模块：多头自注意力模块。

![多头注意力](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/multi_head_attention.png)

所谓多头，就是有h个缩放点积注意力最终拼接在一起。

下面我们用PyTorch来实现这个多头的拼接模块：

```python
# 定义自注意力模型类
class SelfAttention(nn.Module):
    def __init__(self, input_dim, output_dim, num_heads):
        super(SelfAttention, self).__init__()
        # 参数检查
        assert output_dim % num_heads == 0, "output_dim must be divisible by num_heads"
        # 定义线性变换层
        self.W_q = nn.Linear(input_dim, output_dim)
        self.W_k = nn.Linear(input_dim, output_dim)
        self.W_v = nn.Linear(input_dim, output_dim)
        # 定义输出层
        self.W_o = nn.Linear(output_dim, output_dim)
        # 定义头数和头部维度
        self.num_heads = num_heads
        self.head_dim = output_dim // num_heads

    def forward(self, x):
        # x: (batch_size, seq_len, input_dim)
        # 计算Q,K,V
        Q = self.W_q(x) # (batch_size, seq_len, output_dim)
        K = self.W_k(x) # (batch_size, seq_len, output_dim)
        V = self.W_v(x) # (batch_size, seq_len, output_dim)
        # 将Q,K,V分割成多个头部
        Q = Q.view(Q.shape[0], Q.shape[1], self.num_heads, self.head_dim) # (batch_size, seq_len, num_heads, head_dim)
        K = K.view(K.shape[0], K.shape[1], self.num_heads, self.head_dim) # (batch_size, seq_len, num_heads, head_dim)
        V = V.view(V.shape[0], V.shape[1], self.num_heads, self.head_dim) # (batch_size, seq_len, num_heads, head_dim)
        # 调整维度顺序，便于计算注意力权重
        Q = Q.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_len, head_dim)
        K = K.permute(0, 2, 3, 1) # (batch_size, num_heads, head_dim, seq_len)
        V = V.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_len, head_dim)
        # 计算注意力权重，使用缩放点积注意力
        A = torch.matmul(Q, K) / np.sqrt(self.head_dim) # (batch_size, num_heads, seq_len_q ,seq_len_k)
        A = torch.softmax(A ,dim=-1) # (batch_size,num_heads ,seq_len_q ,seq_len_k)
        # 计算注意力输出
        O = torch.matmul(A ,V) # (batch_size,num_heads ,seq_len_q ,head_dim)
        # 调整维度顺序，便于拼接头部
        O = O.permute(0 ,2 ,1 ,3) # (batch_size ,seq_len_q ,num_heads ,head_dim)
        # 拼接头部，得到最终输出
        O = O.reshape(O.shape[0] ,O.shape[1] ,-1) # (batch_size ,seq_len_q ,output_dim)
        O = self.W_o(O) # (batch_size ,seq_len_q ,output_dim)

        return O
```

我们可以写一段测试代码来跑一下：
```python
# 测试代码
input_dim = 8
output_dim = 16
num_heads = 8
seq_len = 3
batch_size = 2

x = torch.randn(batch_size ,seq_len ,input_dim)

model = SelfAttention(input_dim ,output_dim ,num_heads)

y = model(x)

print(y.shape)
```

输出为：
```
torch.Size([2 ,3 ,16])
```

我们来看看编译后的结构图：

```python
# 使用 TorchInductor 编译模型
compiled_model = torch.compile(model)

# 查看模型结构
print(compiled_model)

# 查看模型参数
for name, param in compiled_model.named_parameters():
    print(f"Parameter name: {name}")
    print(f"Parameter shape: {param.shape}")
    print(f"Parameter requires_grad: {param.requires_grad}")
    print()

# 生成计算图
dot = make_dot(y, params=dict(compiled_model.named_parameters()))

# 保存计算图为 png 文件
dot.render("attention_graph", format="png")
```

生成的attention_graph.png如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/attention_graph.png)

我们再看看用TensorFlow.js如何实现：
```javascript
// 导入TensorFlow.js库
import * as tf from '@tensorflow/tfjs';

class SelfAttention {
  constructor(dModel, numHeads) {
    this.dModel = dModel;
    this.numHeads = numHeads;
    this.depth = dModel / numHeads;

    // 定义Q, K, V线性层的权重矩阵
    this.wq = tf.variable(tf.randomNormal([dModel, dModel]));
    this.wk = tf.variable(tf.randomNormal([dModel, dModel]));
    this.wv = tf.variable(tf.randomNormal([dModel, dModel]));

    // 定义输出线性层的权重矩阵
    this.wo = tf.variable(tf.randomNormal([dModel, dModel]));
  }

  // 自注意力计算过程
  async call(inputs) {
    const q = tf.matMul(inputs, this.wq);
    const k = tf.matMul(inputs, this.wk);
    const v = tf.matMul(inputs, this.wv);

    const batchSize = inputs.shape[0];
    const seqLen = inputs.shape[1];

    // 将Q, K, V分成多个头
    const qHeads = this.splitHeads(q, batchSize);
    const kHeads = this.splitHeads(k, batchSize);
    const vHeads = this.splitHeads(v, batchSize);

    // 计算注意力分数
    const scores = tf.matMul(qHeads, kHeads, false, true).div(tf.sqrt(this.depth));

    // 应用softmax函数进行归一化
    const attentionWeights = tf.softmax(scores, -1);

    // 计算加权的Value向量
    const weightedValues = tf.matMul(attentionWeights, vHeads);

    // 将多个头的输出重新组合
    const output = this.combineHeads(weightedValues, batchSize);

    // 应用输出线性层
    return tf.matMul(output, this.wo);
  }

  // 将张量分割为多个头
  splitHeads(tensor, batchSize) {
    return tensor.reshape([batchSize, -1, this.numHeads, this.depth]).transpose([0, 2, 1, 3]);
  }

  // 将多个头的输出重新组合
  combineHeads(tensor, batchSize) {
    return tensor.transpose([0, 2, 1, 3]).reshape([batchSize, -1, this.dModel]);
  }
}
```

来段调用让其跑起来：
```javascript
// 使用示例
(async () => {
  const dModel = 64;
  const numHeads = 8;
  const inputShape = [1, 10, dModel]; // 假设输入序列有10个词，每个词的向量维度是64

  const inputs = tf.randomNormal(inputShape);
  const selfAttention = new SelfAttention(dModel, numHeads);
  const outputs = await selfAttention.call(inputs);

  outputs.print();
})();
```

### 11.2 Transformer

有了多头注意力模块，剩下就是搭积木的工作了，我们看下面的图：

![Transformer的架构图](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/transformer.png)

input上面是编码器，target上面的是解码器。可以两个一起用，也可以只用编码器或者只用解码器。

图中有一个新增的模块，就是位置嵌入层。因为自注意力机制只跟相关性有关，而跟位置远近无关。所以我们需要一个另外的机制将位置信息编码进去。

对于偶数位，我们使用sin函数来编码位置。对于奇数位，我们采用cos函数来编码位置。
下面是用PyTorch的实现：

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)
```

我们再来看下TensorFlow.js的实现：

```javascript
class PositionalEncoding {
  constructor(dModel) {
    this.dModel = dModel;
  }

  call(x) {
    const pos = tf.range(0, x.shape[1]).reshape([-1, 1]);
    const divTerm = tf.range(0, this.dModel, 2).mul(-Math.log(10000) / this.dModel).exp();
    const pe = tf.zeros([x.shape[1], this.dModel]);

    const sinPos = tf.sin(pos.matMul(divTerm));
    const cosPos = tf.cos(pos.matMul(divTerm));
    const updatedPE = pe.bufferSync();

    for (let i = 0; i < pe.shape[1]; i += 2) {
      updatedPE.set(sinPos.dataSync()[Math.floor(i / 2)], 0, i);
      updatedPE.set(cosPos.dataSync()[Math.floor(i / 2)], 0, i + 1);
    }

    return x.add(updatedPE.toTensor());
  }
}
```

JAX的实现如下：
```python
import jax.numpy as jnp
import math

def get_positional_encoding(seq_len, d_model):
    """
    获取位置编码矩阵

    参数:
        seq_len (int): 序列长度
        d_model (int): 词嵌入维度

    返回:
        jnp.ndarray: 位置编码矩阵，形状为 (seq_len, d_model)
    """
    position = jnp.arange(seq_len)[:, jnp.newaxis]
    div_term = jnp.exp(jnp.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))

    pe = jnp.zeros((seq_len, d_model))
    pe = pe.at[:, 0::2].set(jnp.sin(position * div_term))
    pe = pe.at[:, 1::2].set(jnp.cos(position * div_term))

    return pe
```

PyTorch为我们封装好了Transformer的编码器和解码器的模块，我们构成多层编码器和解码器组成的Transformers模型，就用封装好的模块就可以了，不需要再像上面一样自己手工写了.

其中，编码器是nn.TransformerEncoder，它可以由多层nn.TransformerEncoderLayer拼装成。
同样，解码器是nn.TransformerDecoder，可以由一层或多层nn.TransformerDecoderLayer组成。

我们来看个例子：

```python
import torch
import torch.nn as nn

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward):
        super(TransformerModel, self).__init__()

        # 词嵌入层
        self.embedding = nn.Embedding(vocab_size, d_model)

        # 位置编码层
        self.pos_encoder = PositionalEncoding(d_model)

        # Transformer编码器层
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)

        # Transformer解码器层
        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)

        # 输出层
        self.linear = nn.Linear(d_model, vocab_size)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):
        # 输入嵌入
        src = self.embedding(src)
        tgt = self.embedding(tgt)

        # 位置编码
        src = self.pos_encoder(src)
        tgt = self.pos_encoder(tgt)

        # 通过Transformer编码器
        memory = self.transformer_encoder(src, src_mask)

        # 通过Transformer解码器
        output = self.transformer_decoder(tgt, memory, tgt_mask, memory_mask)

        # 输出层
        output = self.linear(output)

        return output
```

Torch.nn.Transformer默认是包含了6层编码器和6层解码器的结构，其中输入是512维，头有8个
```python
torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation=<function relu>, custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None)
```

我们再看下用TensorFlow.js来实现Transformer. 

这次用我们使用封装的tf.layers.multiHeadAttention来构建编码器：

```javascript
class EncoderLayer {
  constructor(dModel, nhead, ffDim) {
    this.multiHeadAttention = tf.layers.multiHeadAttention({headSize: nhead, numHeads: dModel / nhead, outputDim: dModel});
    this.ffn = tf.sequential();
    this.ffn.add(tf.layers.dense({units: ffDim, activation: 'relu'}));
    this.ffn.add(tf.layers.dense({units: dModel}));
    this.norm1 = tf.layers.layerNormalization();
    this.norm2 = tf.layers.layerNormalization();
  }

  call(x) {
    const attention = this.multiHeadAttention.apply([x, x, x]);
    const norm1 = this.norm1.apply(x.add(attention));
    const ffn = this.ffn.apply(norm1);
    const norm2 = this.norm2.apply(norm1.add(ffn));
    return norm2;
  }
}
```

解码器要复杂一点，因为还要有编码器的输入：

```javascript
class DecoderLayer {
  constructor(dModel, nhead, ffDim) {
    this.multiHeadAttention1 = tf.layers.multiHeadAttention({headSize: nhead, numHeads: dModel / nhead, outputDim: dModel});
    this.multiHeadAttention2 = tf.layers.multiHeadAttention({headSize: nhead, numHeads: dModel / nhead, outputDim: dModel});
    this.ffn = tf.sequential();
    this.ffn.add(tf.layers.dense({units: ffDim, activation: 'relu'}));
    this.ffn.add(tf.layers.dense({units: dModel}));
    this.norm1 = tf.layers.layerNormalization();
    this.norm2 = tf.layers.layerNormalization();
    this.norm3 = tf.layers.layerNormalization();
  }

  call(inputs) {
    const [x, encOutput] = inputs;

    const attention1 = this.multiHeadAttention1.apply([x, x, x]);
    const norm1 = this.norm1.apply(x.add(attention1));

    const attention2 = this.multiHeadAttention2.apply([norm1, encOutput, encOutput]);
    const norm2 = this.norm2.apply(norm1.add(attention2));

    const ffn = this.ffn.apply(norm2);
    const norm3 = this.norm3.apply(norm2.add(ffn));

    return norm3;
  }
}
```

最后，我们把位置嵌入、编码器、解码器都整合在一起：

```javascript
class TransformerModel {
  constructor(vocabSize, dModel, nhead, numLayers, ffDim) {
    this.embedding = tf.layers.embedding({inputDim: vocabSize, outputDim: dModel});
    this.posEncoder = new PositionalEncoding(dModel);
    this.encoderLayer = new Array(numLayers).fill(null).map(() => new EncoderLayer(dModel, nhead, ffDim));
    this.decoderLayer = new Array(numLayers).fill(null).map(() => new DecoderLayer(dModel, nhead, ffDim));
    this.finalLayer = tf.layers.dense({units: vocabSize});
  }

  async call(src, tgt) {
    const srcEmbedding = this.embedding.apply(src);
    const tgtEmbedding = this.embedding.apply(tgt);

    const srcPos = this.posEncoder.call(srcEmbedding);
    const tgtPos = this.posEncoder.call(tgtEmbedding);

    let encOutput = srcPos;
    this.encoderLayer.forEach(layer => {
      encOutput = layer.call(encOutput);
    });

    let decOutput = tgtPos;
    this.decoderLayer.forEach(layer => {
      decOutput = layer.call([decOutput, encOutput]);
    });

    const finalOutput = this.finalLayer.apply(decOutput);
    return finalOutput;
  }
}
```

### 11.3 我们的第一个语言模型

下面我们来一个真实的用Transformer来学习wiki文本，然后根据学习的语言模型让它来生成胡说八道的句子的例子。

我们使用PyTorch官方的例子，以节省往篇幅。

下载方法：
```
git clone https://github.com/pytorch/examples
```

然后进入word_language_model目录，运行下面的命令：
```
python main.py --cuda --epochs 6 --model Transformer --lr 5
```

如果你没有GPU，就把--cuda去掉。

学习的数据在word_language_model\data\wikitext-2目录下面，训练数据都是从wiki里面提取的，开始是战场女武神3游戏的词条。

训练好之后，我们就可以利用我们刚才训练的语言模型来由AI生成语句啦。

命令为：
```
python .\generate.py --cuda --outf g2.txt
```

打开g2.txt，我们就能看到我们自己的小语言模型的效果啦。
我来几句话：
```
to every year was stabilized . From ( 1964 , it had undergone a viable against criminal @-@ architectural scholar
as Main Online , which was moved into Webster were contacted ;
```

跟现在虽然经常胡说八道的gpt4相比，我们的小小语言模型还处于人话都说不明白的阶段。

虽然非常非常小，但是我们用的思想、原理和技术，跟gpt4是一样的。
我们堆很多很多的文本，训练很大参数的模型，我们也一样可以做成跟某些厂商水平差不多的大模型来。不过就算比gpt4的参数还多，文本用的还好，跟chatgpt还是比不了的。还有好多的其他知识我们需要学习的。

现在我们回来看看这个能生成基本上词还算是正确的语言模型的源码，我们先看它的位置嵌入层的：https://github.com/pytorch/examples/blob/main/word_language_model/model.py

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)
```

大家看看是不是跟我上面写的基本一样？那就对了，我就基本上照这个抄的 ：）

我们再看下它的TransformerModel的代码，我把mask部分删掉了，大家看下是不是它就只用了TransformerEncoder，解码器用的是全连接网络：

```python
class TransformerModel(nn.Module):
    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):
        super(TransformerModel, self).__init__()
        self.model_type = 'Transformer'
        self.src_mask = None
        self.pos_encoder = PositionalEncoding(ninp, dropout)
        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)
        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)
        self.encoder = nn.Embedding(ntoken, ninp)
        self.ninp = ninp
        self.decoder = nn.Linear(ninp, ntoken)

        self.init_weights()

    def init_weights(self):
        initrange = 0.1
        nn.init.uniform_(self.encoder.weight, -initrange, initrange)
        nn.init.zeros_(self.decoder.bias)
        nn.init.uniform_(self.decoder.weight, -initrange, initrange)

    def forward(self, src, has_mask=True):
        src = self.encoder(src) * math.sqrt(self.ninp)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src, self.src_mask)
        output = self.decoder(output)
        return F.log_softmax(output, dim=-1)
```

我们最后再学习下调用语言模型生成文本的代码，这部分会了我们就掌握了推理的能力：

```python
with open(args.checkpoint, 'rb') as f:
    model = torch.load(f, map_location=device)
model.eval()

corpus = data.Corpus(args.data)
ntokens = len(corpus.dictionary)

input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)

with open(args.outf, 'w') as outf:
    with torch.no_grad():  # no tracking history
        for i in range(args.words):
            output = model(input, False)
            word_weights = output[-1].squeeze().div(args.temperature).exp().cpu()
            word_idx = torch.multinomial(word_weights, 1)[0]
            word_tensor = torch.Tensor([[word_idx]]).long().to(device)
            input = torch.cat([input, word_tensor], 0)

            word = corpus.dictionary.idx2word[word_idx]

            outf.write(word + ('\n' if i % 20 == 19 else ' '))

            if i % args.log_interval == 0:
                print('| Generated {}/{} words'.format(i, args.words))
```

基本原理就是根据当前情况下的最大概率值来生成文本。

### 11.4 用Hugging Face进行预训练模型的编程

这一节我们来学习下预训练模型的封装库，Hugging Face的Transformers库的使用。Hugging Face的库非常活跃，比如支持LLaDA大规型的类，是在本文开始写作的前一天发布的。
库新到这种程度，而且相应配套的库也在不停修改中，这个时候进入这个领域一定要做好要花时间完善还不成熟的功能，尤其是花较多时间debug问题的思想准备。

另外，还是再提醒大家，大模型算法不是普通编程。模型规模和思维链仍然非常重要。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/CoT.png)

#### 11.4.1 Pipeline编程

Pipeline是transformers库中面向任务的编程方式。比如我们最常用的任务就是文本生成。

我们只需要指定"text-generation"任务，再选择一种模型，就可以了。比如下面这样，我们选择使用gpt2来进行文本生成：

```python
text_generator = pipeline("text-generation", model="gpt2")
```

我们来个完整版，除去引用包和设置一个结束符，基本上就是两句话，一句生成pipeline，一句打印结果。

```python
from transformers import pipeline

text_generator = pipeline("text-generation", model="gpt2", max_new_tokens=250)

text_generator.model.config.pad_token_id = text_generator.model.config.eos_token_id

text = text_generator("I have a dream ")[0]["generated_text"]

print(text)
```

这是其中一次我运行的结果：
```
I have a dream "

The young man's lips parted under a wave of laughter. "My dream!"

Bagel said that "My dream!"

The young man jumped back the moment he got off the train. "Good, good!"

On the other hand, the boy had gotten off. "My dream!"

There he was again in that black and white moment that his soul couldn't shake.

In this youth, the only thing that could stop him from reaching his dream was this.

"Dad, we're here now!"

Bagel didn't know how to react, at his level of maturity, he had to show up before the others to ask him something, if that wasn't his right, then his first duty had always been to save Gung-hye's life. But even so, he didn't understand why Bamboo was being so careful and so slow to respond to him. It turned out that she hadn't sent him one word to the authorities, she had simply told them not to respond.

Of course they wouldn't listen to the question, it was even worse after realizing it, Bamboo had to understand when his next
```

GPT2是openai的第二代GPT模型。我们可以看到在你个人目录下的.cache\huggingface\hub\models--gpt2目录下面，会有500多M的数据，这就是gpt2模型的大小。

如果觉得gpt2的效果不够好，我们可以换一个更大的gpt-large模型：
```python
text_generator = pipeline("text-generation", model="gpt2-large", max_new_tokens=250)

text_generator.model.config.pad_token_id = text_generator.model.config.eos_token_id

text = text_generator("I have a dream ")[0]["generated_text"]

print(text)
```

.cache\huggingface\hub\models--gpt2-large这个大小就有3G多了。

还不过瘾的话可以使用gpt2-xl，这下子模型大小就有6个G了。

如果C盘空间有限，可以通过指定TRANSFORMERS_CACHE环境变量将其指向D盘或者其它盘。

除了文本生成之外，pipeline支持很多其它的基于文本、语音、图像等任务。
虽然不推荐，不指定模型的时候，系统其实也会给我们默认配一个模型。

比如我们写一个情感分析的pipeline: 
```python
from transformers import pipeline

pipe = pipeline("text-classification")
result = pipe("这个游戏不错")
print(result)
```

系统就默认给我们找了distilbert-base-uncased-finetuned-sst-2-english模型。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/chat.png)

同样，我们也可以搞一个对话的pipeline。唯一的区别是我们需要用Conversation把输入信息包装一下，获取的结果也从Conversation对象中读取。
比如我们使用facebook的blenderbot模型：

```python
from transformers import pipeline, Conversation

pipe = pipeline('conversational', model='facebook/blenderbot-1B-distill')

conversation_1 = Conversation("What's your favorite moive?") # 创建一个对话对象
pipe([conversation_1]) # 传入一个对话对象列表，得到模型的回复
print(conversation_1.generated_responses) # 打印模型的回复
conversation_1.add_user_input("Avatar") # 添加用户的输入
pipe([conversation_1]) # 再次传入对话对象列表，得到模型的回复
print(conversation_1.generated_responses) # 打印模型的回复
```

#### 11.4.2 使用分词器和模型

除了使用pipeline之外，我们有更传统一点的用法，就是显示使用分词器和模型的方法。

语言字符串，尤其是像中文和日文这样不使用拉丁字母或者西里尔字母的语言，不方便直接被语言模型所使用，所以我们要先用分词器Tokenizer来编码字符串，推理完成后再用分词器来进行解码。
一般来说，我们不需要指定分词器的类型，通过AutoTokenizer就可以了：

```
tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

我们来个例子来看一下：

```python
import torch
from transformers import GPT2LMHeadModel, AutoTokenizer

# 加载预训练模型及对应的分词器
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

# 使用分词器将文本转换为tokens
input_tokens = tokenizer.encode("I have a dream ", return_tensors="pt")

model.config.pad_token_id = model.config.eos_token_id

# 使用模型生成文本
output = model.generate(input_tokens, max_length=250,
                        num_return_sequences=1, no_repeat_ngram_size=2)

# 将生成的tokens转换回文本
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(generated_text)
```

我们还可以更抽象一下，使用语言模型的通用抽象类AutoModelForCausalLM：

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("gpt2")

# 加载预训练模型及对应的分词器
tokenizer = AutoTokenizer.from_pretrained("gpt2", cache_dir='e:/xulun/models/')
tokenizer.pad_token_id = tokenizer.eos_token_id
model = AutoModelForCausalLM.from_pretrained("gpt2", cache_dir='e:/xulun/models/')

# 使用分词器将文本转换为tokens
input_tokens = tokenizer.encode("I have a dream ", return_tensors="pt")

# 使用模型生成文本
output = model.generate(input_tokens, max_length=250,
                        num_return_sequences=1, no_repeat_ngram_size=2)

# 将生成的tokens转换回文本
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(generated_text)
```

有了上面的抽象层，我们使用其他大模型就可以照方抓药了。
不过，LlaMA的模型目前还没有完全能支持，比如LlamaTokenizerFast还处于测试阶段。将来随着更新，我再回来更新本文吧。

```python
from transformers import LlamaTokenizerFast

tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
print(tokenizer.encode("Hello this is a test"))
```

#### 11.4.3 执行其它任务的大模型

有了上面的框架之后，我们只要知道有什么模型可以用，我们得来介绍一些预训练模型。

首先第一个肯定是我们已经多次熟悉过的GPT模型了，gpt2我们刚学习过，gpt3的API我们在第二篇中openai API部分介绍过。

第二个值得一提的是Google的T5模型。它的核心思想是基于迁移学习，能够将各种文本任务统一起来。我们可以看下表了解T5在各个子任务上取得的成果。
![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/T5.png)
另外，T5的训练已经使用了1024和TPU v3的加速器。

我们使用large的T5 1.1模型来尝试去写个摘要：

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained("google/t5-v1_1-large")
model = T5ForConditionalGeneration.from_pretrained("google/t5-v1_1-base",max_length=250)

str1 = """
Summarize:
We have explored chain-of-thought prompting as a simple and broadly applicable method for enhancing
reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense
reasoning, we find that chain-of-thought reasoning is an emergent property of model scale that allows
sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves.
Broadening the range of reasoning tasks that language models can perform will hopefully inspire
further work on language-based approaches to reasoning.
"""

input_ids = tokenizer(str1, return_tensors="pt").input_ids
outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

GPT来自openai，BERT来自Google. Facebook的团队尝试集合二者之所长，推出了BART模型。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/bart.png)

BART的预训练过程包括两个步骤：（1）使用任意的去噪函数对文本进行损坏，例如随机打乱句子顺序或用掩码符号替换文本片段；（2）学习一个模型来重建原始文本。BART使用了一个标准的基于Transformer的神经机器翻译架构，它可以看作是泛化了BERT（由于双向编码器）、GPT（由于左到右解码器）和其他更多最近的预训练方案。

下面我们来个用bart-large-cnn来写摘要的例子：

```python
from transformers import AutoTokenizer, BartForConditionalGeneration

model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")

ARTICLE_TO_SUMMARIZE = (
    """
    We have explored chain-of-thought prompting as a simple and broadly applicable method for enhancing
reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense
reasoning, we find that chain-of-thought reasoning is an emergent property of model scale that allows
sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves.
Broadening the range of reasoning tasks that language models can perform will hopefully inspire
further work on language-based approaches to reasoning.
    """
)
inputs = tokenizer([ARTICLE_TO_SUMMARIZE],
                   max_length=1024, return_tensors="pt")

# Generate Summary
summary_ids = model.generate(
    inputs["input_ids"], num_beams=2, min_length=0, max_length=100)
print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True,
      clean_up_tokenization_spaces=False)[0])
```

生成的结果如下：
```
We find that chain-of-thought reasoning is an emergent property of model scale that allows large language models to perform reasoning tasks. Broadening the range of reasoning tasks that language models can perform will hopefully inspire further work.
```

## 第十二章 深度强化学习

深度强化学习是深度学习领域的第二次革命，一举在游戏水平上大部分超越人类的水平。

下图是发表在自然杂志上的DeepMind的DQN算法在雅达利游戏上的表现，可以看到在大部分游戏上，DQN的表现已经超越了人类的水平。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/dqn_atari.png)

下面我们就学习，亲手完成这样水平的强化学习算法。

### 12.1 什么是强化学习

强化学习是机器学习的一个分支,它模仿了人类和动物的学习方式。

想象一个迷宫，你的目标是让你的角色从起点到达终点。你不知道迷宫的布局，但你可以通过尝试不同的路径来学习。每次你走一条路，你都会收到一个奖励或惩罚。如果你走对了路，你就会得到一个奖励；如果你走错了路，你就会得到一个惩罚。通过不断尝试和学习，你的角色可以找到从起点到终点的最佳路径。不断学习和适应。

想象一下你正在玩一个电子游戏，你的目标是让你的角色到达终点。你不知道如何到达那里，但你可以通过尝试不同的动作来学习。每次你采取一个动作，游戏都会给你一个奖励或惩罚。

强化学习就像这个游戏。计算机程序（称为代理）尝试不同的动作来完成任务。每次它采取一个动作，它都会收到一个奖励或惩罚。通过不断尝试和学习，代理可以找到完成任务的最佳动作序列。

强化学习涉及三个主要组件：

- 代理：尝试不同动作的计算机程序。
- 环境：代理与之交互并从中获得奖励或惩罚的世界。
- 奖励函数：定义代理采取的每个动作的奖励或惩罚。

#### 12.1.1 马尔可夫决策过程

强化学习的数学基础是马尔可夫决策过程（MDP）。MDP是一个五元组，包括：

1. 状态集（States, S)：系统可能处于的所有状态的集合。每个状态 $s \in S$ 描述了系统在某一时刻的情况。
2. 动作集（Actions, $A$ )：智能体可以在每个状态 $s$ 下执行的所有可能动作的集合。动作集可以是全局的（对所有状态相同）或局部的（依赖于状态）。
3. 状态转移概率（State Transition Probabilities, $P$）：描述在特定状态 $s$ 下执行某个动作 $a$ 后转移到下一个状态 $s'$ 的概率。记作 $P(s' | s, a)$。
4. 奖励函数（Reward Function, $R$)：描述在特定状态 $s$ 下执行动作 $a$ 所获得的即时奖励。记作 $R(s, a)$。
5. 折扣因子（Discount Factor, ($\gamma$：一个介于 0 和 1 之间的值，用于度量未来奖励的当前价值。折扣因子 $\gamma$ 越接近 1，智能体越关注长期回报；越接近 0，智能体越关注即时回报。

MDP 的目标是找到一个策略（Policy, $\pi$），该策略能够最大化累积奖励。策略 $\pi(a|s)$ 是从状态 s 选择动作 a 的概率分布。

为了评估策略的优劣，引入了价值函数：

1. 状态价值函数（State Value Function, $V^\pi(s)$)**：表示在策略 $\pi$ 下，从状态s开始的预期累积奖励。

$$
V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \bigg| s_0 = s \right]
$$

2. 状态-动作价值函数（Action Value Function, $Q^\pi(s, a)$）：表示在策略 $\pi$ 下，从状态s执行动作a后的预期累积奖励。

$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \bigg| s_0 = s, a_0 = a \right]
$$

价值函数可以通过贝尔曼方程递归地表示:

1. 状态价值函数的贝尔曼方程：
$$
V^\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) [R(s, a) + \gamma V^\pi(s')]
$$

2. 状态-动作价值函数的贝尔曼方程：
$$
Q^\pi(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) \sum_{a' \in A} \pi(a'|s') Q^\pi(s', a')
$$


#### 12.1.2 贝尔曼最优方程

在 MDP 中，策略优化的目标是找到最优策略 \(\pi^*\)，使得对于所有状态 \(s \in S\)，\(V^{\pi^*}(s)\) 最大化。

1. 最优状态价值函数：
$$
V^*(s) = \max_\pi V^\pi(s)
$$

2. 最优状态-动作价值函数：
$$
Q^*(s, a) = \max_\pi Q^\pi(s, a)
$$

求解贝尔曼最优方程的方法有两种，一种是值迭代法，另一种是策略迭代法。

值迭代是一种直接迭代更新状态价值函数 $V^*(s)$ 的方法，直到收敛到最优值。步骤如下：

1. 初始化状态价值函数 $V(s)$ 为任意值（通常初始化为零）。
2. 不断迭代更新 $V(s)$ 直到收敛：
$$
V_{k+1}(s) = \max_{a \in A} \sum_{s' \in S} P(s' | s, a) [R(s, a) + \gamma V_k(s')]
$$
3. 根据最终的状态价值函数 \(V^*(s)\) 提取出最优策略：
$$
\pi^*(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s' | s, a) [R(s, a) + \gamma V^*(s')]
$$

策略迭代是一种交替进行策略评估和策略改进的方法。

策略评估的目的是计算当前策略 $\pi_k$ 的状态价值函数 $V^{\pi_k}(s)$。这一步通过迭代求解贝尔曼期望方程来实现：

$$
V^{\pi_k}(s) = \sum_{a \in A} \pi_k(a|s) \sum_{s' \in S} P(s' | s, a) [R(s, a) + \gamma V^{\pi_k}(s')]
$$

利用迭代法，直到 $V$ 收敛。

策略改进的目的是利用当前的状态价值函数 \(V^{\pi_k}(s)\) 来生成一个新的策略 \(\pi_{k+1}\)。新的策略选择使得每个状态下的行动都能最大化未来的期望累积奖励：

$$
\pi_{k+1}(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s' | s, a) [R(s, a) + \gamma V^{\pi_k}(s')]
$$

#### 12.1.3 动态规划

动态规划（Dynamic Programming）是一种用于解决最优化问题的算法设计方法，特别适用于具有重叠子问题和最优子结构性质的问题。它通过将复杂问题分解为更小的子问题，并存储其结果以避免重复计算，从而提高效率。

动态规划的核心思想包括以下几个方面：

1. 重叠子问题：问题可以分解为相互重叠的子问题，即子问题在问题求解过程中被多次计算。
2. 最优子结构：问题的最优解包含其子问题的最优解。
3. 子问题重用：通过存储子问题的解来避免重复计算，通常使用一个表格（数组或矩阵）来存储子问题的解。

解决动态规划问题通常包含以下几个步骤：

1. 定义子问题：明确如何将原问题分解为子问题。
2. 递归关系：找出子问题之间的关系，通过递归公式描述问题的解。
3. 边界条件：确定基本的边界条件（初始条件）。
4. 计算顺序：决定计算子问题的顺序，通常是从小到大。
5. 存储和重用：使用表格（如数组）存储子问题的解，防止重复计算。

斐波那契数列是动态规划的经典例子。斐波那契数列的递推公式为：$F(n) = F(n-1) + F(n-2)$
边界条件为：$F(0) = 0, \quad F(1) = 1$

如果不使用动态规划，计算斐波那契数列的时间复杂度为 $O(2^n)$，而使用动态规划可以将时间复杂度降低到 $O(n)$。

```python
def fib_recursive(n):
    if n <= 1:
        return n
    return fib_recursive(n-1) + fib_recursive(n-2)
```

我们使用动态规划，可以将斐波那契数列的计算过程优化为：

```python
def fib_dp(n):
    if n <= 1:
        return n
    dp = [0] * (n + 1)
    dp[1] = 1
    for i in range(2, n + 1):
        dp[i] = dp[i-1] + dp[i-2]
    return dp[n]
```

#### 12.1.4 蒙特卡洛方法

蒙特卡罗方法（Monte Carlo Methods）是一类基于随机采样的算法，用于求解各种数值问题。在强化学习中，蒙特卡罗方法通过模拟多个从起始状态到终止状态的轨迹（episodes），然后利用这些轨迹的数据来估计状态值或行动值，从而求解马尔科夫决策过程（MDP）。

蒙特卡罗方法在强化学习中的主要思想是通过多次模拟从起始状态到终止状态的完整轨迹，并使用这些轨迹中的回报（returns）来估计状态值函数 $V(s)$ 或行动值函数 $Q(s, a)$。

使用蒙特卡罗方法求解强化学习问题的基本步骤为：

1. 初始化

初始化状态值函数 $V(s)$ 或行动值函数 $Q(s, a)$，以及策略 $\pi$。

2. 生成轨迹

利用当前策略 $\pi$，生成一个从起始状态到终止状态的轨迹（episode）。轨迹包括状态、动作、奖励的序列。

3. 计算回报

对于轨迹中的每个状态或状态-动作对，计算从该状态或状态-动作对开始的总回报。总回报通常通过折扣因子 \(\gamma\) 来计算：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots
$$

4. 更新估计

使用轨迹中的回报来更新状态值函数 \(V(s)\) 或行动值函数 \(Q(s, a)\)。通常使用增量更新公式：

$$
V(s) \leftarrow V(s) + \alpha (G_t - V(s))
$$

或

$$
Q(s, a) \leftarrow Q(s, a) + \alpha (G_t - Q(s, a))
$$

其中，$\alpha$是学习率。

5. 改进策略

根据更新后的值函数或行动值函数改进策略。对于策略 \(\pi\)，可以使用 \(\epsilon\)-贪婪策略改进方法，以确保策略探索。

6. 重复

重复上述步骤，直到值函数或策略收敛。

以下是使用蒙特卡罗方法求解强化学习问题的伪代码示例：

```python
import numpy as np

def monte_carlo_control(env, num_episodes, gamma=1.0, epsilon=0.1, alpha=0.1):
    Q = np.zeros((env.num_states, env.num_actions))  # 初始化Q值函数
    returns_count = np.zeros((env.num_states, env.num_actions))  # 计数器

    def generate_episode(policy):
        episode = []
        state = env.reset()
        while True:
            action = np.random.choice(env.num_actions, p=policy[state])
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            if done:
                break
            state = next_state
        return episode

    def update_policy(Q, epsilon):
        policy = np.ones((env.num_states, env.num_actions)) * epsilon / env.num_actions
        best_actions = np.argmax(Q, axis=1)
        for state, action in enumerate(best_actions):
            policy[state, action] += (1.0 - epsilon)
        return policy

    policy = np.ones((env.num_states, env.num_actions)) / env.num_actions

    for _ in range(num_episodes):
        episode = generate_episode(policy)
        G = 0
        for state, action, reward in reversed(episode):
            G = gamma * G + reward
            returns_count[state, action] += 1
            Q[state, action] += (G - Q[state, action]) / returns_count[state, action]
        policy = update_policy(Q, epsilon)

    return policy, Q
```

蒙特卡罗方法的优缺点为：

- 优点
    - 简单易实现：蒙特卡罗方法的基本原理和实现相对简单。它通过直接采样和模拟环境的交互来估计值函数，而不需要复杂的数学推导和矩阵运算。
    - 非逐步更新：蒙特卡罗方法不需要知道状态转移概率，可以直接从样本中学习。这使得它适用于模型未知的环境，即不需要事先知道环境的动态模型。
    - 适用于模型未知环境：由于蒙特卡罗方法依赖于实际的经验数据（样本轨迹），而不是依赖于环境的数学描述，因此它可以在模型未知的环境中使用。这对很多实际问题来说非常有用，因为在许多情况下，环境的精确模型是未知的或难以获取的。
    - 处理高维状态空间：蒙特卡罗方法可以处理高维状态空间，因为它的计算复杂度主要取决于采样的数量和质量，而不是状态空间的维度。
- 缺点
    - 样本效率低：蒙特卡罗方法通常需要大量样本来获得准确的估计。这是因为它依赖于多次模拟和累积经验数据来进行估计。因此，样本效率较低，特别是在状态空间和动作空间大的情况下。
    - 仅适用于终止状态存在的环境：蒙特卡罗方法要求轨迹必须有终止状态。每次更新需要完整的轨迹，因此不适用于没有自然结束点的任务（如持续的时间序列预测任务）。
    - 高方差：由于蒙特卡罗方法基于样本的估计，所得到的值函数或策略可能会有较高的方差。这意味着估计结果可能不够稳定，需要更多的样本来平滑估计值。
    - 延迟更新：蒙特卡罗方法仅在一整条轨迹结束后才进行更新，这意味着在长轨迹中，更新的反馈会有较大的延迟。这与基于时间差分（TD）的方法不同，后者可以在每一步之后立即进行更新。
    - 不适用于非马尔科夫环境：蒙特卡罗方法假设环境满足马尔科夫性质（即当前状态和动作完全决定未来的状态和回报），如果环境不满足这个假设，估计结果的准确性会受到影响。

#### 12.1.5 时序差分学习

时序差分法（Temporal Difference, 简称TD）是一种重要的强化学习方法，它结合了蒙特卡罗方法和动态规划的思想。TD方法通过在每一步更新值函数，利用当前状态和下一状态的估计值之间的差异（即时序差分误差）来逐步改进策略。

TD方法的核心在于利用 **引导回报（Bootstrapping）**，即通过当前状态的估计值和下一状态的估计值之间的差异来更新值函数。这种方法既不完全依赖最终回报（如蒙特卡罗方法），也不需要完全的环境模型（如动态规划）。

在TD方法中，时序差分误差（Temporal Difference Error, $\delta$）是更新值函数的关键。对于状态值函数 $V(s)$，时序差分误差定义为：

$\delta = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$

其中：
- $R_{t+1}$ 是从状态 $S_t$ 采取动作 $A_t$ 得到的即时奖励。
- $\gamma$ 是折扣因子。
- $V(S_{t+1})$ 是下一状态的估计值。
- $V(S_t)$ 是当前状态的估计值。

利用时序差分误差，状态值函数 $V(s)$ 的更新公式为：

$$
V(S_t) \leftarrow V(S_t) + \alpha \delta
$$

其中，$\alpha$ 是学习率。


TD方法有多种具体实现，主要包括以下几种：
- TD(0):TD(0) 是最基本的TD方法，也是单步TD方法。每一步仅使用下一个状态的估计值来更新当前状态的值函数。
- SARSA:SARSA 是一种基于策略的TD方法，估计的是当前策略下的行动值函数 \(Q(s, a)\)。其更新公式为：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
$$
- Q-Learning:Q-Learning 是一种基于策略的TD方法，其目标是找到最优策略，对应的行动值函数 \(Q^*(s, a)\)。其更新公式为：
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)]
$$
- TD(λ):TD(λ) 是一种将TD方法与蒙特卡罗方法结合的广义算法，其中 \(\lambda\) 是一个权衡参数。TD(λ) 使用资格迹（Eligibility Traces）来在不同步长上平滑更新。

以下是TD(0)的伪代码示例：

```python
import numpy as np

def td_0(env, num_episodes, alpha=0.1, gamma=1.0):
    V = np.zeros(env.num_states)  # 初始化状态值函数
    
    for _ in range(num_episodes):
        state = env.reset()
        while True:
            action = np.random.choice(env.num_actions)
            next_state, reward, done, _ = env.step(action)
            # 计算时序差分误差
            td_error = reward + gamma * V[next_state] - V[state]
            # 更新状态值函数
            V[state] += alpha * td_error
            if done:
                break
            state = next_state
    
    return V
```

时序差分法的优点和缺点为：

- 优点
    - 样本效率高：TD方法可以在每一步更新值函数，而不需要等待整个轨迹结束。这使得它在样本利用效率上优于蒙特卡罗方法，特别是在序列较长或环境较复杂的情况下。
    - 实时学习：TD方法能够在互动过程中实时更新值函数，适用于在线学习。每一步的更新使得算法可以迅速适应环境的变化。
    - 适用于非终止任务：TD方法不需要完整的轨迹，因此可以处理无限长或没有自然终点的任务。这个特性使其在处理持续任务时非常有效。
    - 无需环境模型：虽然TD方法结合了动态规划的思想，但它不需要对环境的转移概率进行建模，仅依赖于经验（采样）数据。这使得它能够在模型未知的环境中应用。
    - 平衡偏差和方差：通过引导回报（Bootstrapping），TD方法在某种程度上平衡了蒙特卡罗方法的高方差和动态规划的高偏差问题。
- 缺点
    - 偏差：由于使用引导回报，TD方法可能存在偏差。初始估计不准确时，这种偏差可能会影响学习的效果。特别是在初期，估计值的准确性可能较差。
    - 参数敏感性：TD方法的性能对学习率𝛼和折扣因子𝛾的选择高度敏感。不当的参数选择可能会导致学习过程中的振荡或收敛缓慢。
    - 稳定性和收敛性问题：在某些情况下，尤其是使用函数逼近时，TD方法可能会遇到稳定性和收敛性问题。需要使用诸如经验回放和目标网络等技术来缓解这些问题。
    - 复杂度：虽然TD方法的基本形式较为简单，但在实际应用中，为了提高性能，常常需要结合其他技术（如资格迹、深度学习等）。这种结合会增加算法的复杂度和实现难度。
    - 需要良好的探索策略：为了确保状态空间的充分探索，TD方法通常需要结合有效的探索策略（如ϵ-贪婪策略）。不良的探索策略可能会导致欠探索，从而影响学习效果。


#### 12.1.6 SARSA算法

这一节我们介绍时序差分法中的一种经典算法：SARSA算法。SARSA算法是一种基于策略的时序差分学习方法，用于估计当前策略下的行动值函数 $Q(s, a)$。SARSA算法的目标是找到最优策略，使得在每个状态下选择的动作序列能够最大化累积奖励。

SARSA算法名称中的每个字母分别对应一次更新中涉及的五个元素：当前状态 $S_t$、当前动作 $A_t$、即时奖励 $R_{t+1}$、下一状态 $S_{t+1}$ 和下一动作 $A_{t+1}$。
SARSA的核心是利用当前策略来更新行动值函数 \(Q(s, a)\)。更新公式为：

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right]
$$

其中：
- $Q(S_t, A_t)$ 是当前状态-动作对的估计值。
- $R_{t+1}$ 是从状态 $S_t$ 采取动作 $A_t$ 获得的即时奖励。
- $S_{t+1}$ 是采取动作 $A_t$ 后到达的下一状态。
- $A_{t+1}$ 是在状态 $S_{t+1}$ 下根据当前策略选择的下一动作。
- $\alpha$ 是学习率。
- $\gamma$ 是折扣因子。


SARSA算法是一种 **on-policy** 算法，这意味着它直接学习当前策略的行为。因此，选择下一动作 $A_{t+1}$ 时，通常采用 $\epsilon$-贪婪策略，以确保在学习过程中有足够的探索：

- 以概率 $\epsilon$ 随机选择动作（探索）。
- 以概率 $1 - \epsilon$ 选择当前行动值函数 $Q(s, a)$ 最大的动作（利用）。

下面是SARSA算法的伪代码示例：

```python
import numpy as np

def epsilon_greedy_policy(Q, state, epsilon, num_actions):
    if np.random.rand() < epsilon:
        return np.random.randint(num_actions)  # 探索
    else:
        return np.argmax(Q[state])  # 利用

def sarsa(env, num_episodes, alpha=0.1, gamma=1.0, epsilon=0.1):
    Q = np.zeros((env.num_states, env.num_actions))  # 初始化行动值函数

    for _ in range(num_episodes):
        state = env.reset()
        action = epsilon_greedy_policy(Q, state, epsilon, env.num_actions)

        while True:
            next_state, reward, done, _ = env.step(action)
            next_action = epsilon_greedy_policy(Q, next_state, epsilon, env.num_actions)

            # 更新行动值函数
            Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])

            if done:
                break
            
            state = next_state
            action = next_action

    return Q
```

SARSA算法的优点和缺点为：

- 优点
    - 策略一致性：SARSA直接优化当前策略，因此在策略改进过程中保持一致性，适用于需要稳定策略的应用场景。
    - 简单易实现：SARSA算法的实现较为简单，适合初学者理解和应用。
    - 适应性强：在动态和非静态环境中，SARSA能够较好地适应环境的变化。
- 缺点
    - 收敛速度慢：相比于Q-Learning，SARSA的收敛速度可能较慢，尤其是在初始阶段。
    - 依赖探索策略：SARSA的性能高度依赖于探索策略的选择，𝜖值的设置对结果有显著影响。
    - 可能陷入次优策略：如果探索不足，SARSA可能会陷入次优策略，难以找到全局最优解。

#### 12.1.7 Q-Learning算法

Q-learning是一种无模型（model-free）的强化学习算法，用于找到一个马尔可夫决策过程（MDP）的最优策略。该算法通过学习一个动作值函数 \(Q(s, a)\) 来指导智能体选择最优的动作。Q-learning是一种 off-policy 方法，这意味着它通过学习一个独立于当前策略的行为策略，从而估算最优策略的值。


Q-learning通过更新状态-动作值函数 $Q(s, a)$，不断逼近最优的状态-动作值函数 $Q^*(s, a)$，从而找到最优策略 $\pi^*$。具体来说，Q-learning通过以下更新公式来迭代地改进 $Q(s, a)$：

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \right]
$$

其中：
- $Q(S_t, A_t)$ 是当前状态-动作对的估计值。
- $R_{t+1}$ 是从状态 $S_t$ 采取动作 $A_t$ 获得的即时奖励。
- $S_{t+1}$ 是采取动作 $A_t$ 后到达的下一状态。
- $\alpha$ 是学习率，控制更新的步长。
- $\gamma$ 是折扣因子，衡量未来奖励对当前决策的影响。
- $\max_{a} Q(S_{t+1}, a)$ 是在状态 $S_{t+1}$ 下所有可能动作的最大值，代表了最优策略的估计值。

尽管Q-learning是一个 **off-policy** 算法，它通常也使用 $\epsilon$-贪婪策略进行动作选择，以保证探索和利用的平衡：

- 以概率 $\epsilon$ 随机选择动作（探索）。
- 以概率 $1 - \epsilon$ 选择当前行动值函数 $Q(s, a)$ 最大的动作（利用）。

下面是Q-learning算法的伪代码示例：

```python
import numpy as np

def epsilon_greedy_policy(Q, state, epsilon, num_actions):
    if np.random.rand() < epsilon:
        return np.random.randint(num_actions)  # 探索
    else:
        return np.argmax(Q[state])  # 利用

def q_learning(env, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):
    Q = np.zeros((env.num_states, env.num_actions))  # 初始化行动值函数

    for _ in range(num_episodes):
        state = env.reset()

        while True:
            action = epsilon_greedy_policy(Q, state, epsilon, env.num_actions)
            next_state, reward, done, _ = env.step(action)

            # 更新行动值函数
            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])

            if done:
                break
            
            state = next_state

    return Q
```

Q-learning算法的优点和缺点为：

- 优点
    - 简单易实现：Q-learning算法的实现较为简单，适合初学者理解和应用。
    - 无模型：不需要对环境的转移概率进行建模，适用于复杂和未知的环境。
    - 全局最优：在理论上，Q-learning能够在无限探索和适当学习率的条件下收敛到最优策略。
- 缺点
    - 收敛速度慢：在状态空间较大或动作空间较大的情况下，Q-learning的收敛速度可能较慢。
    - 存储需求高：需要为每个状态-动作对存储一个值，当状态空间和动作空间较大时，存储需求会显著增加。
    - 探索策略依赖：Q-learning的性能高度依赖于探索策略，ϵ值的设置对结果有显著影响。

#### 12.1.8 策略梯度法

策略梯度法（Policy Gradient Methods）是强化学习中的一种方法，通过直接优化策略的参数来最大化累计奖励。与基于值函数的方法（如Q-learning和SARSA）不同，策略梯度法不显式地估计状态值函数或动作值函数，而是直接优化策略。

策略梯度法的核心思想是通过参数化策略函数 $\pi_\theta(a|s)$ 来表示智能体在状态 $s$ 下选择动作 $a$ 的概率。这里，$\theta$ 是策略的参数。目标是找到最优的策略参数 $\theta$，使得智能体在环境中累积的期望奖励最大化。

策略梯度法的目标是最大化以下目标函数：

$J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t R_t \right]$

其中：
- $\pi_\theta$ 是参数化策略。
- $R_t$ 是在时间步 $t$ 获得的奖励。
- $\gamma$ 是折扣因子。


策略梯度定理为我们提供了目标函数 \(J(\theta)\) 的梯度表达式，从而可以使用梯度上升法来优化策略参数。策略梯度定理表明：

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s, a) \right]
$$

其中：
- $\nabla_\theta \log \pi_\theta(a|s)$ 是策略的梯度。
- $Q^{\pi_\theta}(s, a)$ 是在状态 $s$ 下采取动作 $a$ 的动作值函数。

REINFORCE算法是最基本的策略梯度算法，其核心步骤如下：

1. 采样轨迹：从策略  $\pi_\theta$ 中采样轨迹 $(s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T)$。
2. 计算梯度：计算每个时间步的梯度 $\nabla_\theta \log \pi_\theta(a_t|s_t)$。
3. 更新参数：使用梯度上升法更新策略参数 $\theta$。

伪代码如下：

```python
import numpy as np

def reinforce(env, num_episodes, alpha=0.01, gamma=0.99):
    # 初始化策略参数
    theta = np.random.rand(env.num_features, env.num_actions)
    
    def policy(state):
        # 计算动作概率
        logits = np.dot(state, theta)
        return np.exp(logits) / np.sum(np.exp(logits))
    
    for _ in range(num_episodes):
        state = env.reset()
        trajectory = []
        
        # 生成轨迹
        while True:
            probs = policy(state)
            action = np.random.choice(len(probs), p=probs)
            next_state, reward, done, _ = env.step(action)
            trajectory.append((state, action, reward))
            if done:
                break
            state = next_state
        
        # 计算每个时间步的回报
        returns = []
        G = 0
        for _, _, reward in reversed(trajectory):
            G = reward + gamma * G
            returns.insert(0, G)
        
        # 更新策略参数
        for (state, action, _), G in zip(trajectory, returns):
            probs = policy(state)
            gradient = -probs
            gradient[action] += 1
            theta += alpha * G * gradient.reshape(-1, 1) * state.reshape(1, -1)
    
    return theta
```

策略梯度法的优点和缺点为：

- 优点
    - 处理连续动作空间：策略梯度法能够自然地处理连续的动作空间，而基于值函数的方法在处理连续动作空间时较为困难。
    - 策略直接优化：通过直接优化策略，策略梯度法能够在策略空间中进行更精细的搜索。
    - 稳定性：策略梯度法在一些情况下比基于值函数的方法更稳定，因为它不会依赖于值函数的估计。
    - 适应性强：策略梯度方法可以处理非确定性策略，这在一些需要探索和利用平衡的复杂任务中非常有用。
- 缺点
    - 高方差：策略梯度估计通常具有较高的方差，这会导致训练过程中的不稳定性和收敛速度慢。
    - 收敛速度慢：由于高方差的影响，策略梯度方法的收敛速度通常较慢，尤其是在高维的状态和动作空间中。
    - 依赖于探索策略：策略梯度方法的性能高度依赖于策略的探索性，如果策略过于保守或过于激进，都可能影响训练效果。
    - 计算成本高：由于需要对策略参数进行更新，策略梯度方法的计算成本较高，特别是在大规模问题中。
    - 局部最优：策略梯度法优化容易陷入局部最优，特别是在复杂的策略空间中。

#### 12.1.9 Actor-Critic方法

Actor-Critic方法是一种用于强化学习的算法，它结合了策略优化（Policy Optimization）和价值估计（Value Estimation）的优点。该方法同时使用两个模型：Actor（行为者）和Critic（评论者），分别负责策略的更新和价值的评估。

- Actor（行为者）
    - Actor负责选择动作并根据策略梯度更新策略参数。
    - Actor的目标是最大化预期累积奖励。

- Critic（评论者）：
    - Critic评估当前策略的好坏，计算状态值函数（Value Function）或优势函数（Advantage Function）。
    - Critic的目标是提供更准确的价值估计，指导Actor的策略更新。

Actor-Critic算法步骤为

1. 策略表示：
    - 策略 $\pi_{\theta}(a|s)$ 表示在状态 $s$ 下选择动作 $a$ 的概率，由参数 $\theta$ 控制。

2. 状态值函数：
    - 价值函数 $V(s)$ 表示在状态 $s$ 下的预期累积奖励。

3. 优势函数：
    - 优势函数 $A(s, a)$ 衡量特定动作 $a$ 相对于状态 $s$ 的平均水平的好坏。
    - 通常，优势函数可以表示为 $A(s, a) = Q(s, a) - V(s)$。

4. 策略更新：
    - 使用策略梯度法更新Actor的参数：

$$
\theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(a|s) A(s, a)
$$

其中，$\alpha$ 是学习率。

5. 价值更新：
    - 使用TD误差（Temporal Difference Error）更新Critic的参数：

$$
\delta = r + \gamma V(s') - V(s)
$$

其中，$r$ 是即时奖励，$\gamma$ 是折扣因子，$s'$ 是下一状态。

6. 同步更新：
    - 在每个时间步，Actor和Critic交替更新，Actor根据Critic的反馈调整策略，Critic根据Actor的策略调整价值估计。

算法流程

1. 初始化Actor的策略参数 \(\theta\) 和Critic的价值函数参数 $\phi$。
2. 在每个时间步：
    1. 在状态 $s_t$ 下，Actor根据策略 $\pi_{\theta}$ 选择动作 $a_t$。
    2. 执行动作 $a_t$，获得即时奖励 \(r_t\) 和下一状态 $s_{t+1}$。
    3. Critic计算TD误差 $\delta$：

$$
\delta_t = r_t + \gamma V_{\phi}(s_{t+1}) - V_{\phi}(s_t)
$$

    4. 更新Critic的参数 $\phi$：

$$
\phi \leftarrow \phi + \beta \delta_t \nabla_{\phi} V_{\phi}(s_t)
$$

    5. 计算优势函数 $A(s_t, a_t) = \delta_t$。
    6. 更新Actor的参数 $\theta$：

$$
\theta \leftarrow \theta + \alpha \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \delta_t
$$

3. 重复上述过程，直到策略收敛或达到预定的训练步数。

Actor-Critic方法通过将策略梯度和价值估计相结合，显著提高了强化学习算法的效率和稳定性。

### 12.2 深度强化学习

#### 12.2.1 DQN算法

深度Q网络（Deep Q-Network, DQN）结合了Q学习和深度神经网络的优点。DQN算法特别适用于处理高维状态空间，比如游戏中的图像数据。以下是DQN算法的核心概念和步骤：

前面我们介绍过，Q学习是一种无模型的强化学习算法，旨在找到一个策略，使得在特定状态下采取特定动作的期望回报最大化。Q学习使用一个Q表（Q-table）来存储状态-动作对的价值（Q值），但是对于高维状态空间，Q表变得不可行。
于是，DQN使用一个深度神经网络来近似Q值函数，解决了Q表在高维状态空间中的扩展问题。神经网络的输入是状态，输出是每个可能动作的Q值。

算法步骤

1. 经验回放（Experience Replay）：
    - 代理（Agent）在环境中与环境交互，并将每次交互（状态、动作、奖励、下一个状态）存储在一个回放缓冲区（Replay Buffer）中。
    - 从回放缓冲区中随机抽取小批量的经验进行训练，打破了数据的时间相关性，提高了训练的稳定性。

2. 目标网络（Target Network）：
    - DQN使用两个网络：一个是在线网络（Online Network），另一个是目标网络（Target Network）。
    - 在线网络用于生成动作和更新参数；目标网络用于计算目标Q值。
    - 目标网络的参数定期从在线网络复制，以保持训练的稳定性。

3. 损失函数：
    - 使用均方误差（Mean Squared Error, MSE）来衡量预测的Q值和目标Q值之间的差异。
    - 目标Q值通过贝尔曼方程计算：

$$
y = r + \gamma \max_{a'} Q'(s', a')
$$

其中，$r$ 是奖励，$\gamma$ 是折扣因子，$Q'$ 是目标网络的Q值，$s'$ 和 $a'$ 分别是下一个状态和动作。

4. 参数更新：使用梯度下降法来最小化损失函数，更新在线网络的参数。

算法流程

- 初始化在线网络和目标网络的参数。
- 初始化回放缓冲区。
- 在每个时间步：
    - 根据当前状态，使用在线网络选择动作（通常用 $\epsilon$-贪婪策略）。
    - 执行动作，观测奖励和下一个状态。
    - 将经验（状态、动作、奖励、下一个状态）存储到回放缓冲区。
    - 从回放缓冲区中随机抽取小批量经验进行训练。
    - 计算目标Q值并更新在线网络参数。
    - 定期更新目标网络的参数。

DQN算法在处理复杂环境（如Atari游戏）方面取得了显著成功，是深度强化学习领域的重要里程碑。

我们下面来用PyTorch实现一个DQN网络。

首先我们定义一个Q网络：

```python
# 定义 Q 网络
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, seed):
        super(QNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)
```

这个网络的作用是为给定的状态估计每个动作的Q值，即在遵循当前策略的情况下，采取每个动作所能获得的预期回报。在强化学习中，这样的网络可以帮助智能体学习在给定状态下选择最佳动作。

下面我们实现回放缓冲区，解释我直接写在代码里了：

```python
class ReplayBuffer:
    def __init__(self, action_size, buffer_size, batch_size, seed):
        self.action_size = action_size
        self.memory = deque(maxlen=buffer_size)
        self.batch_size = batch_size
        self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])
        self.seed = random.seed(seed)
    
    # 用于向回放缓冲区添加新的经验
    def add(self, state, action, reward, next_state, done):
        e = self.experience(state, action, reward, next_state, done)
        self.memory.append(e)
    
    # 用于从回放缓冲区中随机采样一批经验
    def sample(self):
        experiences = random.sample(self.memory, k=self.batch_size)
        
        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()
        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()
        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()
        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()
        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()
        
        return (states, actions, rewards, next_states, dones)
    
    # 返回回放缓冲区中存储的经验数量
    def __len__(self):
        return len(self.memory)
```

经验回放缓冲区的主要目的是打破经验之间的相关性，并更有效地利用过去的经验来训练智能体。通过随机采样，可以减少训练过程中的方差，从而提高学习的稳定性。

最后我们实现DQN算法：

```python
def dqn(n_episodes=1000, max_t=300, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, tau=1e-3):
    scores = []
    scores_window = deque(maxlen=100)
    epsilon = epsilon_start
    
    for i_episode in range(1, n_episodes+1):
        state = env.reset()
        score = 0
        
        for t in range(max_t):
            action = select_action(state, epsilon)
            next_state, reward, done, _ = env.step(action)
            replay_buffer.add(state, action, reward, next_state, done)
            
            state = next_state
            score += reward
            
            if len(replay_buffer) > batch_size:
                experiences = replay_buffer.sample()
                learn(experiences, gamma, tau)
            
            if done:
                break
        
        scores_window.append(score)
        scores.append(score)
        epsilon = max(epsilon_end, epsilon_decay * epsilon)
        
        print(f'\rEpisode {i_episode}\tAverage Score: {np.mean(scores_window):.2f}', end="")
        if i_episode % 100 == 0:
            print(f'\rEpisode {i_episode}\tAverage Score: {np.mean(scores_window):.2f}')
        if np.mean(scores_window) >= 195.0:
            print(f'\nEnvironment solved in {i_episode-100} episodes!\tAverage Score: {np.mean(scores_window):.2f}')
            torch.save(qnetwork_local.state_dict(), 'checkpoint.pth')
            break

    return scores
```

我们还需要写一个函数，用于结合探索（exploration）和利用（exploitation）的策略，通过一个参数epsilon来平衡两者。

```python
def select_action(state, epsilon):
    state = torch.from_numpy(state).float().unsqueeze(0)
    qnetwork_local.eval()
    with torch.no_grad():
        action_values = qnetwork_local(state)
    qnetwork_local.train()

    if random.random() > epsilon:
        return np.argmax(action_values.cpu().data.numpy())
    else:
        return random.choice(np.arange(action_size))
```

下面我们来看下如何实现学习过程：

```python
def learn(experiences, gamma, tau):
    states, actions, rewards, next_states, dones = experiences
    
    # 计算下一个状态的Q值，并取最大值作为下一个状态的最佳Q值。detach()用于防止梯度传播到目标网络。
    q_targets_next = qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)
    # 计算目标Q值，它是即时奖励加上折扣后的最佳未来奖励。
    q_targets = rewards + (gamma * q_targets_next * (1 - dones))
    
    # 计算当前策略下的预期Q值，即对于给定的状态和动作，网络预测的Q值。
    q_expected = qnetwork_local(states).gather(1, actions)
    
    # 计算预期Q值和目标Q值之间的均方误差损失
    loss = nn.MSELoss()(q_expected, q_targets)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    soft_update(qnetwork_local, qnetwork_target, tau)

# 这个函数用于平滑地更新目标网络的参数，以防止训练过程中的不稳定
def soft_update(local_model, target_model, tau):
    # 对于目标网络和本地网络的每一对参数，使用tau比例更新目标网络的参数，保持1-tau比例的原参数。
    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
        target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data
```

下面我们看下完整的代码：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import namedtuple, deque

# 定义 Q 网络
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, seed):
        super(QNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# 创建环境
env = gym.make('CartPole-v1')

# 设置参数
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
seed = 0

# 初始化 Q 网络
qnetwork_local = QNetwork(state_size, action_size, seed)
qnetwork_target = QNetwork(state_size, action_size, seed)
optimizer = optim.Adam(qnetwork_local.parameters(), lr=0.001)

# 设置随机种子
np.random.seed(seed)
env.seed(seed)
torch.manual_seed(seed)

class ReplayBuffer:
    def __init__(self, action_size, buffer_size, batch_size, seed):
        self.action_size = action_size
        self.memory = deque(maxlen=buffer_size)
        self.batch_size = batch_size
        self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])
        self.seed = random.seed(seed)
    
    def add(self, state, action, reward, next_state, done):
        e = self.experience(state, action, reward, next_state, done)
        self.memory.append(e)
    
    def sample(self):
        experiences = random.sample(self.memory, k=self.batch_size)
        
        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()
        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()
        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()
        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()
        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()
        
        return (states, actions, rewards, next_states, dones)
    
    def __len__(self):
        return len(self.memory)

# 初始化经验回放缓冲区
buffer_size = int(1e5)
batch_size = 64
replay_buffer = ReplayBuffer(action_size, buffer_size, batch_size, seed)

def dqn(n_episodes=1000, max_t=300, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, tau=1e-3):
    scores = []
    scores_window = deque(maxlen=100)
    epsilon = epsilon_start
    
    for i_episode in range(1, n_episodes+1):
        state = env.reset()
        score = 0
        
        for t in range(max_t):
            action = select_action(state, epsilon)
            next_state, reward, done, _ = env.step(action)
            replay_buffer.add(state, action, reward, next_state, done)
            
            state = next_state
            score += reward
            
            if len(replay_buffer) > batch_size:
                experiences = replay_buffer.sample()
                learn(experiences, gamma, tau)
            
            if done:
                break
        
        scores_window.append(score)
        scores.append(score)
        epsilon = max(epsilon_end, epsilon_decay * epsilon)
        
        print(f'\rEpisode {i_episode}\tAverage Score: {np.mean(scores_window):.2f}', end="")
        if i_episode % 100 == 0:
            print(f'\rEpisode {i_episode}\tAverage Score: {np.mean(scores_window):.2f}')
        if np.mean(scores_window) >= 195.0:
            print(f'\nEnvironment solved in {i_episode-100} episodes!\tAverage Score: {np.mean(scores_window):.2f}')
            torch.save(qnetwork_local.state_dict(), 'checkpoint.pth')
            break

    return scores

def select_action(state, epsilon):
    state = torch.from_numpy(state).float().unsqueeze(0)
    qnetwork_local.eval()
    with torch.no_grad():
        action_values = qnetwork_local(state)
    qnetwork_local.train()

    if random.random() > epsilon:
        return np.argmax(action_values.cpu().data.numpy())
    else:
        return random.choice(np.arange(action_size))

def learn(experiences, gamma, tau):
    states, actions, rewards, next_states, dones = experiences
    
    q_targets_next = qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)
    q_targets = rewards + (gamma * q_targets_next * (1 - dones))
    
    q_expected = qnetwork_local(states).gather(1, actions)
    
    loss = nn.MSELoss()(q_expected, q_targets)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    soft_update(qnetwork_local, qnetwork_target, tau)

def soft_update(local_model, target_model, tau):
    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
        target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)

# 训练 DQN 代理
scores = dqn()
```

运行结果如下：

```
Episode 100	Average Score: 18.26
Episode 200	Average Score: 34.83
Episode 300	Average Score: 128.78
Episode 400	Average Score: 148.33
Episode 486	Average Score: 195.24
Environment solved in 386 episodes!	Average Score: 195.24
```

针对图像的游戏环境，我们可以使用卷积神经网络（CNN）来提取图像特征，然后连接到全连接层来估计Q值。

```python

# 定义CNN模型
class DQN(nn.Module):
    def __init__(self, input_shape, num_actions):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(self.feature_size(input_shape), 512)
        self.fc2 = nn.Linear(512, num_actions)

    def feature_size(self, input_shape):
        with torch.no_grad():
            return self.conv3(self.conv2(self.conv1(torch.zeros(1, *input_shape)))).view(1, -1).size(1)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)
```

除了CNN之前，我们也可以对图像通过CV预处理，比如裁剪、缩放、灰度化等，以提高模型的性能。

```python
# 图像预处理函数
def preprocess_observation(obs):
    if not isinstance(obs, np.ndarray):
        obs = np.array(obs)
    if obs.shape[-1] == 3:  # 确保输入是RGB图像
        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
    obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)
    obs = np.expand_dims(obs, axis=0)  # 增加一个维度以匹配卷积神经网络的输入
    return obs / 255.0  # 归一化图像数据
```

我们来看一下CNN策略的完整的代码：

```python
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import namedtuple, deque
import random
import cv2

# 定义设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 定义CNN模型
class DQN(nn.Module):
    def __init__(self, input_shape, num_actions):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(self.feature_size(input_shape), 512)
        self.fc2 = nn.Linear(512, num_actions)

    def feature_size(self, input_shape):
        with torch.no_grad():
            return self.conv3(self.conv2(self.conv1(torch.zeros(1, *input_shape)))).view(1, -1).size(1)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# 定义经验回放缓冲区
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, *args):
        self.buffer.append(Transition(*args))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)

# 定义DQN智能体
class DQNAgent:
    def __init__(self, input_shape, num_actions, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=500):
        self.gamma = gamma
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.epsilon = epsilon_start
        self.num_actions = num_actions

        self.policy_net = DQN(input_shape, num_actions).to(device)
        self.target_net = DQN(input_shape, num_actions).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters())
        self.memory = ReplayBuffer(10000)
        self.steps_done = 0

    def select_action(self, state):
        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1. * self.steps_done / self.epsilon_decay)
        self.steps_done += 1
        if random.random() > self.epsilon:
            with torch.no_grad():
                return self.policy_net(state).max(1)[1].view(1, 1)
        else:
            return torch.tensor([[random.randrange(self.num_actions)]], device=device, dtype=torch.long)

    def optimize_model(self, batch_size):
        if len(self.memory) < batch_size:
            return
        transitions = self.memory.sample(batch_size)
        batch = Transition(*zip(*transitions))

        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)
        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])
        state_batch = torch.cat(batch.state)
        action_batch = torch.cat(batch.action)
        reward_batch = torch.cat(batch.reward)

        state_action_values = self.policy_net(state_batch).gather(1, action_batch)

        next_state_values = torch.zeros(batch_size, device=device)
        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()

        expected_state_action_values = (next_state_values * self.gamma) + reward_batch

        loss = nn.functional.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))

        self.optimizer.zero_grad()
        loss.backward()
        for param in self.policy_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()

# 图像预处理函数
def preprocess_observation(obs):
    if not isinstance(obs, np.ndarray):
        obs = np.array(obs)
    if obs.shape[-1] == 3:  # 确保输入是RGB图像
        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
    obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)
    obs = np.expand_dims(obs, axis=0)  # 增加一个维度以匹配卷积神经网络的输入
    return obs / 255.0  # 归一化图像数据

# 主训练循环
def train_dqn(env_name, num_episodes, batch_size):
    env = gym.make(env_name)
    input_shape = (1, 84, 84)
    num_actions = env.action_space.n
    agent = DQNAgent(input_shape, num_actions)

    for episode in range(num_episodes):
        obs = env.reset()
        obs = preprocess_observation(obs[0])
        state = torch.tensor([obs], device=device, dtype=torch.float32)
        sum = 0

        for t in range(10000):
            action = agent.select_action(state)
            next_obs, reward, done, _, _ = env.step(action.item())
            sum = sum + reward
            reward = torch.tensor([reward], device=device)

            if not done:
                next_obs = preprocess_observation(next_obs)
                next_state = torch.tensor([next_obs], device=device, dtype=torch.float32)
            else:
                next_state = None

            agent.memory.push(state, action, next_state, reward)

            state = next_state

            agent.optimize_model(batch_size)

            if done:
                print(f"Episode {episode} finished after {t+1} timesteps")
                print(sum)
                break

        if episode % 10 == 0:
            agent.target_net.load_state_dict(agent.policy_net.state_dict())

    env.close()

# 训练DQN
if __name__ == "__main__":
    train_dqn("PongNoFrameskip-v4", num_episodes=500, batch_size=32)
```

运行结果如下：
```
Episode 0 finished after 3243 timesteps
-21.0
Episode 1 finished after 3879 timesteps
-21.0
Episode 2 finished after 3408 timesteps
-21.0
Episode 3 finished after 3056 timesteps
-21.0
Episode 4 finished after 3775 timesteps
-19.0
Episode 5 finished after 3748 timesteps
-19.0
Episode 6 finished after 3304 timesteps
-21.0
Episode 7 finished after 3056 timesteps
-21.0
Episode 8 finished after 3607 timesteps
-20.0
Episode 9 finished after 3131 timesteps
-21.0
Episode 10 finished after 3853 timesteps
-20.0
...
```

flax是一个基于JAX的深度学习库，它提供了高级的神经网络模型和训练API，可以帮助我们更方便地实现深度学习模型。
flax.linen是flax的一个子模块，提供了一些预定义的神经网络层和模型，可以帮助我们更快地搭建神经网络。

我们以CartPole游戏为例，看看如何用JAX和Flax实现DQN算法：

```python
import gym
import numpy as np
import jax
import jax.numpy as jnp
import flax.linen as nn
from flax.training import train_state
import optax
from collections import namedtuple, deque
import random

# 创建环境
env = gym.make('CartPole-v1')

# 设置随机种子
key = jax.random.PRNGKey(0)
np.random.seed(0)
env.seed(0)

# 定义 Q 网络
class QNetwork(nn.Module):
    action_size: int

    @nn.compact
    def __call__(self, x):
        x = nn.relu(nn.Dense(128)(x))
        x = nn.relu(nn.Dense(128)(x))
        x = nn.Dense(self.action_size)(x)
        return x

def create_train_state(rng, learning_rate, action_size):
    q_net = QNetwork(action_size=action_size)
    params = q_net.init(rng, jnp.ones([1, env.observation_space.shape[0]]))['params']
    tx = optax.adam(learning_rate)
    return train_state.TrainState.create(apply_fn=q_net.apply, params=params, tx=tx)

# 初始化 Q 网络
rng, init_rng = jax.random.split(key)
learning_rate = 1e-3
state = create_train_state(init_rng, learning_rate, env.action_space.n)
target_state = state

# 经验回放缓冲区
class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def add(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*batch)
        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)

    def __len__(self):
        return len(self.buffer)

# 设定折扣因子
gamma = 0.99

def q_loss(params, target_params, states, actions, rewards, next_states, dones):
    q_values = state.apply_fn({'params': params}, states)
    next_q_values = target_state.apply_fn({'params': target_params}, next_states)
    next_q_values = jnp.max(next_q_values, axis=1)
    target_q_values = rewards + gamma * next_q_values * (1 - dones)
    
    action_q_values = q_values[jnp.arange(q_values.shape[0]), actions]
    
    loss = jnp.mean((action_q_values - target_q_values) ** 2)
    return loss


# 更新函数
@jax.jit
def update(state, target_state, states, actions, rewards, next_states, dones):
    loss, grads = jax.value_and_grad(q_loss)(state.params, target_state.params, states, actions, rewards, next_states, dones)
    state = state.apply_gradients(grads=grads)
    return state, loss

def select_action(obs, params, epsilon):
    if np.random.rand() < epsilon:
        return env.action_space.sample()
    else:
        q_values = state.apply_fn({'params': params}, obs[None, :])
        return int(jnp.argmax(q_values))

num_episodes = 500
batch_size = 32
buffer_capacity = 10000
epsilon_start = 1.0
epsilon_end = 0.1
epsilon_decay = 0.995

replay_buffer = ReplayBuffer(buffer_capacity)

for episode in range(num_episodes):
    obs = env.reset()
    total_reward = 0
    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))
    
    while True:
        action = select_action(obs, state.params, epsilon)
        next_obs, reward, done, _ = env.step(action)
        total_reward += reward
        
        replay_buffer.add(obs, action, reward, next_obs, done)
        obs = next_obs
        
        if len(replay_buffer) >= batch_size:
            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
            states = jnp.array(states)
            actions = jnp.array(actions)
            rewards = jnp.array(rewards)
            next_states = jnp.array(next_states)
            dones = jnp.array(dones)
            
            # 更新 Q 网络
            state, loss = update(state, target_state, states, actions, rewards, next_states, dones)
        
        if done:
            print(f"Episode {episode}: Total Reward = {total_reward}")
            break

    # 定期更新目标网络
    if episode % 10 == 0:
        target_state = state

print("训练完成")
```

#### 12.2.2 TRPO算法

TRPO（Trust Region Policy Optimization，信任域策略优化）是基于策略梯度法的算法，旨在确保每次策略更新时不会对策略造成过大的变化，从而提高训练的稳定性和效率。TRPO通过引入信任域约束，避免了策略更新过程中过大的波动。

介绍算法之前，我们先了解一下两个关键概念：
- 信任域（Trust Region）：
    - 信任域的概念来源于优化理论，即在每次更新策略时，只允许策略在一个“信任”的范围内变化，以避免过大的更新导致训练不稳定。
    - TRPO通过引入信任域约束，限制每次策略更新的幅度。
- KL散度（Kullback-Leibler Divergence）：
    - 用于衡量新旧策略之间的差异。TRPO通过限制新旧策略之间的平均KL散度，使策略更新在信任域内。
    - KL散度约束确保新策略不会偏离旧策略太远，从而提高训练的稳定性。

下面我们看下TRPO算法的步骤：

1. 目标函数：TRPO的目标是最大化策略的期望回报，同时约束新旧策略之间的平均KL散度：

$$
\max_{\theta} \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}, a \sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} \hat{A}(s, a) \right]
$$

其中：
- $\pi_{\theta}(a|s)$ 是新策略在状态 $s$ 下选择动作 $a$ 的概率。
- $\pi_{\theta_{\text{old}}}(a|s)$ 是旧策略在状态 $s$ 下选择动作 $a$ 的概率。
- $\hat{A}(s, a)$ 是优势函数（Advantage Function）的估计。

2. KL散度约束：
    - TRPO引入一个约束，确保新旧策略之间的平均KL散度不超过一个预设的阈值 \(\delta\)：

$$
    \mathbb{E}_{s \sim \rho_{\theta_{\text{old}}}} \left[ D_{KL}(\pi_{\theta_{\text{old}}}(\cdot|s) || \pi_{\theta}(\cdot|s)) \right] \leq \delta
$$

其中：
- $D_{KL}(\cdot || \cdot)$ 表示KL散度。

3. 优化过程：

- 通过拉格朗日乘子法将约束优化问题转化为无约束优化问题，进而使用共轭梯度法（Conjugate Gradient Method）进行求解。
- 在每次更新中，首先计算策略梯度，然后找到符合KL散度约束的更新方向和步长。

算法流程

- 初始化策略参数 $\theta$。
- 在每个训练周期：
    - 与环境交互，收集一批数据（状态、动作、奖励、下一状态）。
    - 计算优势函数 $\hat{A}(s, a)$。
    - 计算策略梯度。
    - 使用共轭梯度法找到满足KL散度约束的更新方向和步长。
    - 更新策略参数 $\theta$。
- 重复上述过程，直到策略收敛或达到预定的训练步数。

TRPO通过引入信任域约束，显著提高了策略优化的稳定性和效率。尽管计算复杂度较高，但其在处理高维连续动作空间问题时表现尤为出色。

#### 12.2.3 PPO算法

PPO（Proximal Policy Optimization，近端策略优化）由OpenAI提出，它通过引入新的目标函数和约束，稳定了策略优化过程，提高了样本效率和训练稳定性。PPO算法是一种基于策略梯度的算法，是在TRPO算法的基础上进行改进的。PPO通过引入剪切（Clipping）机制，简化了TRPO的复杂性，避免了计算二阶导数。PPO优化目标函数时，限制策略变化的幅度，以防止策略更新过大。

算法步骤

1. 策略更新目标函数：PPO的目标函数如下：

$$
    L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t) \right]
$$

其中：
- $ r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} $
- $\hat{A}_t$ 是优势函数（Advantage Function）的估计
- $\epsilon$ 是一个超参数，用于限制策略变化的幅度

2. 剪切机制（Clipping）：

- 剪切机制通过限制策略更新的比例，确保策略不会偏离旧策略太远。
- 具体实现是限制 $r_t(\theta)$ 的值在 $[1 - \epsilon, 1 + \epsilon]$ 范围内。

3. 优势函数估计：

- 优势函数 $\hat{A}_t$ 用于衡量当前动作相对于平均水平的好坏。
- 可以使用广义优势估计（Generalized Advantage Estimation, GAE）来计算。

4. 训练过程：

- 在每个训练周期，收集一批数据（状态、动作、奖励等），并计算优势函数。
- 使用剪切目标函数和梯度下降法更新策略参数。
- 更新值函数（Value Function）的参数，以更好地估计状态价值。

PPO算法流程如下：

1. 初始化策略参数和价值函数参数。
2. 在每个训练周期：
    1. 与环境交互，收集一批数据（状态、动作、奖励、下一状态）。
    2. 计算优势函数 $\hat{A}_t$。
    3. 使用剪切目标函数 $L^{CLIP}(\theta)$ 更新策略参数。
    4. 更新价值函数参数，以最小化预测值与实际回报之间的差异。
3. 重复上述过程，直到策略收敛或达到预定的训练步数。

PPO算法在实践中表现出色，具有较高的样本效率和训练稳定性，已被广泛应用于各种强化学习任务中。

要实现PPO算法，我们先定义一个ActorCritic网络，使用卷积神经网络以便获取图像特征。

```python
class ActorCritic(nn.Module):
    def __init__(self, input_channels, action_dim):
        super(ActorCritic, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, 8, 4)
        self.conv2 = nn.Conv2d(32, 64, 4, 2)
        self.conv3 = nn.Conv2d(64, 64, 3, 1)
        self.fc1 = nn.Linear(2304, 512)  
        self.fc_pi = nn.Linear(512, action_dim)
        self.fc_v = nn.Linear(512, 1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)  # 展平
      
        x = F.relu(self.fc1(x))
        pi = self.fc_pi(x)
        v = self.fc_v(x)
        return pi, v
```

下面我们实现PPO中的截断操作：

```python
class PPO:
    def __init__(self, actor_critic, lr=2.5e-4, gamma=0.99, eps_clip=0.2, K_epochs=4):
        self.actor_critic = actor_critic
        self.optimizer = optim.Adam(actor_critic.parameters(), lr=lr)
        self.gamma = gamma
        self.eps_clip = eps_clip # 这是截断项，用于限制策略更新的步长
        self.K_epochs = K_epochs

    # 用于根据当前状态选择一个动作
    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0).to(next(self.actor_critic.parameters()).device)
        policy, value = self.actor_critic(state)
        dist = Categorical(logits=policy) # 创建一个类别分布，用于从策略中采样动作
        action = dist.sample() # 从分布中随机采样一个动作
        return action.item(), dist.log_prob(action), value

    # 用于根据记忆库中的数据更新模型
    def update(self, memory):
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)

        rewards = torch.tensor(rewards, dtype=torch.float32).to(next(self.actor_critic.parameters()).device)
        old_states = torch.cat(memory.states).detach().to(next(self.actor_critic.parameters()).device)
        old_actions = torch.cat(memory.actions).detach().to(next(self.actor_critic.parameters()).device)
        old_logprobs = torch.cat(memory.logprobs).detach().to(next(self.actor_critic.parameters()).device)

        for _ in range(self.K_epochs):
            # 获取旧状态的策略和价值估计
            policy, values = self.actor_critic(old_states) 
            dist = Categorical(logits=policy)
            logprobs = dist.log_prob(old_actions)
            dist_entropy = dist.entropy() # 计算分布的熵
            state_values = values.squeeze()

            ratios = torch.exp(logprobs - old_logprobs) # 计算新旧策略的概率比率
            advantages = rewards - state_values.detach() # 计算优势函数
            surr1 = ratios * advantages # 计算第一个截断项
            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages # 计算第二个截断项
            # 计算策略损失
            loss = -torch.min(surr1, surr2) + 0.5 * nn.MSELoss()(state_values, rewards) - 0.01 * dist_entropy

            self.optimizer.zero_grad() # 梯度清零
            loss.mean().backward() # 反向传播
            self.optimizer.step() # 更新参数
```

按惯例，我们定义一个记忆库，用于存储交互数据：

```python
class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []

    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]
```

另外，我们要对图像进行一些处理。

```python
import cv2
import numpy as np

# 这个函数用于预处理单个游戏帧
def preprocess(frame):
    if isinstance(frame, tuple) and len(frame) == 2:
        frame = frame[0]  # 假设需要第一个元素
    frame = np.array(frame)  # 确保 frame 是一个 NumPy 数组
    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # 转灰度
    frame = cv2.resize(frame, (80, 80))  # 缩放到 80x80
    frame = frame / 255.0  # 归一化
    return frame

# 这个函数用于将多个游戏帧堆叠起来，形成一个四帧的序列，这是许多强化学习算法中常用的输入格式
def stack_frames(stacked_frames, frame, is_new_episode):
    if is_new_episode:
        stacked_frames = np.stack([frame] * 4, axis=0)
    else:
        stacked_frames = np.concatenate((stacked_frames[1:, :, :], np.expand_dims(frame, 0)), axis=0)
    return stacked_frames
```

最后我们将所有部分组合起来，实现PPO算法：

```python
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import torch.nn.functional as F

class ActorCritic(nn.Module):
    def __init__(self, input_channels, action_dim):
        super(ActorCritic, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, 8, 4)
        self.conv2 = nn.Conv2d(32, 64, 4, 2)
        self.conv3 = nn.Conv2d(64, 64, 3, 1)
        self.fc1 = nn.Linear(2304, 512)  
        self.fc_pi = nn.Linear(512, action_dim)
        self.fc_v = nn.Linear(512, 1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)  # 展平
      
        x = F.relu(self.fc1(x))
        pi = self.fc_pi(x)
        v = self.fc_v(x)
        return pi, v

class PPO:
    def __init__(self, actor_critic, lr=2.5e-4, gamma=0.99, eps_clip=0.2, K_epochs=4):
        self.actor_critic = actor_critic
        self.optimizer = optim.Adam(actor_critic.parameters(), lr=lr)
        self.gamma = gamma
        self.eps_clip = eps_clip
        self.K_epochs = K_epochs

    def select_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0).to(next(self.actor_critic.parameters()).device)
        policy, value = self.actor_critic(state)
        dist = Categorical(logits=policy)
        action = dist.sample()
        return action.item(), dist.log_prob(action), value

    def update(self, memory):
        rewards = []
        discounted_reward = 0
        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):
            if is_terminal:
                discounted_reward = 0
            discounted_reward = reward + (self.gamma * discounted_reward)
            rewards.insert(0, discounted_reward)

        rewards = torch.tensor(rewards, dtype=torch.float32).to(next(self.actor_critic.parameters()).device)
        old_states = torch.cat(memory.states).detach().to(next(self.actor_critic.parameters()).device)
        old_actions = torch.cat(memory.actions).detach().to(next(self.actor_critic.parameters()).device)
        old_logprobs = torch.cat(memory.logprobs).detach().to(next(self.actor_critic.parameters()).device)

        for _ in range(self.K_epochs):
            policy, values = self.actor_critic(old_states)
            dist = Categorical(logits=policy)
            logprobs = dist.log_prob(old_actions)
            dist_entropy = dist.entropy()
            state_values = values.squeeze()

            ratios = torch.exp(logprobs - old_logprobs)
            advantages = rewards - state_values.detach()
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            loss = -torch.min(surr1, surr2) + 0.5 * nn.MSELoss()(state_values, rewards) - 0.01 * dist_entropy

            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()

class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
        self.is_terminals = []

    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_terminals[:]

import cv2
import numpy as np

def preprocess(frame):
    if isinstance(frame, tuple) and len(frame) == 2:
        frame = frame[0]  # 假设需要第一个元素
    frame = np.array(frame)  # 确保 frame 是一个 NumPy 数组
    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # 转灰度
    frame = cv2.resize(frame, (80, 80))  # 缩放到 80x80
    frame = frame / 255.0  # 归一化
    return frame

def stack_frames(stacked_frames, frame, is_new_episode):
    if is_new_episode:
        stacked_frames = np.stack([frame] * 4, axis=0)
    else:
        stacked_frames = np.concatenate((stacked_frames[1:, :, :], np.expand_dims(frame, 0)), axis=0)
    return stacked_frames

def main():
    env = gym.make('PongNoFrameskip-v4')
    input_channels = 4
    action_dim = env.action_space.n
    
    memory = Memory()
    actor_critic = ActorCritic(input_channels, action_dim).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    ppo = PPO(actor_critic)
    
    max_episodes = 1000
    max_timesteps = 10000
    update_timestep = 2000
    timestep = 0
    
    for episode in range(max_episodes):
        state = env.reset()
        state = preprocess(state)
        stacked_frames = stack_frames(None, state, True)
        
        for t in range(max_timesteps):
            timestep += 1
            
            action, log_prob, value = ppo.select_action(stacked_frames)
            new_state, reward, done, _ = env.step(action)
            new_state = preprocess(new_state)
            stacked_frames = stack_frames(stacked_frames, new_state, False)
            
            # 确保输入形状为 [batch_size, channels, height, width]
            input_tensor = torch.tensor(stacked_frames, dtype=torch.float32).unsqueeze(0)  # 添加 batch 维度
            memory.states.append(input_tensor)
            memory.actions.append(torch.tensor([action]))
            memory.logprobs.append(log_prob)
            memory.rewards.append(reward)
            memory.is_terminals.append(done)
            
            if timestep % update_timestep == 0:
                ppo.update(memory)
                memory.clear_memory()
                timestep = 0
                
            if done:
                break
        
        print(f"Episode: {episode}, Reward: {sum(memory.rewards)}")

if __name__ == '__main__':
    main()
```

运行结果如下：

```
Episode: 0, Reward: -10.0
Episode: 1, Reward: -7.0
Episode: 2, Reward: -14.0
Episode: 3, Reward: -7.0
Episode: 4, Reward: -1.0
Episode: 5, Reward: -8.0
Episode: 6, Reward: -1.0
Episode: 7, Reward: -9.0
Episode: 8, Reward: -2.0
Episode: 9, Reward: -10.0
Episode: 10, Reward: -3.0
...
```

下面是原论文中PPO算法在Atari游戏上的实验结果：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/ppo_atari.png)

#### 12.2.4 A3C算法

A3C（Asynchronous Advantage Actor-Critic，异步优势行为者-评论者）是一种由DeepMind提出的改进Actor-Critic算法。A3C通过并行执行多个代理（agent），加快了训练过程，并提高了策略的稳定性和样本效率。

算法步骤

1. 全局网络与本地网络：
    - 全局网络存储共享的策略和价值函数参数。
    - 每个代理拥有一个本地网络，本地网络与全局网络参数相同，但在每个代理内独立更新。

2. 与环境交互：
    - 每个代理与环境互动，收集状态、动作、奖励、下一状态等数据。

3. 计算优势函数：
    - 优势函数可以基于状态值函数 $V(s)$ 和动作值函数 $Q(s, a)$ 计算：$\hat{A}(s, a) = Q(s, a) - V(s)$

4. 更新本地网络参数：
    - 通过梯度下降法，使用本地数据更新本地网络参数。

5. 异步更新全局网络参数：将本地网络的梯度异步应用到全局网络，更新全局参数。

算法流程

1. 初始化全局策略参数 $\theta$ 和全局价值函数参数 $\theta_v$。
2. 启动多个代理，每个代理执行以下步骤：
    1. 初始化本地策略参数 $\theta'$ 和本地价值函数参数 $\theta_v'$，并从全局网络复制参数。
    2. 与环境交互，收集一批数据（状态、动作、奖励、下一状态）。
    3. 计算优势函数 $\hat{A}(s, a)$。
    4. 使用本地数据更新本地策略参数 $\theta'$ 和本地价值函数参数 $\theta_v'$。
    5. 将本地网络的梯度异步应用到全局网络，更新全局参数 $\theta$ 和 $\theta_v$。
3. 重复上述过程，直到策略收敛或达到预定的训练步数。

#### 12.2.5 DDPG算法

DDPG（Deep Deterministic Policy Gradient，深度确定性策略梯度）也是一种基于Actor-Critic框架的算法，适用于连续动作空间。它结合了确定性策略梯度和深度Q网络的优点，能够有效处理高维连续动作空间问题。

与随机策略不同，确定性策略直接输出一个具体的动作，而不是动作的概率分布。

参考了DQN的机制，DDPG使用了经验回放缓冲区（Replay Buffer）和目标网络（Target Network）来提高训练的稳定性。
DDPG使用经验回放缓冲区（replay buffer）存储代理与环境交互的经验 $(s_t, a_t, r_t, s_{t+1})$。
从缓冲区中随机采样经验进行训练，以打破数据相关性，提高样本效率。

为了稳定训练过程，使用目标网络 $\theta^{\mu'}$ 和 $\theta^{Q'}$，它们的参数是Actor和Critic网络参数的延迟复制。


1. 初始化Actor网络 $\mu(s|\theta^{\mu})$ 和Critic网络 $Q(s, a|\theta^{Q})$ 的参数 $\theta^{\mu}$ 和 $\theta^{Q}$。
2. 初始化目标网络 $\theta^{\mu'} \leftarrow \theta^{\mu}$ 和 $\theta^{Q'} \leftarrow \theta^{Q}$。
3. 初始化经验回放缓冲区 $\mathcal{D}$。
4. 在每个时间步：
    1. 从当前状态 $s_t$ 开始，根据策略 $\mu(s_t|\theta^{\mu})$ 选择动作 $a_t$，并添加探索噪声 $N_t$。
    2. 执行动作 $a_t$，观察奖励 $r_t$ 和新的状态 $s_{t+1}$。
    3. 将经验 $(s_t, a_t, r_t, s_{t+1})$ 存储到回放缓冲区 $\mathcal{D}$。
    4. 从回放缓冲区 $\mathcal{D}$ 中随机采样一个小批量 $(s_i, a_i, r_i, s_{i+1})$。
    5. 计算目标值 $y_i$：$y_i = r_i + \gamma Q'(s_{i+1}, \mu'(s_{i+1}|\theta^{\mu'})|\theta^{Q'})$
    6. 最小化Critic的损失：$L = \frac{1}{N} \sum_i (y_i - Q(s_i, a_i|\theta^Q))^2$
    7. 使用梯度下降法更新Critic网络参数 $\theta^Q$。
    8. 使用策略梯度法更新Actor网络参数 $\theta^{\mu}$：$\nabla_{\theta^{\mu}} J \approx \frac{1}{N} \sum_i \nabla_a Q(s, a|\theta^Q)|_{s=s_i, a=\mu(s_i)} \nabla_{\theta^{\mu}} \mu(s|\theta^{\mu})|_{s_i}$
    9. 软更新目标网络参数：$\theta^{Q'} \leftarrow \tau \theta^Q + (1 - \tau) \theta^{Q'}$ $\theta^{\mu'} \leftarrow \tau \theta^{\mu} + (1 - \tau) \theta^{\mu'}$
其中，$\tau$ 是一个小常数，通常设为0.001。

#### 12.2.6 TD3算法

TD3（Twin Delayed Deep Deterministic Policy Gradient）是是对DDPG（Deep Deterministic Policy Gradient）算法的改进，旨在缓解DDPG中存在的一些问题，如过估计和训练不稳定性。TD3通过以下几个关键策略来增强稳定性和性能：

- 双重Q网络（Double Q Learning）：
TD3使用了两个独立的Q网络来估计Q值。通过选择两个Q值中的较小值来更新目标Q值，可以有效地减少过估计偏差。
$y = r + \gamma \min(Q_{\theta_1'}(s', \pi_{\phi'}(s')), Q_{\theta_2'}(s', \pi_{\phi'}(s')))$
- 延迟策略更新（Delayed Policy Updates）：
TD3引入了延迟策略更新机制，即策略网络（π）的更新频率低于Q网络。这种策略可以使Q网络更好地收敛，从而提高策略网络的更新效果。

```
if time_step % policy_delay == 0:
    Update policy network
```

- 目标策略平滑（Target Policy Smoothing）：
在计算目标动作时，TD3向动作中添加噪声，从而使得目标Q值更加平滑。这减少了策略对目标Q值高峰的依赖，增强了策略的鲁棒性。

$\pi'(s') = \pi(s') + \epsilon, \quad \epsilon \sim \text{clip}(\mathcal{N}(0, \sigma), -c, c)$

TD3算法的主要步骤

- 初始化：
    - 初始化策略网络和两个Q网络，以及它们对应的目标网络。
    - 初始化经验回放缓冲区。
- 经验采集：根据当前策略与环境交互，收集状态、动作、奖励和下一个状态。
- 经验存储：将收集到的经验存储在回放缓冲区中。
- 经验回放与训练：
    - 从回放缓冲区中随机采样一批经验。
    - 使用双重Q网络计算目标Q值，并更新Q网络的参数。
    - 每隔一定步数，更新策略网络的参数。
    - 更新目标网络的参数，使其慢慢跟随当前网络参数。

TD3的伪代码

```
Initialize critic networks Q1, Q2 with random parameters θ1, θ2
Initialize actor network π with random parameters φ
Initialize target networks Q1', Q2', π' with θ1' ← θ1, θ2' ← θ2, φ' ← φ
Initialize replay buffer R

for each iteration do
    for each environment step do
        Select action with exploration noise: a ← π(s) + noise
        Execute action a and observe reward r and next state s'
        Store transition (s, a, r, s') in replay buffer R
    
    for each gradient step do
        Sample a batch of transitions (s, a, r, s') from R
        Compute target actions with policy smoothing: a' ← π'(s') + clipped noise
        Compute target Q values: y ← r + γ min(Q1'(s', a'), Q2'(s', a'))
        Update critics by minimizing the loss: L(θi) = 1/N Σ (Qθi(s, a) - y)^2
        if step % policy_delay == 0 then
            Update the policy using the sampled policy gradient
            Update target networks with polyak averaging:
            θ1' ← τθ1 + (1-τ)θ1'
            θ2' ← τθ2 + (1-τ)θ2'
            φ' ← τφ + (1-τ)φ'
```

TD3在许多连续控制任务中表现出色，如OpenAI Gym的Mujoco环境，显示出比DDPG更稳定和高效的性能。

#### 12.2.7 SAC算法

SAC（Soft Actor-Critic）旨在通过在策略优化过程中引入熵正则化来实现更好的探索和平衡。它在连续动作空间任务中表现出色，因其稳定性和高效性而受到广泛关注。

SAC的目标是最大化累积奖励的同时，最大化策略的熵。策略的熵越大，表示策略越随机，从而增强了探索能力。通过引入熵项，SAC不仅关注收益最大化，还鼓励策略的多样性。

SAC的主要步骤
- 初始化：
初始化策略网络（Actor）和两个Q网络（Critic），以及它们对应的目标网络。
初始化经验回放缓冲区。
- 经验采集：
根据当前策略与环境交互，收集状态、动作、奖励和下一个状态。
- 经验存储：
将收集到的经验存储在回放缓冲区中。
- 经验回放与训练：
从回放缓冲区中随机采样一批经验。
使用双重Q网络计算目标Q值，并更新Q网络的参数。
更新策略网络的参数，最大化预期收益和策略熵之和。
更新目标网络的参数，使其慢慢跟随当前网络参数。
- SAC的目标函数
SAC的目标函数包括两部分：Q函数的贝尔曼期望方程和策略的最大熵目标。

Q值目标：
通过最小化贝尔曼残差来更新Q网络：
$L(Q) = \mathbb{E}_{(s, a, r, s') \sim D} \left[\left(Q(s, a) - (r + \gamma (V(s') - \alpha \log \pi(a'|s')))\right)^2\right]$

策略目标：
通过最大化期望奖励和策略熵来更新策略网络：

$J(\pi) = \mathbb{E}_{s \sim D, a \sim \pi} \left[\alpha \log \pi(a|s) - Q(s, a)\right]$

温度参数：
温度参数 $\alpha$ 控制熵项的权重，可以是固定值，也可以是可学习的：
$J(\alpha) = \mathbb{E}_{a \sim \pi} \left[-\alpha \log \pi(a|s) - \alpha H_{\text{target}}\right]$

SAC算法的伪代码
```
Initialize critic networks Q1, Q2 with random parameters θ1, θ2
Initialize actor network π with random parameters φ
Initialize target networks Q1', Q2' with θ1' ← θ1, θ2' ← θ2
Initialize temperature parameter α

for each iteration do
    for each environment step do
        Select action a ~ π(s) + noise
        Execute action a and observe reward r and next state s'
        Store transition (s, a, r, s') in replay buffer

    for each gradient step do
        Sample a batch of transitions (s, a, r, s') from replay buffer
        Compute target Q value: y = r + γ (min(Q1'(s', a'), Q2'(s', a')) - α log π(a'|s'))
        Update critics by minimizing the loss: L(θi) = 1/N Σ (Qθi(s, a) - y)^2
        Update policy by maximizing the objective: J(π) = 1/N Σ (α log π(a|s) - Q(s, a))
        Update temperature parameter: J(α) = 1/N Σ (-α log π(a|s) - α H_target)
        Update target networks with polyak averaging:
        θ1' ← τθ1 + (1-τ)θ1'
```

### 12.3 强化学习编程基础

强化学习中最经常使用的环境工具是OpenAI的Gym。Gym是一个用于开发和比较强化学习算法的工具包。它提供了一个简单的接口，可以在不同的环境中测试代理。2021年，是gym升级为gymnasium库，它提供了更多的环境和功能。

不管是gym还是gymanasium，它们都是体育馆的意思，就是让代理在里面锻炼。

作为强化学习最常用的工具，gym一直在不停地升级和折腾，比如gym[atari]变成需要要安装接受协议的包啦，atari环境不支持Windows环境啦之类的，另外比较大的变化就是2021年接口从gym库变成了gymnasium库。让大量的讲强化学习的书中介绍环境的部分变得需要跟进升级了。

不过，不管如何变，gym[nasium]作为强化学习的代理库的总的设计思想没有变化，变的都是接口的细节。

下面我们就来尝试不使用任何强化学习算法，而手工写一个代理程序。

#### 12.3.1 实现第一个代理

总体来说，对于gymnasium我们只需要做两件事情：一个是初始化环境，另一个就是通过step函数不停地给环境做输入，然后观察对应的结果。

初始化环境分为两步。
第一步是创建gymnasium工厂中所支持的子环境，比如我们使用经典的让一个杆子不倒的CartPole环境：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/CartPole1.png)

```python
import gymnasium as gym
env = gym.make("CartPole-v1")
```
第二步，我们就可以通过env的reset函数来进行环境的初始化：
```python
observation, info = env.reset(seed=42)
```

我们可以将observation打印出来，它一个4元组，4个数值分别表示：
- 小车位置
- 小车速度
- 棍的倾斜角度
- 棍的角速度

如果角度大于12度，或者小车位置超出了2.4，就意味着失败了，直接结束。

小车的输入就是一个力，要么是向左的力，要么是向右的力。0是向左推小车，1是向右推小车。

下面我们让代码跑起来。

首先我们通过pip来安装gymnasium的包：
```
pip install gymnasium -U
```

安装成功之后，

```python
import gymnasium as gym
env = gym.make("CartPole-v1")

print(env.action_space)

observation, info = env.reset(seed=42)
steps = 0
for _ in range(1000):
    action = env.action_space.sample()
    observation, reward, terminated, truncated, info = env.step(action)
    print(observation)

    if terminated or truncated:
        print("Episode finished after {} steps".format(steps))
        observation, info = env.reset()
        steps = 0
    else:
        steps += 1
        
env.close()
```

env.action_space输出是Discrete(2)。也就是两个离散的值0和1。前面我们介绍了，这分别代表向左和向右推动小车。

observation输出的4元组，我们前面也讲过了，像这样：
[ 0.0273956  -0.00611216  0.03585979  0.0197368 ]

下面就是关键的step一步：
```python
    action = env.action_space.sample()
    observation, reward, terminated, truncated, info = env.step(action)
```

刚才我们介绍了，CartPole的输入只有0和1两个值。我们采用随机让其左右动的方式来试图让小车不倒。

如果你觉得还是不容易懂的话，我们可以来个更无脑的，管它是什么情况，我们都一直往左推：
```python
observation, reward, terminated, truncated, info = env.step(0)
```
基本上几步就完了：
```
[ 0.02699083 -0.16518621 -0.00058549  0.3023946 ] 1.0 False False {}
[ 0.0236871  -0.36029983  0.0054624   0.5948928 ] 1.0 False False {}
[ 0.01648111 -0.5554978   0.01736026  0.88929135] 1.0 False False {}
[ 0.00537115 -0.750851    0.03514608  1.1873806 ] 1.0 False False {}
[-0.00964587 -0.94641054  0.0588937   1.4908696 ] 1.0 False False {}
[-0.02857408 -1.1421978   0.08871109  1.8013463 ] 1.0 False False {}
[-0.05141804 -1.3381925   0.12473802  2.1202288 ] 1.0 False False {}
[-0.07818189 -1.534317    0.16714258  2.4487078 ] 1.0 False False {}
[-0.10886823 -1.7304213   0.21611674  2.7876763 ] 1.0 True False {}
Episode finished after 8 steps
```

下面我们解释下返回的5元组，observation就是位置4元组，reward是用于强化学习的奖励，在本例中只要是不死就是1. terminated就是是否游戏结束了。
Truncated在官方定义中用于处理比如超时等特殊结束的情况。
truncated, info对于CartPole来说没有用到。

搭建好了gymnasium环境之后，我们就可以进行策略的升级与迭代了。
比如我们写死一个策略，如果位置小于0则向右推，反之则向左推：

```python
def action_pos(status): 
    pos, v, ang, va = status
    #print(status)
    if pos <= 0: 
        return 1
    else: 
        return 0 
```

或者我们根据角度来判断，如果角度大于0则左推，反之则右推：
```python
def action_angle(status): 
    pos, v, ang, va = status
    #print(status)
    if ang > 0: 
        return 1
    else: 
        return 0
```

角度策略的完整代码如下：
```python
import gymnasium as gym
env = gym.make("CartPole-v1")
#env = gym.make("CartPole-v1",render_mode="human")

print(env.action_space)
#print(env.get_action_meanings())

observation, info = env.reset(seed=42)
print(observation,info)

def action_pos(status): 
    pos, v, ang, va = status
    #print(status)
    if pos <= 0: 
        return 1
    else: 
        return 0 

def action_angle(status): 
    pos, v, ang, va = status
    #print(status)
    if ang > 0: 
        return 1
    else: 
        return 0

steps = 0
for _ in range(1000):
    action = env.action_space.sample()
    observation, reward, terminated, truncated, info = env.step(action_angle(observation))
    print(observation, reward, terminated, truncated, info)

    if terminated or truncated:
        print("Episode finished after {} steps".format(steps))
        observation, info = env.reset()
        steps = 0
    else:
        steps += 1
        
env.close()
```

#### 12.3.2 Atari游戏环境

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/Atari.png)

我们通过gymnasium[atari]包来安装atari游戏的gymnasium支持。

```
pip install gymnasium[atari]
```

与之前的gym一样，gymnasium默认是不安装atari游戏的，需要通过accept-rom-license包来安装游戏。

```
!pip install gymnasium[accept-rom-license]
```

我们可以通过get_action_meanings来获取游戏支持的操作

之前的CartPole只知道是离散的两个值。而Atari游戏则可支持获取游戏支持的操作的含义：
```
['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']
```

下面再说下rendor_mode，这个参数是用于显示游戏画面的。如果是human模式，会显示游戏画面，如果是rgb_array模式，只会输出游戏画面的RGB数组。

针对于Atari游戏，render_mode现在是必选项了。要指定是显示成人类可看的human模式，还是只输出rgb_array的模式。

我们以乒乓球游戏为例，组装让其运行起来：

```
import gymnasium as gym
env = gym.make("ALE/Pong-v5", render_mode="human")
observation, info = env.reset()

print(env.get_action_meanings())

scores = 0

for _ in range(1000):
    action = env.action_space.sample()  # agent policy that uses the observation and info
    observation, reward, terminated, truncated, info = env.step(action)
    #print(observation, reward, terminated, truncated, info)

    if terminated or truncated:
        print("Episode finished after {} steps".format(scores))
        observation, info = env.reset()
        scores = 0
    else:
        scores +=1

env.close()
```

完整的游戏支持列表可以在https://gymnasium.farama.org/environments/atari/ 官方文档中查到。

### 12.4 通过stable-baselines3库训练强化学习模型

我们可以通过调用库的方式，不编写一行自己的强化学习代码，就可以训练一个强化学习模型。

stable-baselines3等强化学习库已经对gymnasium进行了支持，所以我们可以在stable-baselines3中直接使用gymnasium的环境。

先安装库：
```
pip install gymnasium[atari]
pip install gymnasium[accept-rom-license]
pip install stable_baselines3
```

#### 12.4.1 用DQN算法实现强化学习

我们以乒乓球游戏为例。乒乓球游戏的规则大家都能理解，在游戏里，我们可以控制球拍上下移动，目标是让球拍击中球，不让球飞出边界。这个操作我们称为"动作"。

在这个游戏中，选择正确的动作会得分，而错误的决策就会失分，也就是对手得分。

想象有一个机器人正在尝试学习玩这个游戏。它通过尝试不同的动作并记住哪些动作导致了高分，哪些导致了低分来学习。这个“记住和学习”的过程就是DQN算法的核心。

另外，我们要有“未来的眼光”。做出决策时不仅要考虑立即获得的分数，还要考虑这个决策会如何影响未来的得分。比如，在游戏中，某个动作可能不是立刻得分，但它能帮助你在游戏后面的某个部分获得更多的分数。

一次玩游戏并不能让机器人学会。要让机器人不断通过玩游戏、尝试不同的动作、记住结果，并从中学习如何做出最好的决策，以获得尽可能多的分数。机器人会不断地重复这个过程，每次都试图改进，以变得更擅长游戏。

稍正式一点，我们可以这样描述DQN算法的工作流程：
- DQN通过不断与环境交互,收集状态、动作、奖励和下一状态的数据
- 将这些数据存储在一个 "经验回放池"(replay buffer)中
- 从回放池中随机采样一批数据,用于训练神经网络
- 通过最小化预测Q值和目标Q值(bellman方程)的误差,来更新神经网络参数

下面，我们用DQN算法来训练乒乓球游戏：

```python
import gymnasium as gym
import numpy as np

from stable_baselines3 import DQN
from stable_baselines3.dqn import CnnPolicy


game = 'ALE/Pong-v5'

env = gym.make(game,render_mode="rgb_array")

save_file = 'dqn_'+game;

print(env.action_space)
model = DQN(CnnPolicy, env, verbose=1,exploration_final_eps=0.01,exploration_fraction=0.1,gradient_steps=1,learning_rate=0.0001,buffer_size=10000)
model.set_env(env)
model.learn(total_timesteps=1000000, log_interval=10)
model.save(save_file)

obs,info = env.reset()

score = 0
rewards_sum = 0

while True:
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    score = score + 1
    rewards_sum += reward
    if reward > 0:
        print('win!!!', reward)

    if terminated or truncated:
        # obs = env.reset()
        print('finished', score)
        print('reward sum=', rewards_sum)
        break
```

上面的代码我们还可以做两处改进：
1. 如果存在save_file，我们可以直接加载模型，在原有模型上继续训练。
2. 我们可以增加一个测试模式，观看训练后模型打游戏的真实效果。

同时，在colab上运行的话，我们可以将模型保存到google drive上，这样可以避免每次重新训练。

先要挂载google drive：

```python
from google.colab import drive
drive.mount('/content/drive')
```

然后我们可以把模型保存到google drive上：

```python
import gymnasium as gym
import numpy as np

import time
from datetime import datetime

from stable_baselines3 import DQN
from stable_baselines3.dqn import MlpPolicy
from stable_baselines3.dqn import CnnPolicy

game = 'ALE/Pong-v5'

#eval = True
eval = False

#cont = True
cont = False

print (time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))

start_time = time.time()
start_date = datetime.now()

if eval:
    env = gym.make(game,render_mode="human")
else:
    env = gym.make(game,render_mode="rgb_array")

save_file = '/content/drive/MyDrive/rl/dqn_'+game;

print(env.action_space)

if eval:
    model = DQN.load(save_file)
    model.set_env(env)
else:
    if cont:
        model = DQN.load(save_file)
    else:
        model = DQN(CnnPolicy, env, verbose=1,exploration_final_eps=0.01,exploration_fraction=0.1,gradient_steps=1,learning_rate=0.0001,buffer_size=10000)

    model.set_env(env)
    model.learn(total_timesteps=1000000, log_interval=10)
    model.save(save_file)

obs,info = env.reset()

score = 0
rewards_sum = 0

while True:
    action, _states = model.predict(obs)
    obs, reward, terminated, truncated, info = env.step(action)
    if eval:
        env.render()

    score = score + 1
    rewards_sum += reward
    if reward > 0:
        print('win!!!', reward)

    if terminated or truncated:
        print('finished', score)
        print('reward sum=', rewards_sum)
        break

duration = time.time() - start_time
print('duration=', duration)

time_cost = datetime.now() - start_date
print('time cost=', time_cost)
```

我们将模型的结构打印出来：

```python
import torch
print(model.policy)
```

```
CnnPolicy(
  (q_net): QNetwork(
    (features_extractor): NatureCNN(
      (cnn): Sequential(
        (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))
        (1): ReLU()
        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
        (3): ReLU()
        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
        (5): ReLU()
        (6): Flatten(start_dim=1, end_dim=-1)
      )
      (linear): Sequential(
        (0): Linear(in_features=22528, out_features=512, bias=True)
        (1): ReLU()
      )
    )
    (q_net): Sequential(
      (0): Linear(in_features=512, out_features=6, bias=True)
    )
  )
  (q_net_target): QNetwork(
    (features_extractor): NatureCNN(
      (cnn): Sequential(
        (0): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))
        (1): ReLU()
        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
        (3): ReLU()
        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
        (5): ReLU()
        (6): Flatten(start_dim=1, end_dim=-1)
      )
      (linear): Sequential(
        (0): Linear(in_features=22528, out_features=512, bias=True)
        (1): ReLU()
      )
    )
    (q_net): Sequential(
      (0): Linear(in_features=512, out_features=6, bias=True)
    )
  )
)
```

从结构图中可以看到，这个网络有两个主要部分：q_net和q_net_target。q_net用于估计动作的价值，而q_net_target用于目标价值的估计。

下面是这个神经网络结构的详细解释：

- 输入层：网络接受一个3通道（RGB）的图像作为输入，这通常是在强化学习环境中捕获的屏幕截图。
- 特征提取器（NatureCNN）：这是一个卷积神经网络（CNN），用于从输入图像中提取特征。它包含以下层：
    - 第一个Conv2d层：使用8x8的卷积核和步长为4的步长进行卷积操作，输出32个特征图。
    - 第二个ReLU层：应用线性整流函数（ReLU）作为激活函数，增加非线性。
    - 第二个Conv2d层：使用4x4的卷积核和步长为2的步长进行卷积操作，输出64个特征图。
    - 第三个ReLU层：再次应用ReLU激活函数。
    - 第三个Conv2d层：使用3x3的卷积核和步长为1的步长进行卷积操作，输出64个特征图。
    - 第四个ReLU层：再次应用ReLU激活函数。
    - Flatten层：将卷积层的输出展平，以便与全连接层兼容。
- 全连接层（Linear）：
    - 第一个Linear层：将展平后的特征图转换为512维的特征向量。
    - 第二个ReLU层：再次应用ReLU激活函数。
- Q值估计层（q_net）：第一个Linear层：将512维的特征向量映射到与动作数量相同的维度（在这个例子中是6个动作）。
- 目标网络（q_net_target）：这个网络与q_net结构相同，但参数不同。在DQN算法中，目标网络用于提供稳定的学习信号，防止过度乐观的估计。

#### 12.4.2 PPO算法

有了上面的框架之后，我们把DQN算法换成PPO算法，就可以让PPO算法来玩乒乓球游戏了。

PPO 算法就像一个教练，可以帮助你找到一个更好的策略。它会观察你的游戏，并告诉你哪些动作更有可能让你收集到硬币。它还会帮助你调整你的策略，这样你就可以采取更多的高奖励动作。

PPO 算法通过不断尝试和学习来工作。它会尝试不同的策略，并根据你收到的奖励和惩罚来调整其建议。通过这种方式，它可以帮助你找到收集尽可能多硬币的最佳策略。

与 DQN 算法不同，PPO 算法不需要经验回放或目标网络。这使得它在某些情况下比 DQN 算法更有效。

```python
import gymnasium as gym
import numpy as np

import time
from datetime import datetime

from stable_baselines3 import DQN
from stable_baselines3.dqn import MlpPolicy
from stable_baselines3.dqn import CnnPolicy

game = 'ALE/Pong-v5'

#eval = True
eval = False

cont = True
#cont = False

print (time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))

start_time = time.time()
start_date = datetime.now()

if eval:
    env = gym.make(game,render_mode="human")
else:
    env = gym.make(game,render_mode="rgb_array")

save_file = '/content/drive/MyDrive/rl/dqn_'+game;

print(env.action_space)

if eval:
    model = PPO.load(save_file)
    model.set_env(env) 
else:
    if cont:
        model = PPO.load(save_file)
    else:
        model = PPO(MlpPolicy, env, verbose=1,learning_rate=2.5e-4,clip_range=0.1,vf_coef=0.5,ent_coef=0.01,n_steps=128)    
    model.set_env(env)
    model.learn(total_timesteps=1000000, log_interval=10)
    model.save(save_file)

obs,info = env.reset()

score = 0
rewards_sum = 0

while True:
    action, _states = model.predict(obs)
    obs, reward, terminated, truncated, info = env.step(action)
    if eval:
        env.render()

    score = score + 1
    rewards_sum += reward
    if reward > 0:
        print('win!!!', reward)

    if terminated or truncated:
        print('finished', score)
        print('reward sum=', rewards_sum)
        break

duration = time.time() - start_time
print('duration=', duration)

time_cost = datetime.now() - start_date
print('time cost=', time_cost)
```

我们可以打印PPO.policy来查看PPO的具体网络结构。

```python
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack

# There already exists an environment generator that will make and wrap atari environments correctly.
env = make_atari_env("PooyanNoFrameskip-v4", n_envs=4, seed=0)
# Stack 4 frames
env = VecFrameStack(env, n_stack=4)

model = PPO("CnnPolicy", env, verbose=1)
model.learn(total_timesteps=100_000)

import torch
print(model.policy)
```

输出的结构如下：

```
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=6, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)
```

我们来解释下这个网络结构：

- 特征提取器 (NatureCNN):
    - 有三个相同的NatureCNN结构：features_extractor, pi_features_extractor, 和 vf_features_extractor。
    - 每个NatureCNN包含:
        - 卷积层 (CNN): 3层卷积网络，每层后跟ReLU激活函数
        - 扁平化层: 将多维输出转为一维
        - 线性层: 512个神经元的全连接层，后跟ReLU激活函数
- MLP提取器 (MlpExtractor):包含策略网络和价值网络，但这里都是空的Sequential()
- 动作网络 (action_net):线性层，输入512维，输出6维（对应6个可能的动作）
- 价值网络 (value_net): 线性层，输入512维，输出1维（估计状态价值）

工作流程：

- 输入图像（4通道，对应4帧堆叠）经过NatureCNN处理
- 提取的特征通过MLP提取器（这里是空的）
- 然后分别输入到动作网络和价值网络
- 动作网络输出动作概率分布，价值网络输出状态价值估计

#### 12.4.3 视频输出 - 从Monitor到RecordVideo

有时候我们希望把游戏的视频输出出来，gym曾经使用Monitor来实现。现在gymnasium则改用RecordVideo来实现。

使用RecordVideo需要先安装moviepy库：
```
pip install moviepy
```

然后从gymnasium.wrappers包中引用RecordVideo：

```python
from gymnasium.wrappers import RecordVideo
```

human模式是没有办法输出视频的，所以我们需要把human模式改成rgb_array模式。然后我们指定RecordVideo的输出目录就可以了：

```python
env = gym.make(game,render_mode="rgb_array")
env = RecordVideo(env, './video')
```

输出默认是mp4格式，如果需要其他格式，比如我们在网页中要显示成gif格式，可以使用ffmpeg来转换：

```p
ffmpeg -i rl-video-episode-0.mp4 -vf "fps=10,scale=320:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse" -loop 0 output.gif
```

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/pong1.gif)

我们换个游戏：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/pig1.gif)

如果某大漠老师强烈bs你使用gif，那么也可以转成apng格式：

```
ffmpeg -i rl-video-episode-0.mp4  output.apng
```

效果如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/rl1.apng)


### 12.5 蒙特卡洛树搜索

蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）是一种用于决策过程的算法，特别适用于博弈问题（如围棋、国际象棋等）。它结合了蒙特卡洛方法和树搜索方法，通过模拟未来可能的决策路径来评估当前决策的价值。MCTS的核心思想是通过多次随机模拟来估计每个决策的长远价值，从而选择最优的决策。

MCTS 主要由以下四个步骤组成：

- 选择（Selection）：从根节点开始，使用某种策略（如UCB1, Upper Confidence Bound）选择一个子节点，直到找到一个未被完全扩展的节点。
- 扩展（Expansion）：如果选择的节点不是终局，就根据可能的动作扩展一个或多个子节点。
- 模拟（Simulation）：从扩展的节点开始，进行一次完整的随机模拟，直到达到终局。模拟过程使用随机策略或其他启发式策略。
- 回溯（Backpropagation）：将模拟的结果（胜负）从模拟终局向上回溯，更新所有经过的节点的统计信息（如访问次数和胜率）。

在选择步骤中，MCTS 通常使用 UCB1（Upper Confidence Bound）公式来平衡探索（exploration）和利用（exploitation）：

$\text{UCB1}=\frac{w_i}{n_i}+ C\sqrt{\frac{\ln N}{n_i}} $

其中，
- $w_i$ 是节点 $i$ 的总奖励，
- $n_i$ 是节点 $i$ 的访问次数，
- $N$ 是总访问次数，
- $C$ 是一个探索参数，用于调节探索和利用之间的平衡

我们尝试实现一个简单的蒙特卡洛树搜索算法。

凡是树，首先都是要定义一个节点：

```python
import math
import numpy as np

class MCTSNode:
    def __init__(self, state, parent=None):
        self.state = state
        self.parent = parent
        self.children = []
        self.visits = 0
        self.value = 0.0

    def is_leaf(self):
        return len(self.children) == 0

    def expand(self, game, valid_actions):
        for action in valid_actions:
            next_state, reward, done = self.simulate_action(game, action)
            self.children.append((action, MCTSNode(next_state, parent=self), reward, done))

    def simulate_action(self, game, action):
        game.env.env.state = self.state
        state, reward, done, _ = game.step(action)
        return state, reward, done

    def best_child(self, c_param=1.4):
        if len(self.children) == 0:
            return None
        choices_weights = [
            (child[1].value / (child[1].visits + 1e-5)) + c_param * math.sqrt((2 * math.log(self.visits + 1) / (child[1].visits + 1e-5)))
            for child in self.children
        ]
        return self.children[np.argmax(choices_weights)][1]

    def update(self, reward):
        self.visits += 1
        self.value += reward
```

每个节点包含以下属性：

- state：当前节点的状态。
- parent：指向父节点的引用。
- children：子节点的列表。
- visits：该节点被访问的次数。
- value：该节点的价值，通常是根据模拟的结果计算得出的。

MCTSNode类的方法包括：

- is_leaf：判断当前节点是否是叶子节点，即没有子节点。
- expand：扩展当前节点，创建新的子节点。
- simulate_action：模拟执行一个行动，返回新的状态、奖励和游戏是否结束的标志。
- best_child：选择最佳的子节点，根据UCB1公式计算每个子节点的权重。
- update：更新节点的访问次数和价值。

下面我们实现这个树：

```python
class MCTS:
    def __init__(self, num_simulations):
        self.num_simulations = num_simulations

    def search(self, root_state, game):
        root = MCTSNode(root_state)
        for _ in range(self.num_simulations):
            node = root
            # Selection
            while not node.is_leaf() and node.best_child() is not None:
                node = node.best_child()

            # Expansion
            if not self.is_terminal(node.state):
                node.expand(game, [0, 1])  # Blackjack 的动作是 0（保持）和 1（要牌）

            # Simulation
            reward = self.simulate(game, node.state)

            # Backpropagation
            self.backpropagate(node, reward)

        best_child_node = root.best_child(c_param=0)
        return best_child_node.state if best_child_node else root.state

    def simulate(self, game, state):
        game.env.env.state = state
        done = False
        total_reward = 0
        while not done:
            action = game.action_space.sample()
            state, reward, done, _ = game.step(action)
            total_reward += reward
        return total_reward

    def backpropagate(self, node, reward):
        while node is not None:
            node.update(reward)
            reward = -reward  # 反转奖励以模拟两玩家对抗
            node = node.parent

    def is_terminal(self, state):
        # 假设状态为 None 或者是 done 状态表示游戏结束。可以根据实际情况修改。
        return state is None or state[2]  # state[2] 是一个布尔值，表示是否游戏结束
```

MCTS类实现了MCTS算法的主要逻辑。它的方法包括：

- search：执行MCTS搜索，返回最佳的状态。
- simulate：从给定的状态开始，随机模拟游戏的进行，直到游戏结束，返回总奖励。
- backpropagate：将模拟得到的奖励反向传播到根节点。
- is_terminal：判断给定的状态是否是终止状态。

在search方法中，算法首先创建一个根节点，然后进行多次模拟。每次模拟包括四个步骤：

- Selection：从根节点开始，选择最佳的子节点，直到到达一个叶子节点。
- Expansion：如果叶子节点不是终止状态，就扩展它，创建新的子节点。
- Simulation：从新的子节点开始，随机模拟游戏的进行，直到游戏结束。
- Backpropagation：将模拟得到的奖励反向传播到根节点，更新沿途节点的价值和访问次数。

最后，search方法返回具有最高价值的子节点的状态，这代表了在当前状态下最佳的行动。

我们用21点游戏，将上面的代码串起来：

```python
import gym

class BlackjackEnv:
    def __init__(self):
        self.env = gym.make('Blackjack-v1')
        self.action_space = self.env.action_space
        self.observation_space = self.env.observation_space

    def reset(self):
        return self.env.reset()

    def step(self, action):
        return self.env.step(action)

    def render(self):
        self.env.render()

    def close(self):
        self.env.close()

import math
import numpy as np

class MCTSNode:
    def __init__(self, state, parent=None):
        self.state = state
        self.parent = parent
        self.children = []
        self.visits = 0
        self.value = 0.0

    def is_leaf(self):
        return len(self.children) == 0

    def expand(self, game, valid_actions):
        for action in valid_actions:
            next_state, reward, done = self.simulate_action(game, action)
            self.children.append((action, MCTSNode(next_state, parent=self), reward, done))

    def simulate_action(self, game, action):
        game.env.env.state = self.state
        state, reward, done, _ = game.step(action)
        return state, reward, done

    def best_child(self, c_param=1.4):
        if len(self.children) == 0:
            return None
        choices_weights = [
            (child[1].value / (child[1].visits + 1e-5)) + c_param * math.sqrt((2 * math.log(self.visits + 1) / (child[1].visits + 1e-5)))
            for child in self.children
        ]
        return self.children[np.argmax(choices_weights)][1]

    def update(self, reward):
        self.visits += 1
        self.value += reward

class MCTS:
    def __init__(self, num_simulations):
        self.num_simulations = num_simulations

    def search(self, root_state, game):
        root = MCTSNode(root_state)
        for _ in range(self.num_simulations):
            node = root
            # Selection
            while not node.is_leaf() and node.best_child() is not None:
                node = node.best_child()

            # Expansion
            if not self.is_terminal(node.state):
                node.expand(game, [0, 1])  # Blackjack 的动作是 0（保持）和 1（要牌）

            # Simulation
            reward = self.simulate(game, node.state)

            # Backpropagation
            self.backpropagate(node, reward)

        best_child_node = root.best_child(c_param=0)
        return best_child_node.state if best_child_node else root.state

    def simulate(self, game, state):
        game.env.env.state = state
        done = False
        total_reward = 0
        while not done:
            action = game.action_space.sample()
            state, reward, done, _ = game.step(action)
            total_reward += reward
        return total_reward

    def backpropagate(self, node, reward):
        while node is not None:
            node.update(reward)
            reward = -reward  # 反转奖励以模拟两玩家对抗
            node = node.parent

    def is_terminal(self, state):
        # 假设状态为 None 或者是 done 状态表示游戏结束。可以根据实际情况修改。
        return state is None or state[2]  # state[2] 是一个布尔值，表示是否游戏结束

def main():
    game = BlackjackEnv()
    mcts = MCTS(num_simulations=1000)
    episodes = 10

    for episode in range(episodes):
        state = game.reset()
        done = False
        total_reward = 0
        while not done:
            best_action_state = mcts.search(state, game)
            game.env.env.state = best_action_state
            action = game.action_space.sample()  # 选择最佳动作的占位符，可以改为对应动作
            state, reward, done, _ = game.step(action)
            total_reward += reward
            game.render()

        print(f"Episode {episode + 1}: Total Reward = {total_reward}")

    game.close()

if __name__ == "__main__":
    main()
```

### 12.6 基于人类反馈的强化学习

基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）是一种利用人类反馈来训练和优化智能体或模型的强化学习方法。RLHF通过直接引入人类的偏好、评价或指导，帮助智能体更有效地学习和执行任务。与传统的强化学习方法相比，RLHF能够更好地处理复杂任务、稀疏奖励环境以及需要安全性和伦理性保障的情景。

RLHF的向个关键概念

- 人类反馈：指人类通过各种形式（如偏好、评分、演示、指导等）提供给智能体的信息，用以指导智能体的行为和决策。
- 偏好模型：一个根据人类反馈数据训练的模型，用来预测智能体行为的优劣。这通常是一个监督学习模型，能够根据人类提供的偏好数据进行优化。
- 奖励函数优化：通过人类反馈来优化或设计奖励函数，使得智能体的行为更符合人类的期望和标准。
- 策略改进：智能体使用优化后的奖励函数或偏好模型，通过强化学习算法（如DQN、PPO等）不断改进其策略。

RLHF的工作流程为

- 收集人类反馈：从人类用户或专家处收集反馈数据。这些反馈可以是对智能体行为的偏好比较、评分、建议或演示。
- 训练偏好模型：使用收集到的反馈数据训练一个偏好模型，该模型能够预测不同行为的优劣。
- 优化奖励函数：根据偏好模型的输出，优化或设计智能体的奖励函数，使其更符合人类的期望。
- 强化学习训练：智能体使用优化后的奖励函数，通过强化学习算法进行训练，不断改进其策略。
- 迭代改进：通过不断收集新的反馈数据和更新偏好模型，迭代改进智能体的表现。


## 第十三章 在网页和手机里运行机器学习

### 13.1 TensorFlow.js的基本使用

Python确实在机器学习和深度学习领域有着不可替代的生态优势，不过，放到浏览器端和手机端，Python的生态优势好像就发挥不出来了。不管是Android手机还是iOS手机，默认都没有Python运行环境，也写不了Python应用。浏览器里和小程序里，就更没Python什么事儿了。

在浏览器里，可以直接使用TensorFlow.js库，尽管可能会有性能的问题，但是至少是从0到1的突破。

![](https://img-blog.csdnimg.cn/img_convert/5e72f38de1714c77a25374648884d534.png)

我们看个例子：
```html
<!DOCTYPE html>
<html>
    <head>
        <meta encoding="UTF-8"/>
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.6.0/dist/tf.min.js"></script>
    </head>
    <body>
        <div id="tf-display"></div>
        <script>
            let a = tf.tensor1d([1.0]);
            let d1 = document.getElementById("tf-display");
            d1.innerText = a;
        </script>
    </body>
</html>
```

可以看到，在浏览器里显示了一个值为1.0的张量的值。我们的第一个TensorFlow.js(以下简称tf.js)应用就算是跑通了。通过引用tf.js的库，我们就可以调用tf下面的函数。

下面我们修改一下，看看tf.js是靠什么技术在运行的。我们通过tf.getBackend()函数来查看支持tf.js

```html
<!DOCTYPE html>
<html>
    <head>
        <meta encoding="UTF-8"/>
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.6.0/dist/tf.min.js"></script>
    </head>
    <body>
        <div id="tf-display"></div>
        <div id="tf-backend"></div>
        <script>
            let a = tf.tensor1d([1.0,2.0,3.0]);
            let d1 = document.getElementById("tf-display");
            d1.innerText = a;

            let backend = tf.getBackend();
            let div_backend = document.getElementById("tf-backend");
            div_backend.innerText = backend;
        </script>
    </body>
</html>
```

在我的浏览器里，tf.js是使用webgl来进行计算的。

#### 14.1.1 运行在node里的tfjs

作为一个js库，tf.js当然也可以运行在node环境里。我们可以通过
```
npm install @tensorflow/tfjs
```
来安装tf.js库。

然后把上面网页里面的代码移值过来：
```js
const tf = require('@tensorflow/tfjs');

let a = tf.tensor1d([1.0,2.0,3.0]);
console.log(a);

console.log(tf.getBackend());
```

在我的电脑里执行，这个getBackend()返回的是'cpu'. 
tf.js还会给tfjs-node做个广告：
```
============================
Hi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.
============================
```

听人劝吃饱饭，那我们就换成tfjs-node吧：

```js
const tf = require('@tensorflow/tfjs-node');

let a = tf.tensor1d([1.0,2.0,3.0]);
console.log(a);

console.log(tf.getBackend());
```

记得要
```
npm install @tensorflow/tfjs-node
```

现在，后端从cpu换成了tensorflow。

还有更凶残的，我们还可以换成tfjs-node-gpu来使用GPU：
```js
const tf = require('@tensorflow/tfjs-node-gpu');

let a = tf.tensor1d([1.0,2.0,3.0]);
console.log(a);

console.log(tf.getBackend());
```
在没有GPU的机器上，会使用CPU版的tensorflow作为后端，不会报错。

#### 14.1.2 JavaScript的数组操作

js是一门动态语言，js的数组是动态数组，没有定长数组越界这一说法的。

比如说我们要给一个空数组的第2个元素赋值，这是没有任何问题的：
```javascript
let a1 = [];
a1[2] = 3;
console.log(a1);
```
输出结果为：
```
[ <2 empty items>, 3 ]
```

我们可以毫无压力地用这样的数组去生成张量：
```js
let a1_t = tf.tensor1d(a1);
a1_t.print();
```

tf.js会给我们甩出两个NaN出来：
```
Tensor
    [NaN, NaN, 3]
```

不但是空数组随便添加元素，我们用new Array生成一个长度的数组后，仍然可以说话不算话，随意给赋值。比如我们new 5个元素的Array，给第9个赋值：
```js
let a2 = new Array(5);
a2[9] = 10;
console.log(a2);


let a2_t = tf.tensor1d(a2);
a2_t.print();
```

tf.js照例给我们补9个NaN出来：
```
[ <9 empty items>, 10 ]
Tensor
    [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, 10]
```

如果懒得数一共多少个元素，就想在数组的末尾添加新元素，可以使用push方法，参数个数不限，push几个元素都可以：
```js
let a3 = new Array();
a3.push(1,2,3);
a3.push(4,5);

let a3_t = tf.tensor1d(a3);
a3_t.print();
```

输出为：
```
Tensor
    [1, 2, 3, 4, 5]
```

如果想从头添加新元素，可以使用unshift方法：
```js
let a3 = new Array();
a3.push(1,2,3);
a3.push(4,5);
a3.unshift(6);

let a3_t = tf.tensor1d(a3);
a3_t.print();
```

输出为：
```
Tensor
    [6, 1, 2, 3, 4, 5]
```

同时我们复习一下，与push相对的，删除最后一个元素的是pop方法；而与unshift相对的是shift方法。

比如我们对上面的a3进行pop：
```javascript
let a4 = a3;
let a00 = a3.pop();
console.log(a00);
console.log(a4);
```

所得结果为：
```
5
[ 6, 1, 2, 3, 4 ]
```

最后，我们还有强大的splice方法，可以在任意位置添加与删除。

splice方法的第一个参数是起始位置，第二个参数是要删除的个数。
我们来看个例子，我们先生成10个元素的数组，然后把前5个空元素都删掉：

```js
let a5 = []
a5.length = 10;
a5[5] = 100;
console.log(a5);
a5.splice(0,5);
console.log(a5);
```

输出结果为：
```
[ <5 empty items>, 100, <4 empty items> ]
[ 100, <4 empty items> ]
```

如果不删除，想要添加元素的话，我们可以给第二个参数置0，然后后面是要添加的元素。比如我们给上面的a5在100后面增加三个新元素1.5, 2.5, 3.5：

```js
a5.splice(1,0,1.5,2.5,3.5);
console.log(a5);
```

输出如下：

```
[ 100, 1.5, 2.5, 3.5, <4 empty items> ]
```

记住是要给元素值，而不是给个数组啊，否则的话就变成二维数组了：

```js
a5.splice(1,0,[1.5,2.5,3.5]);
console.log(a5);
```

结果为：
```
[ 100, [ 1.5, 2.5, 3.5 ], 1.5, 2.5, 3.5, <4 empty items> ]
```

好，复习至此，我们来看tf.js中的张量

#### 14.1.3 tf.js中的张量

![](https://img-blog.csdnimg.cn/img_convert/44330be11dcbd3bb90158082c62d1345.png)

##### 14.1.3.1 一维张量

tfjs支持从1d到6d一共6维张量构造函数，当然7维以上没有专用函数了还是可以reshape出来。

最简单的张量是一维的，我们可以用tf.tensor1d：
```js
let t1d = tf.tensor1d([1, 2, 3]);
t1d.print();
```

输出为：
```
Tensor
    [1, 2, 3]
```

当然，还可以指定数据类型：
```js
const t1d_f = tf.tensor1d([1.0,2.0,3.0],'float32')
t1d_f.print();
```

输出结果为：
```
Tensor
    [1, 2, 3]
```

数据类型可用值为：
- 'float32'
- 'int32'
- 'bool'
- 'complex64'
- 'string'

可以通过linspace函数来生成一维序列，其原型为：
```js
tf.linspace (start, stop, num)
```
其中
- start为起始值
- end为结束值
- num为生成的序列的元素个数

例： 
```js
tf.linspace(1, 10, 10).print();
```

输出结果为：
```
Tensor
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
```

如果想用指定步长的方式来生成，可以使用range函数：
```
tf.range(start, stop, step?, dtype?)
```

我们来看个例子：
```js
tf.range(0, 9, 2).print();
```

输出结果为：
```
Tensor
    [0, 2, 4, 6, 8]
```

##### 14.1.3.2 二维张量

![](https://img-blog.csdnimg.cn/img_convert/4365deb8f649e8abd84166268ae61863.png)

二维张量可以用二维数组来定义：
```js
let t2d = tf.tensor2d([[0,0],[0,1]]);
t2d.print();
```

不过tf.js的二维张量必须是矩阵，而js的二维数组是可以不等长的，这点尤其要注意。

因为二维张量主要用于存放矩阵，有生成矩阵的方法可供调用。

比如我们可以使用tf.eye来生成单位矩阵：
```js
const t_eye = tf.eye(4);
t_eye.print();
```
 
输出结果为：
```
Tensor
    [[1, 0, 0, 0],
     [0, 1, 0, 0],
     [0, 0, 1, 0],
     [0, 0, 0, 1]]
```

我们也可以将一维向量转化为以其为对角向量的二维向量：
```js
const x1 = tf.tensor1d([1, 2, 3, 4, 5, 6, 7, 8]);
tf.diag(x1).print();
```

输出结果为：
```
Tensor
    [[1, 0, 0, 0, 0, 0, 0, 0],
     [0, 2, 0, 0, 0, 0, 0, 0],
     [0, 0, 3, 0, 0, 0, 0, 0],
     [0, 0, 0, 4, 0, 0, 0, 0],
     [0, 0, 0, 0, 5, 0, 0, 0],
     [0, 0, 0, 0, 0, 6, 0, 0],
     [0, 0, 0, 0, 0, 0, 7, 0],
     [0, 0, 0, 0, 0, 0, 0, 8]]
```

从二维张量开始，我们可以指定张量的形状了。

比如我们用一维数组给定值，然后指定[2,2]的形状：
```js
let t2d2 = tf.tensor2d([1,2,3,4],[2,2],'float32');
t2d2.print();
```

输出结果如下：
```
Tensor
    [[1, 2],
     [3, 4]]
```

##### 14.1.3.4 高维向量

![](https://img-blog.csdnimg.cn/img_convert/c3848ff76853c32941b21bf53df27160.png)

从三维开始，用高维数组来表示张量值的可读性就越来越差了。比如：
```js
tf.tensor3d([[[1], [2]], [[3], [4]]]).print();
```

输出结果为：
```
Tensor
    [[[1],
      [2]],

     [[3],
      [4]]]
```

我们可以还是先指定一维数组，然后再指定形状：
```js
tf.tensor3d([1,2,3,4,5,6,7,8],[2,2,2],'int32').print();
```

输出如下：
```
Tensor
    [[[1, 2],
      [3, 4]],

     [[5, 6],
      [7, 8]]]
```

我们向4，5，6维挺进：
```js
tf.tensor4d([[[[1], [2]], [[3], [4]]]]).print();
tf.tensor5d([[[[[1],[2]],[[3],[4]]],[[[5],[6]],[[7],[8]]]]]).print();
tf.tensor6d([[[[[[1],[2]],[[3],[4]]],[[[5],[6]],[[7],[8]]]]]]).print();
```

输出如下：
```
Tensor
    [[[[1],
       [2]],

      [[3],
       [4]]]]
Tensor
    [[[[[1],
        [2]],

       [[3],
        [4]]],


      [[[5],
        [6]],

       [[7],
        [8]]]]]
Tensor
    [[[[[[1],
         [2]],

        [[3],
         [4]]],


       [[[5],
         [6]],

        [[7],
         [8]]]]]]
```

此时，指定形状的优势就更加明显了。

我们可以用tf.zeros函数生成全是0的任意维的张量：
```js
tf.zeros([2,2,2,2,2,2]).print();
```

也可以通过tf.ones将所有值置为1:
```js
tf.ones([3,3,3]).print();
```

还可以通过tf.fill函数生成为指定值的张量：
```js
tf.fill([4,4,4],255).print();
```

比起序列值和固定值，生成符合正态分布的随机值可能是更常用的场景。其原型为：
```js
tf.truncatedNormal(shape, mean?, stdDev?, dtype?, seed?)
```
其中：
- shape是张量形状
- mean是平均值
- stdDev是标准差
- dtype是数据类型，整形和浮点形在此差别可能很大
- seed是随机数种子

我们看个例子：
```js
tf.truncatedNormal([3,3,3],1,1,"float32",123).print();
tf.truncatedNormal([2,2,2],1,1,"int32",99).print();
```

输出如下：
```
Tensor
    [[[0.9669023 , 0.2715541 , 0.6810297 ],
      [-0.8329115, -0.7022814, 1.4331075 ],
      [1.8136243 , 1.8001028 , -0.3285823]],

     [[1.381816  , 1.1050107 , 0.7487067 ],
      [1.9785664 , 0.9248876 , -0.9470147],
      [0.0489896 , 0.3297685 , 0.8626058 ]],

     [[0.3341007 , 1.1067212 , 0.4879217 ],
      [2.1620302 , 1.3034405 , 0.2832415 ],
      [1.3012471 , 1.0853187 , 1.9235317 ]]]
Tensor
    [[[0, 1],
      [1, 0]],

     [[0, 0],
      [1, 2]]]
```

####  14.1.4 将张量转换成js数组

![](https://img-blog.csdnimg.cn/img_convert/740e72498f836a96ad4321dc9e6d3fe1.png)

前面我们学习了很多种张量的生成方法。但是，不知道你意识到了没有，很多时候还是转回到js数组更容易进行一些高阶的操作。

将张量转换成为数组有两种方式，一种是按照原形状转换成数组。异步的可以使用Tensor.array()方法，同步的可以使用Tensor.arraySync()方法。

我们来将上节生成的随机数的向量转回成js的数组：
```js
let t7 = tf.truncatedNormal([2,2,2],1,1,"int32",99);
let a7 = t7.arraySync();
console.log(a7);
```

输出结果为：
```
[ [ [ 0, 1 ], [ 1, 0 ] ], [ [ 0, 0 ], [ 1, 2 ] ] ]
```

记得这是一个高维数组啊，每个元素都是数组。
比如：
```js
a7.forEach(
    (x) => { console.log(x);}
);
```

输出将是两个数组元素：
```
[ [ 0, 1 ], [ 1, 0 ] ]
[ [ 0, 0 ], [ 1, 2 ] ]
```

如果不想要形状，可以用data()或者dataSync()方法将张量转换成TypedArray.

```js
let t5 = tf.truncatedNormal([2,2,2],1,1,"int32",99);
let a5 = t5.dataSync();
console.log(a5);
```

输出结果如下：
```
Int32Array(8) [
  0, 1, 1, 0,
  0, 0, 1, 2
]
```

如果对TypedArray进行forEach操作：
```js
a5.forEach(
    (x) => { console.log(x);}
);
```
获取的结果就是线性的了：
```
0
1
1
0
0
0
1
2
```

拍平成一维的之后，我们就可以用every和some等来进行元素的判断了。
比如我们看a5是不是所有元素都是0，是不是有元素为0：
```js
console.log(a5.every((x) => { return(x===0)}));
console.log(a5.some((x) => { return(x===0)}));
```

因为不全为0，所以every的值为假，而some为真。

### 14.2 用TensorFlow.js进行机器学习编程

#### 14.2.1 用TensorFlow.js处理鸢尾花

温故而知新，我们学习用Tensorflow.js的第一步还是从鸢尾花开始。

首先我们加载鸢尾花的数据：

```javascript
async function loadIrisData() {
  const response = await fetch('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data');
  const data = await response.text();
  
  const parsedData = data.trim().split('\n').map(line => {
    const [sepalLength, sepalWidth, petalLength, petalWidth, species] = line.split(',');
    const speciesMap = { 'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2 };
    return [parseFloat(sepalLength), parseFloat(sepalWidth), parseFloat(petalLength), parseFloat(petalWidth), speciesMap[species]];
  });

  const xs = parsedData.map(row => row.slice(0, 4));
  const ys = parsedData.map(row => row[4]);

  return { xs, ys };
}
```

TensorFlow.js没有提供数据集，我们需要自己加载数据集。这里我们加载了鸢尾花数据集，然后将数据集分成特征和标签。
我们使用 fetch API 发起一个网络请求，获取鸢尾花数据集文件。await 关键字用于等待异步操作（即网络请求）完成，并获取响应对象。
然后我们将响应对象转换为文本格式，即获取到数据集的内容。
接着我们对数据进行预处理：首先使用 trim() 方法去除字符串两端的空白字符，然后使用 split('\n') 将数据分割成单独的行。接着，使用 map 方法遍历每一行数据，将每行数据按逗号分隔，得到五个字段：萼片长度、萼片宽度、花瓣长度、花瓣宽度和物种名称。

下一步我们需要将数据转换成TensorFlow.js中的张量：

```javascript
async function prepareData() {
  const { xs, ys } = await loadIrisData();

  const inputTensor = tf.tensor2d(xs);
  const labelTensor = tf.tensor1d(ys, 'int32');

  const oneHotLabels = tf.oneHot(labelTensor, 3);

  return { inputTensor, oneHotLabels };
}
```

我们使用 tf.tensor2d() 创建一个二维张量，用于存储特征数据。我们使用 tf.tensor1d() 创建一个一维张量，用于存储标签数据。然后我们使用 tf.oneHot() 方法将标签数据转换为独热编码。

这里借机会讲一下独热编码。独热编码（One-Hot Encoding）是一种常用的将类别型数据转换为数值型数据的编码方式。在机器学习和数据处理过程中，独热编码可以将分类变量（Categorical Variables）转换为可以直接用于模型训练的数值格式。在处理分类数据时，直接使用类别标签（如“红色”、“蓝色”、“绿色”）会让模型误认为这些类别之间存在大小或顺序关系。独热编码通过将每个类别转换为一个二进制向量，可以避免这种误解。

数据就绪，我们来创建模型：

```javascript
function createModel() {
  const model = tf.sequential();
  model.add(tf.layers.dense({ inputShape: [4], units: 10, activation: 'relu' }));
  model.add(tf.layers.dense({ units: 3, activation: 'softmax' }));
  return model;
}
```

我们使用 tf.sequential() 创建一个序贯模型。然后我们使用 model.add() 方法添加两个全连接层。第一个全连接层有 10 个神经元，激活函数为 relu。第二个全连接层有 3 个神经元，激活函数为 softmax。

下面我们来写训练的部分：

```javascript
async function trainModel(model, inputs, labels) {
  model.compile({
    optimizer: tf.train.adam(),
    loss: 'categoricalCrossentropy',
    metrics: ['accuracy']
  });

  const batchSize = 32;
  const epochs = 50;

  return await model.fit(inputs, labels, {
    batchSize,
    epochs,
    shuffle: true,
    callbacks: {
      onEpochEnd: (epoch, logs) => {
        console.log(`Epoch ${epoch + 1}: loss = ${logs.loss.toFixed(4)}, accuracy = ${logs.acc.toFixed(4)}`);
      }
    }
  });
}
```

我们使用 model.compile() 方法配置模型, 使用 tf.train.adam() 作为优化器，使用 categoricalCrossentropy 作为损失函数，使用 accuracy 作为评估指标。然后我们使用 model.fit() 方法训练模型。我们设置 batchSize 为 32，epochs 为 50。我们使用 shuffle: true 来打乱数据集。最后，我们使用 callbacks 参数来设置回调函数，当每个 epoch 结束时，我们输出损失和准确率。

最后我们写一个函数将上面的过程串联起来：

```javascript
async function run() {
  const { inputTensor, oneHotLabels } = await prepareData();

  const model = createModel();
  await trainModel(model, inputTensor, oneHotLabels);

  // 预测示例数据
  const testData = tf.tensor2d([[5.1, 3.5, 1.4, 0.2]]);
  const prediction = model.predict(testData);
  const predictedClass = prediction.argMax(-1).dataSync()[0];

  console.log(`Predicted class: ${predictedClass}`); // 0: Iris-setosa, 1: Iris-versicolor, 2: Iris-virginica
}

run();
```

运行结果如下，中间的过程为了简洁我们省略了大部分：

```
Epoch 1 / 50
eta=0.0  acc=0...eta=0.0  
240ms 1599us/step - acc=0.333 loss=1.32 
Epoch 1: loss = 1.3157, accuracy = 0.3333
Epoch 2 / 50
eta=0.0  acc=0...eta=0.0  
141ms 938us/step - acc=0.353 loss=1.26 
Epoch 2: loss = 1.2607, accuracy = 0.3533
Epoch 3 / 50
eta=0.0  acc=0...eta=0.0  
120ms 800us/step - acc=0.493 loss=1.21 
Epoch 3: loss = 1.2096, accuracy = 0.4933
...
Epoch 49 / 50
eta=0.0  acc=0....eta=0.0  
47ms 315us/step - acc=0.900 loss=0.540 
Epoch 49: loss = 0.5401, accuracy = 0.9000
Epoch 50 / 50
eta=0.0  acc=0....eta=0.0  
59ms 391us/step - acc=0.907 loss=0.535 
Epoch 50: loss = 0.5348, accuracy = 0.9067
Predicted class: 0
```

其中预测结果这个代码我们来讲一下，初学的读者可能不容易理解。

```javascript
const predictedClass = prediction.argMax(-1).dataSync()[0];
```

从预测结果中找到最大值的索引，这代表了模型预测的类别。argMax(-1) 表示在最后一个维度上找到最大值的索引，dataSync() 方法将张量转换为 JavaScript 数组，[0] 表示取数组的第一个元素。

小知识：为什么预测结果中最大值的索引代表了模型预测的类别？

在机器学习中，特别是在分类任务中，模型的输出通常是各个类别的概率分布。这意味着模型会为每个可能的类别输出一个概率值，表示该样本属于该类别的置信度。当使用 softmax 激活函数或类似的机制时，所有类别的概率之和等于1。

当我们想要从这样的概率分布中得到一个具体的类别预测时，我们需要选择一个类别。一种常见的方法是选择具有最高概率的类别，因为在概率最高的类别上，模型对其预测最为自信。这就是为什么我们会取概率分布中的最大值所对应的索引作为预测的类别。

具体来说，如果我们有一个由模型输出的概率分布数组，例如 [0.1, 0.3, 0.6]，这个数组表示模型认为样本属于第一个类别的概率是10%，第二个类别的概率是30%，第三个类别的概率是60%。在这个例子中，第三个类别的概率最高，因此我们取索引为2的类别作为模型的预测结果。

#### 14.2.2 用TensorFlow.js实现朴素贝叶斯算法

离开了Scikit-learn的封装，我们不能失去了机器学习的编程能力。比如TensorFlow.js并没有提供朴素贝叶斯算法，我们可以自己实现。

注释我直接写在代码里了：

```javascript
function calculateProbabilities(xs, ys) {
  const numClasses = 3; // 鸢尾花数据集有3个类别
  const numFeatures = xs[0].length; // 特征的数量（在鸢尾花数据集中是4个）

  // 初始化各种统计量
  const classCounts = new Array(numClasses).fill(0); // 用于存储每个类别的样本数量
  const featureSums = Array.from({ length: numClasses }, () => new Array(numFeatures).fill(0)); // 用于存储每个类别中每个特征的和
  const featureSquares = Array.from({ length: numClasses }, () => new Array(numFeatures).fill(0)); // 用于存储每个类别中每个特征的平方和

  // 遍历每个样本，更新统计量
  ys.forEach((label, i) => {
    classCounts[label]++; // 增加该类别的样本计数
    xs[i].forEach((value, j) => {
      featureSums[label][j] += value; // 增加该类别中该特征的值
      featureSquares[label][j] += value * value; // 增加该类别中该特征的平方值
    });
  });

  // 计算先验概率
  const priors = classCounts.map(count => count / ys.length); // 每个类别的样本数量除以总样本数量，得到先验概率

  // 计算均值
  const means = featureSums.map((sums, c) => sums.map(sum => sum / classCounts[c])); // 每个类别中每个特征的和除以该类别的样本数量，得到均值

  // 计算方差
  const variances = featureSquares.map((squares, c) =>
    squares.map((square, j) => square / classCounts[c] - means[c][j] ** 2) // 每个类别中每个特征的平方和除以该类别的样本数量，再减去该特征均值的平方，得到方差
  );

  return { priors, means, variances }; // 返回先验概率、均值和方差
}
```

我们再总结下上面代码的步骤：

- 初始化统计量：
    - classCounts：用于存储每个类别的样本数量。
    - featureSums 和 featureSquares：分别用于存储每个类别中每个特征的和和平方和。
- 更新统计量：
    - 使用 ys.forEach 遍历每个样本的标签 label 和索引 i。
    - 对于每个样本，增加该类别的样本计数，并更新对应特征的和和平方和。
- 计算先验概率：先验概率表示每个类别在数据集中出现的频率，即 classCounts 中每个类别的样本数量除以总样本数量。
- 计算均值：均值是每个类别中每个特征的平均值，即 featureSums 中每个类别和特征的和除以该类别的样本数量。
- 计算方差：方差是每个类别中每个特征的离散程度，即 featureSquares 中每个类别和特征的平方和除以该类别的样本数量，再减去该特征均值的平方。

下面我们来写预测的函数：

```javascript
// 预测函数
function predict(xs, priors, means, variances) {
  const numClasses = priors.length; // 类别的数量
  const numFeatures = xs[0].length; // 特征的数量
  const predictions = []; // 存储预测结果

  xs.forEach(x => {
    // 计算每个类别的概率
    const probabilities = priors.map((prior, c) => {
      let probability = Math.log(prior); // 先验概率的对数
      for (let j = 0; j < numFeatures; j++) {
        const mean = means[c][j]; // 第 c 类别的第 j 个特征的均值
        const variance = variances[c][j]; // 第 c 类别的第 j 个特征的方差
        const value = x[j]; // 当前样本的第 j 个特征值
        probability += -0.5 * Math.log(2 * Math.PI * variance) - (value - mean) ** 2 / (2 * variance);
      }
      return probability;
    });

    // 找出概率最大的类别
    const predictedClass = probabilities.indexOf(Math.max(...probabilities));
    predictions.push(predictedClass);
  });

  return predictions; // 返回所有样本的预测结果
}
```

我们再梳理下预测部分的步骤：

1. **初始化变量**：
   - `numClasses`：类别的数量，从 `priors` 的长度得到。
   - `numFeatures`：特征的数量，从 `xs` 中任意一个样本的长度得到。
   - `predictions`：用于存储对每个样本的预测结果。

2. **遍历每个样本**：
   - 使用 `xs.forEach` 遍历每个样本 `x`。

3. **计算每个类别的概率**：
   - 对于每个类别 `c`，初始化 `probability` 为该类别的先验概率的对数 `Math.log(prior)`。
   - 遍历该样本的每个特征 `j`，根据朴素贝叶斯的公式，计算该类别的条件概率：
     - `Math.log(2 * Math.PI * variance)`：方差的对数部分。
     - `(value - mean) ** 2 / (2 * variance)`：特征值与均值的差平方除以方差的部分。
   - 将对数概率累加到 `probability` 中。

4. **选择最大概率的类别**：
   - 使用 `Math.max(...probabilities)` 找出所有类别中概率最大值。
   - 使用 `probabilities.indexOf` 找出该最大值对应的类别索引 `predictedClass`。

5. **存储预测结果**：
   - 将 `predictedClass` 添加到 `predictions` 数组中。

6. **返回预测结果**：
   - 最终返回 `predictions` 数组，它包含了对所有输入样本的预测类别。


原理部分如果忘记的话我们简单回顾一下：

朴素贝叶斯分类器基于贝叶斯定理，其假设特征之间是条件独立的。给定一个样本 $x$，其属于类别 $c$ 的概率 $P(c|x)$ 可以表示为：

$P(c|x) \propto P(c) \prod_{j=1}^{n} P(x_j|c)$

其中 $P(c)$ 是先验概率，$P(x_j|c)$ 是在类别 $c$ 下特征 $x_j$ 的条件概率。由于概率计算中的乘法容易导致数值下溢，所以在代码中使用对数形式：

$\log P(c|x) = \log P(c) + \sum_{j=1}^{n} \log P(x_j|c)$

对于正态分布，条件概率 $P(x_j|c)$ 的对数形式为：

$\log P(x_j|c) = -0.5 \log(2 \pi \sigma^2) - \frac{(x_j - \mu)^2}{2 \sigma^2}$

其中 $\mu$ 和 $\sigma^2$ 分别是特征的均值和方差。

小知识：数值下溢

数值下溢（Numerical Underflow）是指在计算机进行浮点数运算时，结果小于计算机所能表示的最小正数，从而导致结果被近似为零或被舍入为零的现象。计算机在处理非常小的浮点数时，可能会遇到这种情况。

对数函数（尤其是自然对数）可以将乘法转换为加法。例如，概率值通常是介于0和1之间的非常小的数，直接相乘会导致数值下溢。而对数转换后，这些小数变成了负数（对数结果），这些负数相加后，结果仍然在计算机可以处理的范围内。

在贝叶斯分类器中，我们需要计算多个特征值的联合概率。这通常会涉及将许多小概率值相乘。如果不使用对数，这些小数的乘积会迅速变得非常接近零，甚至超出计算机的表示能力。而使用对数后，相乘的操作变为相加，从而有效避免了这些极小数相乘的问题。

### 14.3 其它JavaScript机器学习库

TensorFlow.js并不是唯一的JavaScript机器学习库。在 JavaScript 生态系统中，还有很多其他优积的机器学习库,不是所有算法都要自己手写,比如我们可以用ml-cart来做决策树.

可以这样写：
```javascript
const fetch = require('node-fetch');
const { DecisionTreeClassifier } = require('ml-cart');

async function loadIrisData() {
  const response = await fetch('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data');
  const data = await response.text();
  
  const parsedData = data.trim().split('\n').map(line => {
    const [sepalLength, sepalWidth, petalLength, petalWidth, species] = line.split(',');
    const speciesMap = { 'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2 };
    return [parseFloat(sepalLength), parseFloat(sepalWidth), parseFloat(petalLength), parseFloat(petalWidth), speciesMap[species]];
  });

  const xs = parsedData.map(row => row.slice(0, 4));
  const ys = parsedData.map(row => row[4]);

  return { xs, ys };
}

async function run() {
  const { xs, ys } = await loadIrisData();

  // 创建决策树分类器
  const decisionTree = new DecisionTreeClassifier();

  // 训练模型
  decisionTree.train(xs, ys);

  // 进行预测
  const testSample = [5.1, 3.5, 1.4, 0.2];  // 一个示例数据
  const prediction = decisionTree.predict([testSample]);

  console.log(`Predicted class: ${prediction}`); // 输出预测类别
}

run();
```

我们可以发现，就像使用了Scikit-learn一样，我们可以使用ml-cart的DecisionTreeClassifier来进行决策树分类。

## 第十四章 机器学习的算力技术

机器学习有三大要素：数据、算法和算力。前面我们花了很多时间讲数据和算法，这一章就来介绍一下算力技术。

2012年，Alex Krizhevsky等人在ImageNet比赛中使用了GPU来训练深度神经网络，取得了惊人的成绩。从那时起，GPU和TPU等硬件加速技术就成为了机器学习的标配。

时至今日，虽然多任务编程早已经深入人心，但是很多同学还没有接触过CPU上的SIMD指令，更不用说GPGPU的编程。本章我们就来介绍一下算力相关的技术。

未来大家可能遇到的问题千变万化，在使用封装的库不够用的时候，或者是要移植到其它平台的时候，就需要用到这些技术了。

### 14.1 CPU上的多任务编程

![Flow](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/CPU_flow.png)

其实，CPU上的多任务编程是被摩尔定律失效逼迫出来的。

摩尔定律是由英特尔联合创始人戈登·摩尔在1965年提出的一条经验法则。摩尔定律预测，集成电路上可容纳的晶体管数量每两年将增加一倍，而成本则减少一半。这一规律推动了半导体工业的快速发展，带来了计算能力的指数级增长。然而，随着技术的发展和物理极限的逼近，摩尔定律逐渐面临失效的挑战。

随着晶体管尺寸的不断缩小，已经接近物理极限。当前，晶体管的尺寸已经缩小到纳米级别（少于10纳米），这带来了诸多不可忽视的物理问题：

- 量子效应：在极小的尺度下，量子效应变得显著，导致电子行为难以预测和控制。
- 隧道效应：电流可能会通过绝缘层隧穿，从而产生泄漏电流，增加能耗和热量。
- 热管理：更高的晶体管密度会产生更多的热量，难以有效散热。

此外，制造更小、更密集的晶体管变得越来越复杂和昂贵。因此，摩尔定律逐渐失效，传统的单核CPU已经无法满足日益增长的计算需求。

#### 8.1.1 从多线程说起

曾经的编程语言是不支持多线程的，需要操作系统和库来提供多线程能力，比如pthread库。时至今日，默认不支持多线程的平台还是有的，比如wasm。
1995年问世的Java语言从1.0开始就支持多线程，虽然一直到5.0版本才对多线程有重大改进。C++语言从C++11开始语言支持多线程了。

我们来看一个用C++多线程来实现矩阵乘法的例子：

```cpp
#include <mutex>
#include <thread>

        // 矩阵维度
        const int width = 4;

        // 矩阵
        int A[width][width] = {
            {1, 2, 3, 4},
            {5, 6, 7, 8},
            {9, 10, 11, 12},
            {13, 14, 15, 16}
        };
        int B[width][width] = {
            {1, 0, 0, 0},
            {0, 1, 0, 0},
            {0, 0, 1, 0},
            {0, 0, 0, 1}
        };
        int C[width][width] = {0};

        // 互斥锁
        std::mutex mtx;

        // 计算线程
        void calculate(int row) {
            for (int col = 0; col < width; col++) {
                if (row < width && col < width) {
                    mtx.lock();
                    C[row][col] = A[row][col] + B[row][col];
                    mtx.unlock();
                }
            }
        }

        int main() {
            // 创建线程
            std::thread t1(calculate, 0);
            std::thread t2(calculate, 1);
            std::thread t3(calculate, 2);
            std::thread t4(calculate, 3);

            // 等待线程结束
            t1.join();
            t2.join();
            t3.join();
            t4.join();

            // 打印结果
            for (int i = 0; i < width; i++) {
                for (int j = 0; j < width; j++) {
                    printf("%d ", C[i][j]);
                }
                printf("\n");
            }
        }
```

我们给它配上一个CMakeLists.txt:
```cmake
cmake_minimum_required(VERSION 3.10)

# Set the project name
project(MatrixAddO)

# Set the C++ standard
set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED True)

# Add the executable
add_executable(matrix_add matadd.cpp)
```

这个代码大家应该都比较熟悉，就不多解释了。现在支持C++11以上已经是标配了。

#### 8.1.2 OpenMP

早在线程写进C++11标准之前，就有很多并发编程的框架了，比如MPI和OpenMP.

OpenMP是一套支持跨平台共享内存方式的多线程并发的编程API，使用C, C++和Fortran语言，可以在多种处理器体系和操作系统中运行。它由OpenMP Architecture Review Board (ARB)牵头提出，并由多家计算机硬件和软件厂商共同定义和管理。OpenMP提供了一组编译指示、库例程和环境变量，使开发者可以在现有的串行代码中插入并行化指令，从而简化并行程序的开发。

OpenMP最早是1997年发布的，当时只支持Fortran语言。1998年开始支持C/C++. 

![fork join](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/fork_join.gif)


我们来看看用OpenMP如何实现矩阵的并发计算：

```cpp
#include <iostream>
#include <omp.h>
#include <vector>

std::vector<std::vector<int>>
matrixAdd(const std::vector<std::vector<int>> &A,
          const std::vector<std::vector<int>> &B) {
  int rows = A.size();
  int cols = A[0].size();

  std::vector<std::vector<int>> C(rows, std::vector<int>(cols));

#pragma omp parallel for collapse(2)
  for (int i = 0; i < rows; i++) {
    for (int j = 0; j < cols; j++) {
      C[i][j] = A[i][j] + B[i][j];
    }
  }

  return C;
}

int main() {
  std::vector<std::vector<int>> A = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};

  std::vector<std::vector<int>> B = {{9, 8, 7}, {6, 5, 4}, {3, 2, 1}};

  std::vector<std::vector<int>> C = matrixAdd(A, B);

  for (const auto &row : C) {
    for (int val : row) {
      std::cout << val << " ";
    }
    std::cout << std::endl;
  }

  return 0;
}                                 
```

`#pragma omp parallel for collapse(2)` 是一个 OpenMP 编译指令，用于表示一个并行区域，其中嵌套的循环将并行执行。让我们详细解释这个指令的各个部分：

`#pragma omp`：这是一个编译指令，表示接下来的代码将使用 OpenMP 进行并行化。

`parallel for`：这是一个组合指令，表示接下来的 for 循环将在多个线程上并行执行。每个线程将处理循环的一部分，从而加速整个循环的执行。

`collapse(2)`：这是一个可选子句，用于指示嵌套循环的并行化。在这个例子中，collapse(2) 表示将两层嵌套的循环（即外层和内层循环）合并为一个并行循环。这样可以更好地利用多核处理器的性能，因为并行度增加了。

在我们的矩阵加法示例中，`#pragma omp parallel for collapse(2)` 指令应用于两个嵌套的 for 循环，它们分别遍历矩阵的行和列。使用此指令，这两个循环将合并为一个并行循环，从而在多核处理器上实现更高的性能。

需要注意的是，为了在程序中使用 OpenMP，你需要使用支持 OpenMP 的编译器（如 GCC 或 Clang），并在编译时启用 OpenMP 支持（如在 GCC 中使用 -fopenmp 标志）。

我们来写个支持OpenMP的CMakeLists.txt:
```cmake
cmake_minimum_required(VERSION 3.10)

# Set the project name
project(MatrixAddOpenMP)

# Set the C++ standard
set(CMAKE_CXX_STANDARD 11)
set(CMAKE_CXX_STANDARD_REQUIRED True)

# Find OpenMP
find_package(OpenMP REQUIRED)

# Add the executable
add_executable(matrix_add main.cpp)

# Link OpenMP to the executable
if(OpenMP_CXX_FOUND)
    target_link_libraries(matrix_add PUBLIC OpenMP::OpenMP_CXX)
endif()
```

可见，用了OpenMP的for循环，就可以变串行为并行。从而大大简化并行编程的难度。

#### 8.1.3 SIMD

![SIMD](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/simd.png)

虽然多线程和OpenMP看起来都不错，都容易编程，但是，我们的优化并不是以简化编程为目的的。

虽然我们抱怨Intel是牙膏厂，每年的进步越来越有限。不过，还总是有新的指令增加到新的架构中来。这其中就有越来越强大的SIMD指令。

SIMD就是一条机器指令可以实现多条数据的操作。

Intel CPU的SIMD发展史如下：

- MMX（MultiMedia eXtensions）:
    - 推出时间：1997年
    - 主要特点：使用64位的MM0 ~ MM7寄存器，能一次性操作1个64位的数据、或者两个32位的数据、或者4个16位的数据、或者8个8位的数据。
- SSE（Streaming SIMD Extensions）:
    - 推出时间：1999年
    - 主要特点：引入了独立的XMM0 ~ XMM7寄存器组，大小为128位。新增了用于处理整型和浮点型数据的指令，能同时处理多个数据。
- SSE2（Streaming SIMD Extensions 2）:
    - 推出时间：2000年
    - 主要特点：允许128位的XMM寄存器组存储整型数据，扩展了浮点数据的处理指令，支持64位的Double-Precision浮点数。
- SSE3（Streaming SIMD Extensions 3）:
    - 推出时间：2004年
    - 主要特点：增加了13条新指令，包括浮点水平算术运算和水平复制移动等。
- SSE4（Streaming SIMD Extensions 4）:
    - 推出时间：2006年
    - 主要特点：分为SSE4.1、SSE4.2和SSE4a三个子集，用于提升音视频、图像、字符串等方面的数据处理性能。
- AVX（Advanced Vector eXtentions）:
    - 推出时间：2008年
    - 主要特点：对XMM寄存器做了扩展，从128位扩展到256位，提供更灵活的指令集。


我们来个例子看看：

```cpp
#include <iostream>
#include <immintrin.h> // 包含 AVX 指令集头文件

void matrix_addition_avx(float* A, float* B, float* C, int size) {
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j += 8) { // 每次处理 8 个元素（AVX 可以处理 256 位，即 8 个单精度浮点数）
            __m256 vecA = _mm256_loadu_ps(&A[i * size + j]);
            __m256 vecB = _mm256_loadu_ps(&B[i * size + j]);
            __m256 vecC = _mm256_add_ps(vecA, vecB);
            _mm256_storeu_ps(&C[i * size + j], vecC);
        }
    }
}

int main() {
    int size = 8; // 假设矩阵大小为 8x8
    float A[64] = { /* ... */ }; // 初始化矩阵 A
    float B[64] = { /* ... */ }; // 初始化矩阵 B
    float C[64] = { 0 }; // 结果矩阵 C

    matrix_addition_avx(A, B, C, size);

    // 输出结果
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j++) {
            std::cout << C[i * size + j] << " ";
        }
        std::cout << std::endl;
    }

    return 0;
}
```

我们来解释一下使用SIMD的几条语句：

`__m256 vecA = _mm256_loadu_ps(&A[i * size + j])`：从矩阵 A 中加载 8 个浮点数（一次性处理 256 位数据），存储在一个名为 vecA 的 __m256 类型变量中。

`__m256 vecB = _mm256_loadu_ps(&B[i * size + j])`：同样地，从矩阵 B 中加载 8 个浮点数，存储在一个名为 vecB 的 __m256 类型变量中。

`__m256 vecC = _mm256_add_ps(vecA, vecB)`：使用 AVX 指令 _mm256_add_ps 对 vecA 和 vecB 中的浮点数分别进行逐元素加法，并将结果存储在名为 vecC 的 __m256 类型变量中。

`_mm256_storeu_ps(&C[i * size + j], vecC)`：将 vecC 中的 8 个加法结果存储回矩阵 C 的相应位置。

这段代码使用了 AVX 指令集，实现了对浮点矩阵的加法运算。请注意，为了充分利用 AVX 的并行处理能力，矩阵尺寸应该是 8 的倍数。如果矩阵尺寸不是 8 的倍数，需要添加额外的逻辑来处理剩余的元素。

后来，Intel又推出了AVX2指令集，不过对于我们上边的代码并没有太多优化，而主要优化是在整数方面。

![Intel SIMD](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/Intel-mmx-sse-sse2-avx-AVX-512.png)

上节我们学习的量化和解量化就用上了，我们这次使用AVX2提供的整数计算的加速来实现：

```cpp
#include <iostream>
#include <immintrin.h> // 包含 AVX2 指令集头文件

void matrix_addition_avx2_int(int *A, int *B, int *C, int size) {
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j += 8) { // 每次处理 8 个元素（AVX2 可以处理 256 位，即 8 个 int32 整数）
            __m256i vecA = _mm256_loadu_si256((__m256i *)&A[i * size + j]);
            __m256i vecB = _mm256_loadu_si256((__m256i *)&B[i * size + j]);
            __m256i vecC = _mm256_add_epi32(vecA, vecB);
            _mm256_storeu_si256((__m256i *)&C[i * size + j], vecC);
        }
    }
}

int main() {
    int size = 8; // 假设矩阵大小为 8x8
    int A[64] = { /* ... */ }; // 初始化矩阵 A
    int B[64] = { /* ... */ }; // 初始化矩阵 B
    int C[64] = {0}; // 结果矩阵 C

    matrix_addition_avx2_int(A, B, C, size);

    // 输出结果
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j++) {
            std::cout << C[i * size + j] << " ";
        }
        std::cout << std::endl;
    }

    return 0;
}
```

我们不惜折腾量化一把转换成整数的原因是，AVX中只有_mm_add_epi32指令，只能对两个128位整数向量的逐元素相加，而_mm256_add_epi32是256位，数据量加倍了。
不只是加法，AVX2 提供了一系列针对整数操作的新指令，例如乘法、位操作和打包/解包操作等。
AVX2指令的执行吞吐量(throughput)一般为1指令/周期,而AVX1为2指令/周期。所以在同频率下,AVX2的整数加法指令性能理论上可以提高一倍。
同时， 与其他AVX2指令结合使用，如_mm256_load_si256或_mm256_store_si256等，来从内存中加载或存储向量，这样可以提高内存访问的性能和带宽。

后来，Intel还推出了AVX512指令，基本上就把AVX1中的256换成512就可以了：

```cpp
#include <iostream>
#include <immintrin.h> // 包含 AVX-512 指令集头文件

void matrix_addition_avx512(float *A, float *B, float *C, int size) {
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j += 16) { // 每次处理 16 个元素（AVX-512 可以处理 512 位，即 16 个单精度浮点数）
            __m512 vecA = _mm512_loadu_ps(&A[i * size + j]);
            __m512 vecB = _mm512_loadu_ps(&B[i * size + j]);
            __m512 vecC = _mm512_add_ps(vecA, vecB);
            _mm512_storeu_ps(&C[i * size + j], vecC);
        }
    }
}

int main() {
    int size = 16; // 假设矩阵大小为 16x16
    float A[256] = { /* ... */ }; // 初始化矩阵 A
    float B[256] = { /* ... */ }; // 初始化矩阵 B
    float C[256] = {0}; // 结果矩阵 C

    matrix_addition_avx512(A, B, C, size);

    // 输出结果
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j++) {
            std::cout << C[i * size + j] << " ";
        }
        std::cout << std::endl;
    }

    return 0;
}
```

但是，优化并不总是一根筋地往上堆指令就可以的，AVX512是一种非常耗电的指令集，此时我们需要实测权衡一下。

![NEON](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/neon.png)

针对手机上用的ARM CPU，可以使用NEON指令来实现SIMD功能：

```cpp
#include <stdio.h>
#include <arm_neon.h>

void matrix_addition_neon(float *A, float *B, float *C, int size) {
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j += 4) { // 每次处理 4 个元素（NEON 可以处理 128 位，即 4 个单精度浮点数）
            float32x4_t vecA = vld1q_f32(&A[i * size + j]);
            float32x4_t vecB = vld1q_f32(&B[i * size + j]);
            float32x4_t vecC = vaddq_f32(vecA, vecB);
            vst1q_f32(&C[i * size + j], vecC);
        }
    }
}

int main() {
    int size = 4; // 假设矩阵大小为 4x4
    float A[16] = { /* ... */ }; // 初始化矩阵 A
    float B[16] = { /* ... */ }; // 初始化矩阵 B
    float C[16] = {0}; // 结果矩阵 C

    matrix_addition_neon(A, B, C, size);

    // 输出结果
    for (int i = 0; i < size; i++) {
        for (int j = 0; j < size; j++) {
            printf("%f ", C[i * size + j]);
        }
        printf("\n");
    }

    return 0;
}
```

对于初接触汇编级优化的同学，可能感觉很新鲜。不过，挑战更大的在后面，我们要进入GPU的世界了。

### 8.2 GPU上的多任务编程

欢迎来到异构计算的世界。之前我们的代码不管怎么写，都是在CPU上运行的。
从这一时刻开始，不管什么技术，我们都是由CPU和GPU两部分代码共同组合的了。

我们先从目前看仍然是主力的CUDA开始。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gpu-computing-applications.png)

#### 8.2.1 CUDA

CUDA 1.0于2007年发布。目前CUDA版本为12.1。

目前广泛适配的是CUDA 11.x，现在较新的版本为CUDA 11.8。因为CUDA 11.x才支持A100为代表的安培架构的GPU。3060，3070，3080，3090也是安培架构的GPU。

2080, 2060, 1660这一系列的是图灵架构，对应的是CUDA 10.x版本。

1060，1080这一系列对应的是帕斯卡架我，对应的是CUDA 8.0版本。

在CUDA中，运行在GPU上的代码我们叫做核函数。
我们先完整地看下这个代码，然后再解释。

```cpp
#include "cuda_runtime.h"
#include "device_launch_parameters.h"
#include <iostream>

// 矩阵加法的CUDA核函数
__global__ void matrixAdd10(int* A, int* B, int* C, int width) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < width && col < width) {
        C[row * width + col] = A[row * width + col] + B[row * width + col];
    }
}

int main() {
    // 矩阵维度
    int width = 4;

    // 分配CPU内存
    int* A, * B, * C;
    A = (int*)malloc(width * width * sizeof(int));
    B = (int*)malloc(width * width * sizeof(int));
    C = (int*)malloc(width * width * sizeof(int));

    // 初始化A和B矩阵
    for (int i = 0; i < width; i++) {
        for (int j = 0; j < width; j++) {
            A[i * width + j] = i;
            B[i * width + j] = j;
        }
    }

    // 为GPU矩阵分配内存
    int* d_A, * d_B, * d_C;
    cudaMalloc((void**)&d_A, width * width * sizeof(int));
    cudaMalloc((void**)&d_B, width * width * sizeof(int));
    cudaMalloc((void**)&d_C, width * width * sizeof(int));

    // 将矩阵从CPU内存复制到GPU内存
    cudaMemcpy(d_A, A, width * width * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, width * width * sizeof(int), cudaMemcpyHostToDevice);

    // 配置CUDA核函数参数
    dim3 threads(width, width);
    dim3 grid(1, 1);
    matrixAdd10 <<<grid, threads >>> (d_A, d_B, d_C, width);

    // 等待CUDA核函数执行完毕
    cudaDeviceSynchronize();

    // 将结果从GPU内存复制到CPU内存
    cudaMemcpy(C, d_C, width * width * sizeof(int), cudaMemcpyDeviceToHost);

    // 验证结果
    for (int i = 0; i < width; i++) {
        for (int j = 0; j < width; j++) {
            if (C[i * width + j] != i + j) {
                printf("错误!");
                return 0;
            }
        }
    }
    printf("矩阵加法成功!");

    // 释放CPU和GPU内存
    free(A); free(B); free(C);
    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
}
```

其实，CPU部分的main函数还是比较好懂的。核函数这边就有点不知所措了，比如下面这两行：

```cpp
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
```

这两行代码用于计算当前 CUDA 线程在二维矩阵中的位置。在 CUDA 编程模型中，我们通常将问题划分为多个线程块 (block)，每个线程块包含多个线程。线程块和线程可以是一维、二维或三维的。在这个矩阵加法的例子中，我们使用二维线程块和二维线程。

blockIdx 和 blockDim 分别表示线程块索引和线程块的尺寸，它们都是 dim3 类型的变量。threadIdx 表示线程的索引，也是一个 dim3 类型的变量。x 和 y 分别表示这些变量的横向和纵向分量。

```cpp
int row = blockIdx.y * blockDim.y + threadIdx.y;
```

这行代码计算当前线程在二维矩阵中的行号。blockIdx.y 表示当前线程所在的线程块在纵向（行方向）上的索引，blockDim.y 表示每个线程块在纵向上包含的线程数，threadIdx.y 表示当前线程在所在线程块中纵向的索引。将这些值组合在一起，可以计算出当前线程在整个矩阵中的行号。

```cpp
int col = blockIdx.x * blockDim.x + threadIdx.x;
```

这行代码计算当前线程在二维矩阵中的列号。blockIdx.x 表示当前线程所在的线程块在横向（列方向）上的索引，blockDim.x 表示每个线程块在横向上包含的线程数，threadIdx.x 表示当前线程在所在线程块中横向的索引。将这些值组合在一起，可以计算出当前线程在整个矩阵中的列号。

通过这两行代码，我们可以为每个线程分配一个特定的矩阵元素，让它执行相应的加法操作。这种并行计算方式可以显著提高矩阵加法的计算速度。

这段代码需要使用NVidia CUDA工具包中的nvcc来编译了，我们将其存为matrix_add.cu: 

```bash
nvcc -o matrix_add matrix_add.cu
./matrix_add
```

#### 8.2.2 OpenCL

CUDA是一门NVidia专有的技术，在其它GPU上用不了。所以其它厂商一直在想办法提供类似的技术。这其中，曾经最被看好的就是OpenCL。OpenCL由Apple最初提出并由Khronos Group牵头制定和管理标准。
OpenCL是一种用于编写跨平台的异构计算程序的框架，支持使用C99, C++14和C++17语言编写代码，可以在多种处理器和操作系统上运行，如CPU, GPU, DSP, FPGA等。
OpenCL的第一个版本于2008年发布。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/2020-opencl-api-overview-7_1.jpg)

我们来看下用OpenCL写的计算矩阵加法的节选。
首先也是运行在GPU上的核函数，然后通过enqueueNDRangeKernel将其放入执行队列中。

```cpp
#include <iostream>
#include <vector>
#include <CL/cl.hpp>

const char* kernelSource = R"CLC(
__kernel void matrix_add(__global const int* A, __global const int* B, __global int* C, int rows, int cols) {
    int i = get_global_id(0);
    int j = get_global_id(1);

    int index = i * cols + j;

    if (i < rows && j < cols) {
        C[index] = A[index] + B[index];
    }
}
)CLC";

int main() {
    std::vector<std::vector<int>> A = {
        {1, 2, 3},
        {4, 5, 6},
        {7, 8, 9}
    };

    std::vector<std::vector<int>> B = {
        {9, 8, 7},
        {6, 5, 4},
        {3, 2, 1}
    };

    int rows = A.size();
    int cols = A[0].size();

    std::vector<int> A_flat(rows * cols), B_flat(rows * cols), C_flat(rows * cols);
    for (int i = 0; i < rows; ++i) {
        for (int j = 0; j < cols; ++j) {
            A_flat[i * cols + j] = A[i][j];
            B_flat[i * cols + j] = B[i][j];
        }
    }

    std::vector<cl::Platform> platforms;
    cl::Platform::get(&platforms);

    cl_context_properties properties[] = {
        CL_CONTEXT_PLATFORM, (cl_context_properties)(platforms[0])(), 0
    };
    cl::Context context(CL_DEVICE_TYPE_GPU, properties);

    cl::Program program(context, kernelSource, true);

    cl::CommandQueue queue(context);

    cl::Buffer buffer_A(context, CL_MEM_READ_ONLY, sizeof(int) * rows * cols);
    cl::Buffer buffer_B(context, CL_MEM_READ_ONLY, sizeof(int) * rows * cols);
    cl::Buffer buffer_C(context, CL_MEM_WRITE_ONLY, sizeof(int) * rows * cols);

    queue.enqueueWriteBuffer(buffer_A, CL_TRUE, 0, sizeof(int) * rows * cols, A_flat.data());
    queue.enqueueWriteBuffer(buffer_B, CL_TRUE, 0, sizeof(int) * rows * cols, B_flat.data());

    cl::Kernel kernel(program, "matrix_add");
    kernel.setArg(0, buffer_A);
    kernel.setArg(1, buffer_B);
    kernel.setArg(2, buffer_C);
    kernel.setArg(3, rows);
    kernel.setArg(4, cols);

    cl::NDRange global_size(rows, cols);
    queue.enqueueNDRangeKernel(kernel, cl::NullRange, global_size);

    queue.enqueueReadBuffer(buffer_C, CL_TRUE, 0, sizeof(int) * rows * cols, C_flat.data());

    std::vector<std::vector<int>> C(rows, std::vector<int>(cols));
    for (int i = 0; i < rows; ++i) {
        for (int j = 0; j < cols; ++j) {
            C[i][j] = C_flat[i * cols + j];
        }
    }

...
```

#### 8.2.3 Direct3D

在Windows上，我们都知道微软的主要用于游戏开发的DirectX。
Direct X作为Windows直接访问硬件的游戏加速接口，早在1995年就推出了。不过Direct X 1.0的时候还不支持3D，只支持2D。因为第一个广泛使用的3D加速卡3dfx Voodoo卡1996年才推出。
Direct3D 1.0于1996年问世。不过这时候只是对标OpenGL的框架，跟GPGPU关系还远着呢。

一直要到2009年，Windows 7时代的Direct3D 11.0，才正式可以支持计算着色器。Direct 3D 12.0于2015年和Windows 10同时代推出。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/dx12.jpg)

在Direct3D 12中，GPU指令是通过HLSL语言来写的：

```hlsl
// MatrixAddition.hlsl

[numthreads(16, 16, 1)]
void main(uint3 dt : SV_DispatchThreadID, uint3 gt : SV_GroupThreadID, uint3 gi : SV_GroupID) {
    // 确保我们在矩阵范围内
    if (dt.x >= 3 || dt.y >= 3) {
        return;
    }

    // 矩阵 A 和 B 的值
    float A[3][3] = {
        {1, 2, 3},
        {4, 5, 6},
        {7, 8, 9}
    };

    float B[3][3] = {
        {9, 8, 7},
        {6, 5, 4},
        {3, 2, 1}
    };

    // 计算矩阵加法
    float result = A[dt.y][dt.x] + B[dt.y][dt.x];

    // 将结果写入输出缓冲区
    RWStructuredBuffer<float> output;
    output[dt.y * 3 + dt.x] = result;
}
```

然后是CPU上的操作，要建立一个计算着色器，因为细节比较多，我就略去了，只写主干：

```cpp
#include <d3d12.h>
#include <d3dcompiler.h>
#include <iostream>

// 创建一个简单的计算着色器的 PSO
ID3D12PipelineState* CreateMatrixAdditionPSO(ID3D12Device* device) {
    ID3DBlob* csBlob = nullptr;
    D3DCompileFromFile(L"MatrixAddition.hlsl", nullptr, nullptr, "main", "cs_5_0", 0, 0, &csBlob, nullptr);

    D3D12_COMPUTE_PIPELINE_STATE_DESC psoDesc = {};
    psoDesc.pRootSignature = rootSignature; // 假设已创建好根签名
    psoDesc.CS = CD3DX12_SHADER_BYTECODE(csBlob);

    ID3D12PipelineState* pso = nullptr;
    device->CreateComputePipelineState(&psoDesc, IID_PPV_ARGS(&pso));

    csBlob->Release();
    return pso;
}

// 执行矩阵加法计算
void RunMatrixAddition(ID3D12GraphicsCommandList* commandList, ID3D12Resource* outputBuffer) {
    commandList->SetPipelineState(matrixAdditionPSO);
    commandList->SetComputeRootSignature(rootSignature);
    commandList->SetComputeRootUnorderedAccessView(0, outputBuffer->GetGPUVirtualAddress());
    // 分发计算着色器，设置线程组的数量
    commandList->Dispatch(1, 1, 1);

    // 确保在继续之前完成计算操作
    commandList->ResourceBarrier(1, &CD3DX12_RESOURCE_BARRIER::UAV(outputBuffer));
}

int main() {
    // 初始化 DirectX 12 设备、命令队列、命令分配器等...
    // ...

    // 创建根签名、PSO 和计算着色器相关资源
    // ...

    // 创建输出缓冲区
    ID3D12Resource* outputBuffer = nullptr;
    device->CreateCommittedResource(
        &CD3DX12_HEAP_PROPERTIES(D3D12_HEAP_TYPE_DEFAULT),
        D3D12_HEAP_FLAG_NONE,
        &CD3DX12_RESOURCE_DESC::Buffer(3 * 3 * sizeof(float)),
        D3D12_RESOURCE_STATE_UNORDERED_ACCESS,
        nullptr,
        IID_PPV_ARGS(&outputBuffer)
    );

    // 创建并执行命令列表
    ID3D12GraphicsCommandList* commandList = nullptr;
    device->CreateCommandList(0, D3D12_COMMAND_LIST_TYPE_DIRECT, commandAllocator, nullptr, IID_PPV_ARGS(&commandList));

    RunMatrixAddition(commandList, outputBuffer);

    // 关闭命令列表并执行
    commandList->Close();
    ID3D12CommandList* commandLists[] = {commandList};
    commandQueue->ExecuteCommandLists(_countof(commandLists), commandLists);

    // 同步 GPU 和 CPU
    // ...

    // 从输出缓冲区中读取结果
    float result[3][3] = {};
    void* mappedData = nullptr;
    outputBuffer->Map(0, nullptr, &mappedData);
    memcpy(result, mappedData, sizeof(result));
    outputBuffer->Unmap(0, nullptr);

    // 输出结果
    for (int i = 0; i < 3; ++i) {
        for (int j = 0; j < 3; ++j) {
            std::cout << result[i][j] << " ";
        }
        std::cout << std::endl;
    }

    // 清理资源
    // ...
}

```

#### 8.2.4 Vulkan

Vulkan由Khronos Group牵头制定和管理标准，是OpenGL的继任者。它最早的技术来自于AMD。
Vulkan是一种用于编写跨平台的图形和计算程序的框架，支持使用C和C++语言编写代码，可以在多种处理器和操作系统上运行，如CPU, GPU, DSP, FPGA等。

Vulkan的1.0版本于2016年发布。

默认情况下，Vulkan使用带计算管线的glsl: 

```glsl
#version 450
#extension GL_ARB_separate_shader_objects : enable

layout (local_size_x = 16, local_size_y = 16, local_size_z = 1) in;

layout (binding = 0) readonly buffer InputA {
    float dataA[];
};

layout (binding = 1) readonly buffer InputB {
    float dataB[];
};

layout (binding = 2) writeonly buffer Output {
    float dataC[];
};

void main() {
    uint index = gl_GlobalInvocationID.x + gl_GlobalInvocationID.y * gl_NumWorkGroups.x * gl_WorkGroupSize.x;
    dataC[index] = dataA[index] + dataB[index];
}
```

然后，在主机程序中，完成以下步骤：

- 初始化Vulkan实例和物理/逻辑设备。
- 创建一个Vulkan计算管道，加载和编译计算着色器。
- 为输入矩阵A和B以及输出矩阵C创建Vulkan缓冲区。
- 将输入矩阵数据复制到输入缓冲区。
- 创建描述符集布局和描述符池，以描述着色器中的资源绑定。
- 创建描述符集，并将输入/输出缓冲区绑定到描述符集中。
- 创建一个Vulkan命令缓冲区，以记录计算着色器调度的命令。
- 开始记录命令缓冲区，并调用vkCmdBindPipeline和vkCmdBindDescriptorSets将计算管道和描- 述符集绑定到命令缓冲区。
- 使用vkCmdDispatch调度计算着色器执行矩阵加法。
- 结束命令缓冲区记录，将命令缓冲区提交到Vulkan队列。
- 等待队列执行完成，并将输出缓冲区的数据复制回主机内存。
- 清理Vulkan资源。

具体代码就不详细列出了。
大致的代码结构为：
```cpp
// Vulkan实例、设备、命令池、队列
VkInstance instance;
VkDevice device;
VkCommandPool commandPool;
VkQueue queue;

// 矩阵维度
const int width = 4;

// 顶点缓冲区对象
VkBuffer vertexBuffer;
VkDeviceMemory vertexBufferMemory;

// 结果缓冲区对象
VkBuffer resultBuffer;
VkDeviceMemory resultBufferMemory;

// 着色器模块和管线
VkShaderModule shaderModule;
VkPipeline pipeline;

// 创建顶点缓冲区
// 向缓冲区填充矩阵A和B
// ...

// 创建结果缓冲区
// 向缓冲区映射内存 
void* resultData;
vkMapMemory(device, resultBufferMemory, 0, sizeof(int) * 4 * 4, 0, &resultData);

// 创建着色器模块(矩阵加法着色器)  
const char* shaderCode = "上面的glsl"; 
shaderModule = createShaderModule(shaderCode);

// 创建图形管线
// ...

// 记录命令
VkCommandBuffer commandBuffer; 
VkCommandBufferAllocateInfo commandBufferAllocateInfo = ...;
vkAllocateCommandBuffers(commandPool, &commandBufferAllocateInfo, &commandBuffer);

// 开始记录命令
vkBeginCommandBuffer(commandBuffer, &beginInfo);

// 绑定顶点缓冲区和结果缓冲区
vkCmdBindVertexBuffers(commandBuffer, 0, 1, &vertexBuffer, &offset);
vkCmdBindBuffer(commandBuffer, 1, 0, resultBuffer, &offset);

// 绘制
vkCmdDraw(commandBuffer, 4, 1, 0, 0); 

// 结束记录命令  
vkEndCommandBuffer(commandBuffer);

// 提交命令并执行
VkSubmitInfo submitInfo = ...;
vkQueueSubmit(queue, 1, &submitInfo, VK_NULL_HANDLE);
vkQueueWaitIdle(queue); 

// 读取结果矩阵
for (int i = 0; i < width; i++) {
    for (int j = 0; j < width; j++) {
        int result = ((int*)resultData)[i * width + j];
        printf("%d ", result);
    }
    printf("\n");
}

// 释放Vulkan资源
...
```

#### 8.2.5 WebGPU

WebGPU是刚刚要被Chrome浏览器支持的用于前端的GPU技术。
WebGPU是一种用于编写跨平台的图形和计算程序的框架，支持使用JavaScript和WebAssembly语言编写代码，可以在多种浏览器和操作系统上运行，如Chrome, Firefox, Safari等。WebGPU是由W3C的GPU for the Web工作组制定和管理标准，是WebGL的继任者。
前面我们看到，源于NVidia技术的CUDA，源于Apple技术的OpenCL，源于微软技术的DirectX，还有源于AMD技术的Vulkan在桌面和服务端百花争艳。在移动端自然也是少不了龙争虎斗。

第一个提出WebGPU想法的是苹果，2016年2月，苹果公司提出了一个名为Web Metal的提案，旨在将Metal API的概念移植到Web平台上。
2017年2月，微软公司提出了一个名为Web D3D的提案，旨在将Direct3D 12 API的概念移植到Web平台上。
2017年8月，Mozilla公司提出了一个名为Obsidian的提案，旨在创建一个基于Vulkan API的抽象层。

几家争执不下，谷歌公司提出了一个名为NXT的提案，旨在创建一个基于Vulkan, Metal和Direct3D 12 API的抽象层。
2018年4月，W3C工作组决定将NXT作为规范草案的起点，并将其重命名为WebGPU。

既然是一个抽象层，着色器语言不管使用SPIR-V，Vulkan的GLSL，DirectX的HLSL或者苹果的Metal Shading Language就都不合适了。
于是2019年，WebGPU社区组提出了一个新的着色器语言的提案，名为WebGPU Shading Language (WGSL)，旨在创建一个基于SPIR-V的文本格式，以提供一种安全、可移植、易于使用和易于实现的着色器语言。

下面的代码展示下流程，这个时刻还有浏览器正式支持。等子弹飞一会儿浏览器正式上线了之后，我们在后面会专门讲。

看下图：WebGPU的规范还没release呢。WGSL的规范也同样没有最后release。

![WebGPU](https://img-blog.csdnimg.cn/224b01362bee4bd3b3402f2974998719.png#pic_center)


```javascript
js
// 获取WebGPU adapter和设备 
const adapter = await navigator.gpu.requestAdapter();
const device = await adapter.requestDevice();

// 矩阵维度
const width = 4;

// 创建缓冲区 - 用作顶点缓冲区和结果缓冲区
const vertexBuffer = device.createBuffer({
  size: width * width * 4 * Int32Array.BYTES_PER_ELEMENT, 
  usage: GPUBufferUsage.VERTEX | GPUBufferUsage.STORAGE 
});

// 获得缓冲区映射 - 填充矩阵A和B
const vertexBufferMapping = await vertexBuffer.map();
new Int32Array(vertexBufferMapping).fill(/* A和B矩阵 */);
vertexBuffer.unmap();

// 着色器代码
const shaderCode = `
  kernel void addMatrices(device int* a [[buffer(0)]], 
                          device int* b [[buffer(1)]], 
                          device int* c [[buffer(2)]]) {
    const int width = 4;
    int tid = threadIdx.x * 4 + threadIdx.y;
    if (tid < width * width) {
      c[tid] = a[tid] + b[tid]; 
    }
  }
`;  

// 创建着色器模块
const shaderModule = device.createShaderModule({ 
  code: shaderCode 
});

// 运行着色器 - 执行矩阵加法
const pipeline = device.createComputePipeline({
  compute: {
    module: shaderModule, 
    entryPoint: "addMatrices" 
  }
});
const passEncoder = device.createCommandEncoder();
const computePass = passEncoder.beginComputePass();   
computePass.setPipeline(pipeline);
computePass.setBuffer(0, vertexBuffer);  
computePass.setBuffer(1, vertexBuffer);
computePass.setBuffer(2, vertexBuffer);  
computePass.dispatch(1); 
computePass.endPass();
device.queue.submit([passEncoder.finish()]);

// 读取结果 
const result = new Int32Array(
  await vertexBuffer.mapRead()
);
// 打印结果矩阵
... 

// 释放资源
```

### 8.3 CUDA应用开发初步

#### 8.3.1 获取CUDA设备信息

在使用CUDA设备之前，首先我们得获取是否支持CUDA，有几个设备。这个可以通过`cudaGetDeviceCount`

```cpp
    int deviceCount;
    cudaError_t cudaError;
    cudaError = cudaGetDeviceCount(&deviceCount);

    if (cudaError == cudaSuccess) {
        cout << "There are " << deviceCount << " cuda devices." << endl;
    }
```

获取了支持多少个设备了之后，我们就可以遍历设备去用cudaGetDeviceProperties函数去查看设备信息了。

```cpp
    for (int i = 0; i < deviceCount; i++)
    {
        cudaError = cudaGetDeviceProperties(&props, i);

        if (cudaError == cudaSuccess) {
            cout << "Device Name： " << props.name << endl;
            cout << "Compute Capability version: " << props.major << "." << props.minor << endl;
        }
    }
```

这是我在我的电脑上输出的结果：
```
There are 1 cuda devices.
Device Name： NVIDIA GeForce RTX 3060
Compute Capability version: 8.6
```

我们来看下cudaDeviceProp函数的定义：

```cpp
struct cudaDeviceProp {
              char name[256];
              cudaUUID_t uuid;
              size_t totalGlobalMem;
              size_t sharedMemPerBlock;
              int regsPerBlock;
              int warpSize;
              size_t memPitch;
              int maxThreadsPerBlock;
              int maxThreadsDim[3];
              int maxGridSize[3];
              int clockRate;
              size_t totalConstMem;
              int major;
              int minor;
              size_t textureAlignment;
              size_t texturePitchAlignment;
              int deviceOverlap;
              int multiProcessorCount;
              int kernelExecTimeoutEnabled;
              int integrated;
              int canMapHostMemory;
              int computeMode;
              int maxTexture1D;
              int maxTexture1DMipmap;
              int maxTexture1DLinear;
              int maxTexture2D[2];
              int maxTexture2DMipmap[2];
              int maxTexture2DLinear[3];
              int maxTexture2DGather[2];
              int maxTexture3D[3];
              int maxTexture3DAlt[3];
              int maxTextureCubemap;
              int maxTexture1DLayered[2];
              int maxTexture2DLayered[3];
              int maxTextureCubemapLayered[2];
              int maxSurface1D;
              int maxSurface2D[2];
              int maxSurface3D[3];
              int maxSurface1DLayered[2];
              int maxSurface2DLayered[3];
              int maxSurfaceCubemap;
              int maxSurfaceCubemapLayered[2];
              size_t surfaceAlignment;
              int concurrentKernels;
              int ECCEnabled;
              int pciBusID;
              int pciDeviceID;
              int pciDomainID;
              int tccDriver;
              int asyncEngineCount;
              int unifiedAddressing;
              int memoryClockRate;
              int memoryBusWidth;
              int l2CacheSize;
              int persistingL2CacheMaxSize;
              int maxThreadsPerMultiProcessor;
              int streamPrioritiesSupported;
              int globalL1CacheSupported;
              int localL1CacheSupported;
              size_t sharedMemPerMultiprocessor;
              int regsPerMultiprocessor;
              int managedMemory;
              int isMultiGpuBoard;
              int multiGpuBoardGroupID;
              int singleToDoublePrecisionPerfRatio;
              int pageableMemoryAccess;
              int concurrentManagedAccess;
              int computePreemptionSupported;
              int canUseHostPointerForRegisteredMem;
              int cooperativeLaunch;
              int cooperativeMultiDeviceLaunch;
              int pageableMemoryAccessUsesHostPageTables;
              int directManagedMemAccessFromHost;
              int accessPolicyMaxWindowSize;
          }
```

我们择其要者介绍几个吧：

- totalGlobalMem是设备上可用的全局内存总量，以字节为单位。
- sharedMemPerBlock是一个线程块可用的最大共享内存量，以字节为单位。
- regsPerBlock是一个线程块可用的最大32位寄存器数量。
- warpSize是线程束的大小，以线程为单位。
- memPitch是涉及通过cudaMallocPitch()分配的内存区域的内存复制函数允许的最大间距，以字节为单位。
- maxThreadsPerBlock是每个块的最大线程数。
- maxThreadsDim[3]包含了一个块的每个维度的最大尺寸。
- maxGridSize[3]包含了一个网格的每个维度的最大尺寸。
- clockRate是时钟频率，以千赫为单位。
- totalConstMem是设备上可用的常量内存总量，以字节为单位。
- major, minor是定义设备计算能力的主要和次要修订号。
- multiProcessorCount是设备上多处理器的数量。
- memoryClockRate是峰值内存时钟频率，以千赫为单位。
- memoryBusWidth是内存总线宽度，以位为单位。
- memoryPoolsSupported 是 1，如果设备支持使用 cudaMallocAsync 和 cudaMemPool 系列 API，否则为 0
- gpuDirectRDMASupported 是 1，如果设备支持 GPUDirect RDMA API，否则为 0
- gpuDirectRDMAFlushWritesOptions 是一个按照 cudaFlushGPUDirectRDMAWritesOptions 枚举解释的位掩码
- gpuDirectRDMAWritesOrdering 参见 cudaGPUDirectRDMAWritesOrdering 枚举的数值
- memoryPoolSupportedHandleTypes 是一个支持与 mempool-based IPC 的句柄类型的位掩码
- deferredMappingCudaArraySupported 是 1，如果设备支持延迟映射 CUDA 数组和 CUDA mipmapped 数组
- ipcEventSupported 是 1，如果设备支持 IPC 事件，否则为 0
- unifiedFunctionPointers 是 1，如果设备支持统一指针，否则为 0

有了更多的信息，我们输出一些看看：

```cpp
    for (int i = 0; i < deviceCount; i++)
    {
        cudaError = cudaGetDeviceProperties(&props, i);

        if (cudaError == cudaSuccess) {
            cout << "Device Name： " << props.name << endl;
            cout << "Compute Capability version: " << props.major << "." << props.minor << endl;
            cout << "设备上可用的全局内存总量:(G字节)" << props.totalGlobalMem / 1024 / 1024 / 1024 << endl;
            cout << "时钟频率（以MHz为单位）:" << props.clockRate / 1000 << endl;
            cout << "设备上多处理器的数量:" << props.multiProcessorCount << endl;
            cout << "每个块的最大线程数:" << props.maxThreadsPerBlock <<endl;
            cout << "内存总线宽度(位)" << props.memoryBusWidth << endl;
            cout << "一个块的每个维度的最大尺寸:" << props.maxThreadsDim[0] << ","<< props.maxThreadsDim[1] << "," << props.maxThreadsDim[2] << endl;
            cout << "一个网格的每个维度的最大尺寸:" << props.maxGridSize[0] << "," << props.maxGridSize[1] << "," << props.maxGridSize[2] <<endl;
        }
    }
```

在我的3060显卡上运行的结果：
```
Device Name： NVIDIA GeForce RTX 3060
Compute Capability version: 8.6
设备上可用的全局内存总量:(G字节)11
时钟频率（以MHz为单位）:1777
设备上多处理器的数量:28
每个块的最大线程数:1024
内存总线宽度(位)192
一个块的每个维度的最大尺寸:1024,1024,64
一个网格的每个维度的最大尺寸:2147483647,65535,65535
```

#### 8.3.2 线程块和线程网格

在CUDA中，线程块（block）和线程网格（grid）是两个非常重要的概念，它们用于描述GPU执行并行任务时的线程组织方式。线程块是由若干个线程（thread）组成的，它们可以在同一个GPU多处理器（multiprocessor）上并行执行。线程网格则是由若干个线程块组成的，它们可以在整个GPU设备上并行执行。每个线程块和线程网格都有一个唯一的索引，用于在CUDA C/C++的GPU核函数中对线程进行标识和控制。

在CUDA中，使用dim3结构体来表示线程块和线程网格的维度。例如，dim3(2,2)表示一个2D线程网格，其中有2x2=4个线程块；dim3(2,2,2)表示一个3D线程块，其中有2x2x2=8个线程。在启动GPU核函数时，可以使用<<< >>>的语法来指定线程网格和线程块的大小，例如：

```cpp
dim3 dimGrid(2, 2);
dim3 dimBlock(2, 2, 2);
myKernel<<<dimGrid, dimBlock>>>(...);
```

这里使用dimGrid和dimBlock指定了线程网格和线程块的大小，然后调用myKernel函数，并传递必要的参数。在执行GPU核函数时，CUDA会按照指定的线程网格和线程块的大小启动对应的线程，并对它们进行分配和协作，从而完成任务的并行执行。线程块和线程网格的组织方式和大小都可以根据具体的应用场景和硬件环境进行调整和优化，以实现最优的性能和效率。

我们再看下在核函数中如何使用线程网格和线程块。

```cpp
__global__ void testKernel(int val) {
    printf("[%d, %d]:\t\tValue is:%d\n", blockIdx.y * gridDim.x + blockIdx.x,
        threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x +
        threadIdx.x,
        val);
}
```

上面有几个点我们需要解释一下：
- `__global__`：并不是表明这是一个全局函数，而是表明这是一个GPU核函数。
- blockIdx：是一个内置的变量，表示当前线程所在的块（block）的索引。它是一个结构体类型，包含了三个成员变量，分别表示当前块在x、y、z三个维度上的索引值。
- threadIdx：也是一个内置的变量，表示当前线程在所在的块中的索引。它也同样是一个结构体类型，包含了三个成员变量，分别表示当前线程在x、y、z三个维度上的索引值。
- blockDim：同样是一个内置的变量，表示每个块（block）的维度（dimension），包括x、y、z三个维度。

在CUDA中，每个核函数（kernel function）被分配到一个或多个块（block）中执行，每个块包含若干个线程（thread），它们可以在GPU上并行执行。通过访问blockIdx的成员变量，可以确定当前线程所在的块在哪个位置，从而在核函数中进行特定的计算。例如，可以使用blockIdx.x表示当前线程所在的块在x轴上的索引值。在CUDA编程中，通常需要使用blockIdx和threadIdx来确定每个线程在整个GPU并行执行中的唯一标识，以便进行任务的分配和协作。

然后将dimGrid和dimBlock传给testKernel.

```cpp
    // Kernel configuration, where a two-dimensional grid and
    // three-dimensional blocks are configured.
    dim3 dimGrid(2, 2);
    dim3 dimBlock(2, 2, 2);
    testKernel << <dimGrid, dimBlock >> > (10);
```

将下面的文件保存为kernel.cu，然后通过nvcc命令编译，最后运行生成的可执行文件就可以了。

```cpp
// System includes
#include <stdio.h>
#include <assert.h>
#include <iostream>

// CUDA runtime
#include <cuda_runtime.h>

using namespace std;

__global__ void testKernel(int val) {
    printf("[%d, %d]:\t\tValue is:%d\n", blockIdx.y * gridDim.x + blockIdx.x,
        threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x +
        threadIdx.x,
        val);
}

int main(int argc, char** argv) {
    int devID;
    cudaDeviceProp props;

    int deviceCount;
    cudaError_t cudaError;
    cudaError = cudaGetDeviceCount(&deviceCount);

    if (cudaError == cudaSuccess) {
        cout << "There are " << deviceCount << " cuda devices." << endl;
    }

    for (int i = 0; i < deviceCount; i++)
    {
        cudaError = cudaGetDeviceProperties(&props, i);

        if (cudaError == cudaSuccess) {
            cout << "Device Name： " << props.name << endl;
            cout << "Compute Capability version: " << props.major << "." << props.minor << endl;
            cout << "设备上可用的全局内存总量:(G字节)" << props.totalGlobalMem / 1024 / 1024 / 1024 << endl;
            cout << "时钟频率（以MHz为单位）:" << props.clockRate / 1000 << endl;
            cout << "设备上多处理器的数量:" << props.multiProcessorCount << endl;
            cout << "每个块的最大线程数:" << props.maxThreadsPerBlock <<endl;
            cout << "内存总线宽度(位)" << props.memoryBusWidth << endl;
            cout << "一个块的每个维度的最大尺寸:" << props.maxThreadsDim[0] << ","<< props.maxThreadsDim[1] << "," << props.maxThreadsDim[2] << endl;
            cout << "一个网格的每个维度的最大尺寸:" << props.maxGridSize[0] << "," << props.maxGridSize[1] << "," << props.maxGridSize[2] <<endl;
        }
    }

    // Kernel configuration, where a two-dimensional grid and
    // three-dimensional blocks are configured.
    dim3 dimGrid(2, 2);
    dim3 dimBlock(2, 2, 2);
    testKernel << <dimGrid, dimBlock >> > (10);
    cudaDeviceSynchronize();

    return EXIT_SUCCESS;
}
```

前面输出的不管，我们只看后面32个线程的结果：
```
[1, 0]:         Value is:10
[1, 1]:         Value is:10
[1, 2]:         Value is:10
[1, 3]:         Value is:10
[1, 4]:         Value is:10
[1, 5]:         Value is:10
[1, 6]:         Value is:10
[1, 7]:         Value is:10
[0, 0]:         Value is:10
[0, 1]:         Value is:10
[0, 2]:         Value is:10
[0, 3]:         Value is:10
[0, 4]:         Value is:10
[0, 5]:         Value is:10
[0, 6]:         Value is:10
[0, 7]:         Value is:10
[3, 0]:         Value is:10
[3, 1]:         Value is:10
[3, 2]:         Value is:10
[3, 3]:         Value is:10
[3, 4]:         Value is:10
[3, 5]:         Value is:10
[3, 6]:         Value is:10
[3, 7]:         Value is:10
[2, 0]:         Value is:10
[2, 1]:         Value is:10
[2, 2]:         Value is:10
[2, 3]:         Value is:10
[2, 4]:         Value is:10
[2, 5]:         Value is:10
[2, 6]:         Value is:10
[2, 7]:         Value is:10
```

前面表示线程块，后面表示线程。

大家第一次搞GPU编程的话很容易被绕晕。我来解释一下这个计算方法。其实就是跟用一维数组来模拟多维数组是一个算法。

blockIdx.y * gridDim.x + blockIdx.x表示当前线程所在的线程块在二维线程网格中的唯一标识。其中，gridDim.x表示线程网格在x方向上的线程块数量，blockIdx.x表示当前线程块在x方向上的索引值，blockIdx.y表示当前线程块在y方向上的索引值。

threadIdx.z * blockDim.x * blockDim.y表示当前线程在z方向上的偏移量，即前面所有线程所占用的空间大小。然后，threadIdx.y * blockDim.x表示当前线程在y方向上的偏移量，即当前线程在所在z平面上的偏移量。最后，threadIdx.x表示当前线程在x方向上的偏移量，即当前线程在所在z平面的某一行上的偏移量。

明白这一点之后，我们尝试将每个线程块从8个线程改成12个：
```cpp
    dim3 dimGrid(2, 2);
    dim3 dimBlock(2, 2, 3);
    testKernel << <dimGrid, dimBlock >> > (12);
```

运行结果如下：

```
[0, 0]:         Value is:12
[0, 1]:         Value is:12
[0, 2]:         Value is:12
[0, 3]:         Value is:12
[0, 4]:         Value is:12
[0, 5]:         Value is:12
[0, 6]:         Value is:12
[0, 7]:         Value is:12
[0, 8]:         Value is:12
[0, 9]:         Value is:12
[0, 10]:                Value is:12
[0, 11]:                Value is:12
[1, 0]:         Value is:12
[1, 1]:         Value is:12
[1, 2]:         Value is:12
[1, 3]:         Value is:12
[1, 4]:         Value is:12
[1, 5]:         Value is:12
[1, 6]:         Value is:12
[1, 7]:         Value is:12
[1, 8]:         Value is:12
[1, 9]:         Value is:12
[1, 10]:                Value is:12
[1, 11]:                Value is:12
[3, 0]:         Value is:12
[3, 1]:         Value is:12
[3, 2]:         Value is:12
[3, 3]:         Value is:12
[3, 4]:         Value is:12
[3, 5]:         Value is:12
[3, 6]:         Value is:12
[3, 7]:         Value is:12
[3, 8]:         Value is:12
[3, 9]:         Value is:12
[3, 10]:                Value is:12
[3, 11]:                Value is:12
[2, 0]:         Value is:12
[2, 1]:         Value is:12
[2, 2]:         Value is:12
[2, 3]:         Value is:12
[2, 4]:         Value is:12
[2, 5]:         Value is:12
[2, 6]:         Value is:12
[2, 7]:         Value is:12
[2, 8]:         Value is:12
[2, 9]:         Value is:12
[2, 10]:                Value is:12
[2, 11]:                Value is:12
```

下面我们正式开启真并发之旅，在上面的48个线程里同时计算正弦。
在GPU里计算，我们CPU上原来的数学库不顶用了，我们要用GPU自己的，在CUDA中我们用`__sinf`:

```cpp
__global__ void testKernel(float val) {
    printf("[%d, %d]:\t\tValue is:%f\n", blockIdx.y * gridDim.x + blockIdx.x,
        threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x +
        threadIdx.x,
        __sinf(val* threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x +
            threadIdx.x));
}
```

main函数里也随便改一个：
```cpp
    dim3 dimGrid(2, 2);
    dim3 dimBlock(2, 2, 3);
    testKernel << <dimGrid, dimBlock >> > (0.5);
```

运行结果如下：
```
[0, 0]:         Value is:0.000000
[0, 1]:         Value is:0.841471
[0, 2]:         Value is:0.909297
[0, 3]:         Value is:0.141120
[0, 4]:         Value is:0.909297
[0, 5]:         Value is:0.141120
[0, 6]:         Value is:-0.756802
[0, 7]:         Value is:-0.958924
[0, 8]:         Value is:-0.756802
[0, 9]:         Value is:-0.958924
[0, 10]:                Value is:-0.279416
[0, 11]:                Value is:0.656986
[1, 0]:         Value is:0.000000
[1, 1]:         Value is:0.841471
[1, 2]:         Value is:0.909297
[1, 3]:         Value is:0.141120
[1, 4]:         Value is:0.909297
[1, 5]:         Value is:0.141120
[1, 6]:         Value is:-0.756802
[1, 7]:         Value is:-0.958924
[1, 8]:         Value is:-0.756802
[1, 9]:         Value is:-0.958924
[1, 10]:                Value is:-0.279416
[1, 11]:                Value is:0.656986
[3, 0]:         Value is:0.000000
[3, 1]:         Value is:0.841471
[3, 2]:         Value is:0.909297
[3, 3]:         Value is:0.141120
[3, 4]:         Value is:0.909297
[3, 5]:         Value is:0.141120
[3, 6]:         Value is:-0.756802
[3, 7]:         Value is:-0.958924
[3, 8]:         Value is:-0.756802
[3, 9]:         Value is:-0.958924
[3, 10]:                Value is:-0.279416
[3, 11]:                Value is:0.656986
[2, 0]:         Value is:0.000000
[2, 1]:         Value is:0.841471
[2, 2]:         Value is:0.909297
[2, 3]:         Value is:0.141120
[2, 4]:         Value is:0.909297
[2, 5]:         Value is:0.141120
[2, 6]:         Value is:-0.756802
[2, 7]:         Value is:-0.958924
[2, 8]:         Value is:-0.756802
[2, 9]:         Value is:-0.958924
[2, 10]:                Value is:-0.279416
[2, 11]:                Value is:0.656986
```

#### 8.3.3 内存与显存间的数据交换

上面我们是传了一个立即数到GPU核函数。我们距离正式能使用GPU进行CUDA编程，就差分配GPU显存和在显存和内存之间复制了。

同malloc类似，CUDA使用cudaMalloc来分配GPU内存，其原型为：

```cpp 
cudaError_t cudaMalloc(void **devPtr, size_t size);
```

参数解释:
- devPtr: 返回分配的设备内存的指针。
- size: 要分配的内存大小,以字节为单位。

返回值:
- cudaSuccess: 分配成功。 
- cudaErrorInvalidValue: size为零或devPtr为NULL。
- cudaErrorMemoryAllocation: 内存分配失败。

一般的用法，记得用完了用cudaFree释放掉：

```cpp
float* devPtr;
cudaMalloc(&devPtr, size * sizeof(float));
...
cudaFree(devPtr);
```

分配完内存了，然后就是从内存复制到显存了。同样类似于memcpy，通过cudaMemcpy来完成。

```cpp
cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind);
```

参数解释:
- dst: 目标内存的指针。
- src: 源内存的指针。
- count: 要拷贝的内存大小,以字节为单位。
- kind: 拷贝的类型,可以是:
  - cudaMemcpyHostToHost
  - cudaMemcpyHostToDevice
  - cudaMemcpyDeviceToHost
  - cudaMemcpyDeviceToDevice

返回值:
- cudaSuccess: 拷贝成功。
- cudaErrorInvalidValue: count或dst或src为NULL。
- cudaErrorMemoryAllocation: 内存分配失败。

下面我们来写一个用CUDA计算平方根的例子：

```cpp
    const int n = 1024;
    size_t size = n * sizeof(float);
    float* h_in = (float*)malloc(size);
    float* h_out = (float*)malloc(size);
    float* d_in, * d_out;

    // Initialize input array
    for (int i = 0; i < n; ++i) {
        h_in[i] = (float)i;
    }

    // Allocate device memory
    cudaMalloc(&d_in, size);
    cudaMalloc(&d_out, size);

    // Copy input data to device
    cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);

    // Launch kernel
    int threadsPerBlock = 256;
    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
    sqrtKernel << <blocksPerGrid, threadsPerBlock >> > (d_in, d_out, n);

    // Copy output data to host
    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);

    // Verify results
    for (int i = 0; i < n; ++i) {
        if (fabsf(h_out[i] - sqrtf(h_in[i])) > 1e-5) {
            printf("Error: h_out[%d] = %f, sqrtf(h_in[%d]) = %f\n", i, h_out[i], i, sqrtf(h_in[i]));
        }
    }

    printf("Success!\n");

    // Free memory
    free(h_in);
    free(h_out);
    cudaFree(d_in);
    cudaFree(d_out);
```

大家关注线程块数和线程数这两个，我们这里没有用多维，就是用两个整数计算的：
```cpp
    int threadsPerBlock = 256;
    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
    sqrtKernel << <blocksPerGrid, threadsPerBlock >> > (d_in, d_out, n);
```

我们用4个块，每个块有256个线程。

此时，就不用计算y和z了，只计算x维度就可以：
```cpp
int i = blockIdx.x * blockDim.x + threadIdx.x;
```

但是要注意，blockIdx和threadIdx仍然是三维的，y和z维仍然是有效的，只不过它们变成0了。

我们的核函数这样写：
```cpp
__global__ void sqrtKernel(float* in, float* out, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        out[i] = sqrtf(in[i]);
        printf("[%d, %d]:\t\tValue is:%f\n", blockIdx.y * gridDim.x + blockIdx.x,
            threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x +
            threadIdx.x, out[i]);
    }
}
```

当然了，因为block和thread的y和z都是0，跟只写x是没啥区别的：
```cpp
__global__ void sqrtKernel(float* in, float* out, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        out[i] = sqrtf(in[i]);
        printf("[%d, %d]:\t\tValue is:%f\n", blockIdx.x, threadIdx.x, out[i]);
    }
}
```

#### 8.3.4 使用封装好的库

除了CUDA运行时之外，针对主要的应用场景，NVidia也提供了很多专门的库。

比如针对矩阵运算，就有cuBLAS库。有的库是跟随CUDA工具包一起安装的，比如cuBLAS, cuFFT。也有的库需要专门下载安装，比如cudnn库。

这里强调一下，所谓的库，不是在核函数中要调用的模块，而是将整个需要在核函数里面要实现的功能全封装好了。所以在使用封装库的时候，并不需要nvcc，就是引用一个库就好了。

我们来看一个使用cuBLAS库来计算矩阵乘法的例子。

cuBLAS库来计算矩阵乘法要用到的主要的函数有4个：
- cublasCreate: 创建cublas句柄
- cublasDestroy：释放cublas句柄
- cublasSetVector：在CPU和GPU内存间复制数据
- cublasSgemm：矩阵乘法运算

```cpp
cublasStatus_t cublasSetVector(int n, int elemSize, const void *x, int incx, void *y, int incy)
```

其中:

- n 是要拷贝的元素个数
- elemSize是每个元素的大小（以字节为单位）
- x是主机端（CPU）内存中的数据起始地址
- incx是x中相邻元素之间的跨度
- y是GPU设备内存中的数据起始地址
- incy是y中相邻元素之间的跨度

```cpp
cublasStatus_t cublasSgemm(cublasHandle_t handle,
                           cublasOperation_t transa, cublasOperation_t transb,
                           int m, int n, int k,
                           const float *alpha, const float *A, int lda,
                           const float *B, int ldb, const float *beta,
                           float *C, int ldc)
```

其中:
- handle是cuBLAS句柄；
- transa是A矩阵的转置选项，取值为CUBLAS_OP_N或CUBLAS_OP_T，分别表示不转置和转置；
- transb是B矩阵的转置选项；m、n、k分别是A、B、C矩阵的维度；
- alpha是一个标量值，用于将A和B矩阵的乘积缩放到C矩阵中；
- A是A矩阵的起始地址；
- lda是A矩阵中相邻列之间的跨度；
- B是B矩阵的起始地址；
- ldb是B矩阵中相邻列之间的跨度；
- beta是一个标量值，用于将C矩阵中的值缩放；
- C是C矩阵的起始地址；
- ldc是C矩阵中相邻列之间的跨度。

我们简化写一个例子，主要说明函数的用法：

```cpp
#include <stdio.h>
#include <cuda_runtime.h>
#include <cublas_v2.h>

int main() {
    int m = 1024, n = 1024, k = 1024;
    float* h_A = (float*)malloc(m * k * sizeof(float));
    float* h_B = (float*)malloc(k * n * sizeof(float));
    float* h_C = (float*)malloc(m * n * sizeof(float));
    for (int i = 0; i < m * k; ++i) {
        h_A[i] = (float)i;
    }
    for (int i = 0; i < k * n; ++i) {
        h_B[i] = (float)i;
    }

    float* d_A, * d_B, * d_C;
    cudaMalloc(&d_A, m * k * sizeof(float));
    cudaMalloc(&d_B, k * n * sizeof(float));
    cudaMalloc(&d_C, m * n * sizeof(float));

    // Copy data from host to device
    cublasSetVector(m * k, sizeof(float), h_A, 1, d_A, 1);
    cublasSetVector(k * n, sizeof(float), h_B, 1, d_B, 1);

    // Initialize cuBLAS
    cublasHandle_t handle;
    cublasCreate(&handle);

    // Do matrix multiplication
    const float alpha = 1.0f, beta = 0.0f;
    cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k,
        &alpha, d_A, m, d_B, k, &beta, d_C, m);

    // Copy data from device to host
    cublasGetVector(m * n, sizeof(float), d_C, 1, h_C, 1);

    // Free memory
    free(h_A);
    free(h_B);
    free(h_C);
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // Destroy cuBLAS handle
    cublasDestroy(handle);

    return 0;
}
```

当然，上面的只是个例子，没有做错误处理，这样是不对的。
我们参考官方的例子，注释我直接写在代码中了：

```cpp
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

/* Includes, cuda */
#include <cublas_v2.h>
#include <cuda_runtime.h>
#include <helper_cuda.h>

/* Matrix size */
#define N (275)

//单精度通用矩阵乘法
static void simple_sgemm(int n, float alpha, const float *A, const float *B,
                         float beta, float *C) {
  int i;
  int j;
  int k;

  for (i = 0; i < n; ++i) {
    for (j = 0; j < n; ++j) {
      float prod = 0; // 这个嵌套循环遍历矩阵C的所有元素。对于每个元素，我们初始化乘积prod为0。

      for (k = 0; k < n; ++k) {
        prod += A[k * n + i] * B[j * n + k]; //这个内层循环计算矩阵A的第i行和矩阵B的第j列的点积，并将结果累加到prod中。
      }

      C[j * n + i] = alpha * prod + beta * C[j * n + i]; //我们将计算出的乘积乘以alpha，再加上beta乘以矩阵C的当前元素，然后将结果存储回矩阵C的相应位置
    }
  }
}

/* Main */
int main(int argc, char **argv) {
  //h_A, h_B, h_C是主机（CPU）内存中的矩阵，d_A, d_B, d_C是设备（GPU）内存中的矩阵。alpha和beta是矩阵乘法的标量系数。n2是矩阵的大小。error_norm和ref_norm用于计算误差。handle是CUBLAS库的句柄。
  cublasStatus_t status;
  float *h_A;
  float *h_B;
  float *h_C;
  float *h_C_ref;
  float *d_A = 0;
  float *d_B = 0;
  float *d_C = 0;
  float alpha = 1.0f;
  float beta = 0.0f;
  int n2 = N * N;
  int i;
  float error_norm;
  float ref_norm;
  float diff;
  cublasHandle_t handle;

  // 初始化CUBLAS库并检查是否成功
  printf("simpleCUBLAS test running..\n");

  status = cublasCreate(&handle);

  if (status != CUBLAS_STATUS_SUCCESS) {
    fprintf(stderr, "!!!! CUBLAS initialization error\n");
    return EXIT_FAILURE;
  }

  
  h_A = reinterpret_cast<float *>(malloc(n2 * sizeof(h_A[0])));

  //为矩阵A、B和C在主机内存中分配空间，并检查是否分配成功。

  if (h_A == 0) {
    fprintf(stderr, "!!!! host memory allocation error (A)\n");
    return EXIT_FAILURE;
  }

  h_B = reinterpret_cast<float *>(malloc(n2 * sizeof(h_B[0])));

  if (h_B == 0) {
    fprintf(stderr, "!!!! host memory allocation error (B)\n");
    return EXIT_FAILURE;
  }

  h_C = reinterpret_cast<float *>(malloc(n2 * sizeof(h_C[0])));

  if (h_C == 0) {
    fprintf(stderr, "!!!! host memory allocation error (C)\n");
    return EXIT_FAILURE;
  }

  // 使用随机数填充矩阵A、B和C
  for (i = 0; i < n2; i++) {
    h_A[i] = rand() / static_cast<float>(RAND_MAX);
    h_B[i] = rand() / static_cast<float>(RAND_MAX);
    h_C[i] = rand() / static_cast<float>(RAND_MAX);
  }

  /* 为矩阵A、B和C在设备内存中分配空间，并检查是否分配成功 */
  if (cudaMalloc(reinterpret_cast<void **>(&d_A), n2 * sizeof(d_A[0])) !=
      cudaSuccess) {
    fprintf(stderr, "!!!! device memory allocation error (allocate A)\n");
    return EXIT_FAILURE;
  }

  if (cudaMalloc(reinterpret_cast<void **>(&d_B), n2 * sizeof(d_B[0])) !=
      cudaSuccess) {
    fprintf(stderr, "!!!! device memory allocation error (allocate B)\n");
    return EXIT_FAILURE;
  }

  if (cudaMalloc(reinterpret_cast<void **>(&d_C), n2 * sizeof(d_C[0])) !=
      cudaSuccess) {
    fprintf(stderr, "!!!! device memory allocation error (allocate C)\n");
    return EXIT_FAILURE;
  }

  /* 将主机内存中的矩阵A、B和C复制到设备内存中 */
  status = cublasSetVector(n2, sizeof(h_A[0]), h_A, 1, d_A, 1);

  if (status != CUBLAS_STATUS_SUCCESS) {
    fprintf(stderr, "!!!! device access error (write A)\n");
    return EXIT_FAILURE;
  }

  status = cublasSetVector(n2, sizeof(h_B[0]), h_B, 1, d_B, 1);

  if (status != CUBLAS_STATUS_SUCCESS) {
    fprintf(stderr, "!!!! device access error (write B)\n");
    return EXIT_FAILURE;
  }

  status = cublasSetVector(n2, sizeof(h_C[0]), h_C, 1, d_C, 1);

  if (status != CUBLAS_STATUS_SUCCESS) {
    fprintf(stderr, "!!!! device access error (write C)\n");
    return EXIT_FAILURE;
  }

  /* 使用CPU进行矩阵乘法计算并将结果保存到h_C_ref中 */
  simple_sgemm(N, alpha, h_A, h_B, beta, h_C);
  h_C_ref = h_C;

  /* 使用CUBLAS库在GPU上进行矩阵乘法计算 */
  status = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N, &alpha, d_A,
                       N, d_B, N, &beta, d_C, N);

  if (status != CUBLAS_STATUS_SUCCESS) {
    fprintf(stderr, "!!!! kernel execution error.\n");
    return EXIT_FAILURE;
  }

  /* Allocate host memory for reading back the result from device memory */
  h_C = reinterpret_cast<float *>(malloc(n2 * sizeof(h_C[0])));

  if (h_C == 0) {
    fprintf(stderr, "!!!! host memory allocation error (C)\n");
    return EXIT_FAILURE;
  }

  /* 为结果分配主机内存，并将设备内存中的结果读取回主机内存 */
  status = cublasGetVector(n2, sizeof(h_C[0]), d_C, 1, h_C, 1);

  if (status != CUBLAS_STATUS_SUCCESS) {
    fprintf(stderr, "!!!! device access error (read C)\n");
    return EXIT_FAILURE;
  }

  /* 计算误差并与参考结果进行比较 */
  error_norm = 0;
  ref_norm = 0;

  for (i = 0; i < n2; ++i) {
    diff = h_C_ref[i] - h_C[i];
    error_norm += diff * diff;
    ref_norm += h_C_ref[i] * h_C_ref[i];
  }

  error_norm = static_cast<float>(sqrt(static_cast<double>(error_norm)));
  ref_norm = static_cast<float>(sqrt(static_cast<double>(ref_norm)));

  if (fabs(ref_norm) < 1e-7) {
    fprintf(stderr, "!!!! reference norm is 0\n");
    return EXIT_FAILURE;
  }

  /* Memory clean up */
  free(h_A);
  free(h_B);
  free(h_C);
  free(h_C_ref);

  if (cudaFree(d_A) != cudaSuccess) {
    fprintf(stderr, "!!!! memory free error (A)\n");
    return EXIT_FAILURE;
  }

  if (cudaFree(d_B) != cudaSuccess) {
    fprintf(stderr, "!!!! memory free error (B)\n");
    return EXIT_FAILURE;
  }

  if (cudaFree(d_C) != cudaSuccess) {
    fprintf(stderr, "!!!! memory free error (C)\n");
    return EXIT_FAILURE;
  }

  /* Shutdown */
  status = cublasDestroy(handle);

  if (status != CUBLAS_STATUS_SUCCESS) {
    fprintf(stderr, "!!!! shutdown error (A)\n");
    return EXIT_FAILURE;
  }

  if (error_norm / ref_norm < 1e-6f) {
    printf("simpleCUBLAS test passed.\n");
    exit(EXIT_SUCCESS);
  } else {
    printf("simpleCUBLAS test failed.\n");
    exit(EXIT_FAILURE);
  }
}
```

上面代码的流程图如下：

```
开始  
│  
├──> 初始化CUBLAS库  
│     └──> 检查CUBLAS初始化是否成功  
│         ├──> 成功 -> 分配主机内存给矩阵A、B、C  
│         │       ├──> 分配成功 -> 填充矩阵A、B、C  
│         │       │   └──> 填充成功 -> 分配设备内存给矩阵A、B、C  
│         │       │       ├──> 分配成功 -> 将主机内存中的矩阵复制到设备内存  
│         │       │       │   └──> 复制成功 -> 使用CPU进行矩阵乘法计算  
│         │       │       │       └──> 计算成功 -> 使用CUBLAS在GPU上进行矩阵乘法计算  
│         │       │       │           └──> 计算成功 -> 为结果分配主机内存  
│         │       │       │               └──> 分配成功 -> 将设备内存中的结果读取回主机内存  
│         │       │       │                   └──> 读取成功 -> 计算误差并与参考结果进行比较  
│         │       │       │                       ├──> 参考范数不为0 -> 清理内存  
│         │       │       │                       │   └──> 清理成功 -> 关闭CUBLAS库  
│         │       │       │                       │       └──> 关闭成功 -> 检查误差是否在可接受范围内  
│         │       │       │                       │           ├──> 在可接受范围内 -> 打印通过测试信息 -> 结束  
│         │       │       │                       │           └──> 不在可接受范围内 -> 打印失败测试信息 -> 结束  
│         │       │       │                       └──> 参考范数为0 -> 打印错误信息 -> 结束  
│         │       │       └──> 复制失败 -> 打印错误信息 -> 结束  
│         │       └──> 分配失败 -> 打印错误信息 -> 结束  
│         └──> 失败 -> 打印错误信息 -> 结束  
└──> 失败 -> 打印错误信息 -> 结束  
```

#### 8.3.5 一些更高级的特性

有了上面的基础，我们就可以写一些可以运行在GPU上的代码了。

结束之前，我们再看几个稍微高级一点的特性。

##### `__device__`关键字

之前我们学习核函数的`__global__`关键字。核函数既可以被CPU端调用，也可以被GPU调用。

如果我们想编写只能在GPU上运行的函数，我们就可以使用`__device__`. 

使用`__device__`定义的函数或变量只能在设备代码中使用，无法在主机端代码中使用。在CUDA程序中，通常使用`__host__`和`__device__`关键字来指定函数或变量在主机端和设备端的执行位置。使用`__device__`定义的函数或变量可以在设备代码中被其他函数调用，也可以在主机端使用CUDA API将数据从主机内存传输到设备内存后，由设备上的函数处理。

##### GPU函数的内联

与CPU函数一样，GPU上的函数也可以内联，使用`__forceinline__`关键字。

##### 并发的"?:"三目运算符

在C语言中，"?:"三目运算符只能做一次判断。
现在来到了GPU的世界，并发能力变强了，可以做多次判断了。

我们来看个例子：
```cpp
__device__ __forceinline__ int qcompare(unsigned &val1, unsigned &val2) {
  return (val1 > val2) ? 1 : (val1 == val2) ? 0 : -1;
}
```

#### 8.3.6 PTX汇编

在上一节我们学习SIMD指令的时候，我们基本都要内联汇编。那么在CUDA里面是不是有汇编呢？
答案是肯定的，既然要做性能优化，那么肯定要挖掘一切潜力。
不过，为了避免跟架构过于相关，NVidia给我们提供的是一种中间指令格式PTX（Parallel Thread Execution）。
PTX assembly是CUDA的一种中间汇编语言，它是一种与机器无关的指令集架构（ISA），用于描述GPU上的并行线程执行。PTX assembly可以被编译成特定GPU家族的实际执行的机器码。使用PTX assembly可以实现跨GPU的兼容性和性能优化。

我们来看一段内嵌汇编：
```cpp
static __device__ __forceinline__ unsigned int __qsflo(unsigned int word) {
  unsigned int ret;
  asm volatile("bfind.u32 %0, %1;" : "=r"(ret) : "r"(word));
  return ret;
}
```

其中用到的bfind.u32指令用于查找一个无符号整数中最右边的非零位（即最低有效位），并返回其位位置。该指令将无符号整数作为操作数输入，并将最低有效位的位位置输出到目的操作数中。
"=r"(ret)表示输出寄存器,返回结果保存在ret中。
"r"(word)表示输入寄存器,将参数word作为输入。

最后一点要强调的时，很多时候将代码并行化，并不是简简单单的从CPU转到GPU，而很有可能是要改变算法。

比如，quicksort是一个(nlog(n))的算法，而bitonic sort是个$(nlog^2(n))$的算法。但是，bitonic sort更适合于在GPU加速。所以我们在CPU上的quicksort改成bitonic sort算法会更好一些。

![Bitonic Sort](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/bitsort.png)

### 8.4 CDUA汇编语言

有的同学表示GPU很神秘，不知道它是怎么工作的。其实，GPU的工作原理和CPU是一样的，都是通过指令来控制硬件的。只不过，GPU的指令集和CPU不一样。下面我们就走进GPU的内部，看看如何用汇编来写GPU的程序。

#### 8.4.1 初识PTX与SASS

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gpu-devotes-more-transistors-to-data-processing.png)

从上图我们可以看到，CPU的架构是复杂的几个核组合在一起。而GPU的架构是大量的简单的核组合在一起。因为GPU的每个单元架构都很简单，所以我们需要用CPU去控制GPU的每个单元，让它们协同工作。CPU上的控制代码，我们称为host代码，而GPU每个单元上运行的代码，我们称为device代码。

CUDA的汇编语言分为两种，一种叫做Parallel Thread Execution，简称PTX，另一种叫做Streaming Assembly，简称SASS。PTX是一种中间语言，可以在不同的GPU上运行，而SASS是一种特定的汇编语言，只能在特定的GPU上运行。

下面我们看几个简单的例子来找找体感。

```cpp
__global__ void test(int& c){
    c= blockIdx.x;
}
```

编译成PTX代码：

```ptx
.visible .entry test(int&)(
        .param .u64 test(int&)_param_0
)
{

        ld.param.u64    %rd1, [test(int&)_param_0];
        cvta.to.global.u64      %rd2, %rd1;
        mov.u32         %r1, %ctaid.x;
        st.global.u32   [%rd2], %r1;
        ret;

}
```

PTX中间代码使用ld指令从内存中加载数据，用st指令将数据写入内存。mov用于在寄存器之间传递数据。cvta用于作地址转换。

因为要编译成真正的汇编代码，所以生成代码就要跟硬件架构相关了。我们来看一下sm值和架构的关系：
- sm50: Maxswell 麦克斯韦架构。比如sm52对应GTX 980.
- sm60: Pascal 帕斯卡架构。比如sm61对应GTX 1080. 
- sm70: Volta 伏特架构。比如sm70对应V100. 
- sm75: Turing 图灵架构。比如sm75对应RTX 2080, T4
- sm80: Ampere 安培架构。比如A100, RTX3080
- sm90: Hopper 哈珀架构。比如H100, RTX4080

下面我们将其编译成sm50架构的SASS代码：

```sass
test(int&):
 MOV R1, c[0x0][0x20] 
 MOV R2, c[0x0][0x140] 
 S2R R0, SR_CTAID.X         
 MOV R3, c[0x0][0x144] 
 STG.E [R2], R0 
 NOP 
 NOP 
 EXIT 
```

与PTX不同，麦克斯韦架构下读取内存没有用ld指令，而仍然是MOV指令。而读取特殊寄存器SR_CTAID有专门指令S2R。写全局内存有指令STG. 

我们再看sm60架构汇编：

```sass
test(int&):
 MOV R1, c[0x0][0x20] 
 MOV R2, c[0x0][0x140] 
 S2R R0, SR_CTAID.X         
 MOV R3, c[0x0][0x144] 
 STG.E [R2], R0 
 NOP 
 NOP 
 EXIT
```

跟sm50的没有什么区别。

再看sm70架构汇编：

```sass
test(int&):
 MOV R1, c[0x0][0x28] 
 @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ 
 S2R R5, SR_CTAID.X 
 MOV R2, c[0x0][0x160] 
 MOV R3, c[0x0][0x164] 
 STG.E.SYS [R2], R5 
 EXIT
```

伏特架构的代码出现了线程同步指令SHFL.IDX，这是一种用于线程之间通信的指令，可以在一个线程中访问另一个线程的寄存器值。这里所有的源和目标寄存器都是RZ，这是一个特殊的寄存器，总是包含0。
@!PT表示这个指令只在谓词寄存器PT的值为false时执行，但是PT始终为true，所以这个SHFL.IDX指令不会执行任何实际操作。

继续看图灵架构的:

```sass
test(int&):
 MOV R1, c[0x0][0x28] 
 S2R R0, SR_CTAID.X 
 ULDC.64 UR4, c[0x0][0x160] 
 STG.E.SYS [UR4], R0 
 EXIT 
```

图灵架构增加了ULDC指令，它用来从常量内存中读取到通用寄存器中。

sm80架构sass:

```sass
test(int&):
 MOV R1, c[0x0][0x28] 
 S2R R5, SR_CTAID.X 
 MOV R2, c[0x0][0x160] 
 ULDC.64 UR4, c[0x0][0x118] 
 MOV R3, c[0x0][0x164] 
 STG.E [R2.64], R5 
 EXIT 
```

sm90架构sass:

```sass
test(int&):
 LDC R1, c[0x0][0x28] 
 S2R R5, SR_CTAID.X 
 LDC.64 R2, c[0x0][0x210] 
 ULDC.64 UR4, c[0x0][0x208] 
 STG.E desc[UR4][R2.64], R5 
 EXIT
```

sm80和90没有实质上的变化。

#### 8.4.3 编译和反汇编工具

有了感性认识之后，我们就来让代码运行起来。然后再介绍如何用工具来查看PTX代码和进行sass反汇编。

我们先写一个可以运行起来的CUDA代码，流程如下：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/cuda.png)

首先是设备上的代码：

```cpp
__global__ void sine(double* a) {
    int i = threadIdx.x;
    a[i] = sin(a[i]);
}
```

然后我们加上CPU和GPU之间内存来回复制以及错误检查的代码：

```cpp
// Helper function for using CUDA to add vectors in parallel.
cudaError_t sineWithCuda(double* a, unsigned int size)
{
    double* dev_a = 0;
    cudaError_t cudaStatus;

    // Choose which GPU to run on, change this on a multi-GPU system.
    cudaStatus = cudaSetDevice(0);
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "cudaSetDevice failed!  Do you have a CUDA-capable GPU installed?");
        goto Error;
    }

    cudaStatus = cudaMalloc((void**)&dev_a, size * sizeof(double));
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "cudaMalloc failed!");
        goto Error;
    }

    // Copy input vectors from host memory to GPU buffers.
    cudaStatus = cudaMemcpy(dev_a, a, size * sizeof(double), cudaMemcpyHostToDevice);
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "cudaMemcpy failed!");
        goto Error;
    }

    // Launch a kernel on the GPU with one thread for each element.
    sine << <1, size >> > (dev_a);

    // Check for any errors launching the kernel
    cudaStatus = cudaGetLastError();
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "addKernel launch failed: %s\n", cudaGetErrorString(cudaStatus));
        goto Error;
    }

    // cudaDeviceSynchronize waits for the kernel to finish, and returns
    // any errors encountered during the launch.
    cudaStatus = cudaDeviceSynchronize();
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "cudaDeviceSynchronize returned error code %d after launching addKernel!\n", cudaStatus);
        goto Error;
    }

    // Copy output vector from GPU buffer to host memory.
    cudaStatus = cudaMemcpy(a, dev_a, size * sizeof(double), cudaMemcpyDeviceToHost);
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "cudaMemcpy failed!");
        goto Error;
    }

Error:
    cudaFree(dev_a);

    return cudaStatus;
}
```

最后写一个main函数来调用，以及释放设备：

```cpp
int main()
{
    const int arraySize = 5;

    double s1[arraySize] = { 1, 2, 3, 4, 5 };

    cudaError_t cudaStatus = sineWithCuda(s1, arraySize);
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "sineWithCuda failed!");
        return 1;
    }

    for (int i0 = 0; i0 < arraySize; i0++) {
        std::cout << s1[i0] <<" ";
    }
    std::cout << std::endl;

    // cudaDeviceReset must be called before exiting in order for profiling and
    // tracing tools such as Nsight and Visual Profiler to show complete traces.
    cudaStatus = cudaDeviceReset();
    if (cudaStatus != cudaSuccess) {
        fprintf(stderr, "cudaDeviceReset failed!");
        return 1;
    }

    return 0;
}
```

将文件保存为kernel.cu，编译运行：

```
nvcc kernel.cu
```

如果是在Linux下，就生成a.out；在Windows下就生成a.exe. 

我们还可以通过gencode参数来指定编译成不同的架构的代码，比如：

```bash
nvcc kernel.cu -gencode=arch=compute_52,code=\"sm_52,compute_52\" -gencode=arch=compute_61,code=\"sm_61,compute_61\" -gencode=arch=compute_70,code=\"sm_70,compute_70\" -gencode=arch=compute_75,code=\"sm_75,compute_75\" -gencode=arch=compute_80,code=\"sm_80,compute_80\" -gencode=arch=compute_90,code=\"sm_90,compute_90\"
```

下面我们就可以通过cuobjdump工具来查看ptx和sass代码。

查看PTX代码，以Windows为例：
```bash
cuobjdump --dump-ptx a.exe
```

查看sass代码，还以Windows为例：
```bash
cuobjdump --dump-sass a.exe
```

通过cubin参数，NVCC可以生成cubin文件：
```bash
nvcc kernel.cu -gencode=arch=compute_90,code=sm_90 --cubin
```

注意，cubin只能支持单一一种架构。

我们可以使用nvdisasm来对cubin文件进行反汇编：

```bash
nvdisasm kernel.cubin
```

我们还可以输出cubin的流程图，通过dot工具转换成png格式：

```bash
nvdisasm -bbcfg kernel.cubin | dot -o1.png -Tpng
```

#### 加法指令

下面我们在上面test的基础上，增加一个加法指令：

```cpp
__global__ void test1(int& c){
    c= blockIdx.x+1;
}
```

编译成PTX代码：

```ptx
.visible .entry test1(int&)(
        .param .u64 test1(int&)_param_0
)
{

        ld.param.u64    %rd1, [test1(int&)_param_0];
        cvta.to.global.u64      %rd2, %rd1;
        mov.u32         %r1, %ctaid.x;
        add.s32         %r2, %r1, 1;
        st.global.u32   [%rd2], %r2;
        ret;

}
```

增加了一条add.s32指令，用于32位有符号加法操作。

编译成sm50架构的SASS代码：

```sass
test1(int&):
 MOV R1, c[0x0][0x20] 
 MOV R2, c[0x0][0x140] 
 S2R R0, SR_CTAID.X         
 MOV R3, c[0x0][0x144] 
 IADD32I R0, R0, 0x1 
 STG.E [R2], R0 
 NOP 
 NOP 
 EXIT 
 ```

 add.s32指令被编译成了IADD32I指令。

 sm70的代码就比较有新意了，它使用加乘计算指令IMAD.MOV.U32来代替sm50,sm60的MOV. 计算时改用了三元计算的IADD3指令。当然，对于功能上没有什么影响。

 ```sass
 test1(int&):
 IMAD.MOV.U32 R1, RZ, RZ, c[0x0][0x28] 
 @!PT SHFL.IDX PT, RZ, RZ, RZ, RZ 
 S2R R5, SR_CTAID.X 
 MOV R2, c[0x0][0x160] 
 IMAD.MOV.U32 R3, RZ, RZ, c[0x0][0x164] 
 IADD3 R5, R5, 0x1, RZ 
 STG.E.SYS [R2], R5 
 EXIT
 ```

sm75的时候又变回来了,不过仍然使用IADD3. 

```sass
test1(int&):
 MOV R1, c[0x0][0x28] 
 S2R R0, SR_CTAID.X 
 ULDC.64 UR4, c[0x0][0x160] 
 IADD3 R0, R0, 0x1, RZ 
 STG.E.SYS [UR4], R0 
 EXIT 
```

#### 数学函数

我们下面来求一个平方根。CUDA内置了很多数学函数，我们可以直接调用：

```cpp
__global__ void test2(float& f){
    f = blockIdx.x;
    f = sqrtf(f);
}
```

我们来看下PTX代码：

```ptx
.visible .entry test2(float&)(
        .param .u64 test2(float&)_param_0
)
{

        ld.param.u64    %rd1, [test2(float&)_param_0];
        cvta.to.global.u64      %rd2, %rd1;
        mov.u32         %r1, %ctaid.x;
        cvt.rn.f32.u32  %f1, %r1;
        sqrt.rn.f32     %f2, %f1;
        st.global.f32   [%rd2], %f2;
        ret;

}
```

我们可以看到，sqrtf被编译成了sqrt.rn.f32指令。

到了SASS代码，这一条指令就变得相当有趣了：

```sass
test2(float&):
 MOV R1, c[0x0][0x20] 
 S2R R0, SR_CTAID.X 
 I2F.F32.U32 R0, R0 
 IADD32I R3, R0, -0xd000000 
 MUFU.RSQ R2, R0         
 ISETP.GT.U32.AND P0, PT, R3, c[0x2][0x0], PT 
 @!P0 BRA `(.L_x_0) 
 CAL `($test2(float&)$__cuda_sm20_sqrt_rn_f32_slowpath) 
 MOV R0, R2 
 BRA `(.L_x_1)         
.L_x_0:
 FMUL.FTZ R3, R0, R2 
 FMUL.FTZ R2, R2, 0.5 
 FFMA R0, R3, -R3, R0 
 FFMA R0, R0, R2, R3 
.L_x_1:
 MOV R2, c[0x0][0x140] 
 MOV R3, c[0x0][0x144] 
 STG.E [R2], R0 
 EXIT 
$test2(float&)$__cuda_sm20_sqrt_rn_f32_slowpath:
 LOP.AND.NZ P0, RZ, R0, c[0x2][0x4] 
 @!P0 MOV R2, R0 
 @!P0 RET         
 FSETP.GEU.FTZ.AND P0, PT, R0, RZ, PT 
 @!P0 MOV32I R2, 0x7fffffff 
 @!P0 RET         
 FSETP.GTU.FTZ.AND P0, PT, |R0|, +INF , PT 
 @P0 FADD.FTZ R2, R0, 1 
 @P0 RET         
 FSETP.NEU.FTZ.AND P0, PT, |R0|, +INF , PT 
 @!P0 MOV R2, R0 
 @!P0 RET         
 FFMA R0, R0, 1.84467440737095516160e+19, RZ 
 MUFU.RSQ R2, R0 
 FMUL.FTZ R3, R0, R2 
 FMUL.FTZ R2, R2, 0.5 
 FADD.FTZ R5, -R3.reuse, -RZ 
 FFMA R5, R3, R5, R0 
 FFMA R2, R5, R2, R3 
 FMUL.FTZ R2, R2, 2.3283064365386962891e-10 
 RET  
```

首先，因为sqrtf要求输入是浮点数，所以用I2F.F32.U32指令将整数转换成浮点数。然后，sqrtf的实现是一个迭代算法，需要一个初始值。这里用IADD32I指令将初始值设为-0xd000000。然后，用MUFU.RSQ指令计算初始值的平方根的倒数。

slowpath这一分支主要用于处理异常情况，比如NaN, INF, 0等。
比如
```
FSETP.GTU.FTZ.AND P0, PT, |R0|, +INF , PT 
```
这一句就是用来计算输入是否是正无穷。

这部分代码从sm50到sm90都是一样的。

##### 不能封装成一条指令的数学计算

求平方根可以用一条指令来搞定，但是对于大多数的数学计算来说，并没有这么多指令。最终的实现还是会以汇编序列的方式来实现。

我们先看一个相对简单的，求自然对数的：

```cpp
__global__ void testLog(float& f){
    f = logf(f);
}
```

下面开奖，我们看看翻译成PTX代码：

```ptx
.visible .entry testLog(float&)(
        .param .u64 testLog(float&)_param_0
)
{

        ld.param.u64    %rd2, [testLog(float&)_param_0];
        cvta.to.global.u64      %rd1, %rd2;
        ld.global.f32   %f5, [%rd1];
        setp.lt.f32     %p1, %f5, 0f00800000;
        mul.f32         %f6, %f5, 0f4B000000;
        selp.f32        %f1, %f6, %f5, %p1;
        selp.f32        %f7, 0fC1B80000, 0f00000000, %p1;
        mov.b32         %r1, %f1;
        add.s32         %r2, %r1, -1059760811;
        and.b32         %r3, %r2, -8388608;
        sub.s32         %r4, %r1, %r3;
        mov.b32         %f8, %r4;
        cvt.rn.f32.s32  %f9, %r3;
        mov.f32         %f10, 0f34000000;
        fma.rn.f32      %f11, %f9, %f10, %f7;
        add.f32         %f12, %f8, 0fBF800000;
        mov.f32         %f13, 0f3E1039F6;
        mov.f32         %f14, 0fBE055027;
        fma.rn.f32      %f15, %f14, %f12, %f13;
        mov.f32         %f16, 0fBDF8CDCC;
        fma.rn.f32      %f17, %f15, %f12, %f16;
        mov.f32         %f18, 0f3E0F2955;
        fma.rn.f32      %f19, %f17, %f12, %f18;
        mov.f32         %f20, 0fBE2AD8B9;
        fma.rn.f32      %f21, %f19, %f12, %f20;
        mov.f32         %f22, 0f3E4CED0B;
        fma.rn.f32      %f23, %f21, %f12, %f22;
        mov.f32         %f24, 0fBE7FFF22;
        fma.rn.f32      %f25, %f23, %f12, %f24;
        mov.f32         %f26, 0f3EAAAA78;
        fma.rn.f32      %f27, %f25, %f12, %f26;
        mov.f32         %f28, 0fBF000000;
        fma.rn.f32      %f29, %f27, %f12, %f28;
        mul.f32         %f30, %f12, %f29;
        fma.rn.f32      %f31, %f30, %f12, %f12;
        mov.f32         %f32, 0f3F317218;
        fma.rn.f32      %f35, %f11, %f32, %f31;
        setp.lt.u32     %p2, %r1, 2139095040;
        @%p2 bra        $L__BB3_2;

        mov.f32         %f33, 0f7F800000;
        fma.rn.f32      %f35, %f1, %f33, %f33;

$L__BB3_2:
        setp.eq.f32     %p3, %f1, 0f00000000;
        selp.f32        %f34, 0fFF800000, %f35, %p3;
        st.global.f32   [%rd1], %f34;
        ret;

}
```

这么复杂的逻辑不用翻译成SASS了，在PTX层就已经看晕了。

翻译成sm50架构的SASS代码感觉似乎还简单了点：

```sass
testLog(float&):
 MOV R1, c[0x0][0x20] 
 MOV R2, c[0x0][0x140] 
 MOV R3, c[0x0][0x144] 
 LDG.E R0, [R2] 
 MOV32I R7, 0x3e1039f6 
 FSETP.GEU.AND P0, PT, R0, 1.175494350822287508e-38, PT 
 @!P0 FMUL R0, R0, 8388608 
 IADD32I R4, R0, -0x3f2aaaab 
 ISETP.GE.U32.AND P1, PT, R0.reuse, c[0x2][0x28], PT 
 LOP32I.AND R5, R4, 0xff800000 
 IADD R4, R0, -R5 
 I2F.F32.S32 R5, R5         
 FADD R6, R4, -1 
 FFMA R4, R6.reuse, c[0x2][0x4], R7 
 FFMA R4, R6, R4, c[0x2][0x8] 
 FFMA R4, R6.reuse, R4, c[0x2][0xc] 
 FFMA R4, R6, R4, c[0x2][0x10] 
 FFMA R4, R6.reuse, R4, c[0x2][0x14] 
 FFMA R4, R6.reuse, R4, c[0x2][0x18] 
 FFMA R4, R6.reuse, R4, c[0x2][0x1c] 
 FFMA R7, R6, R4, c[0x2][0x20] 
 SEL R4, RZ, c[0x2][0x0], P0 
 FMUL R7, R6.reuse, R7 
 FFMA R4, R5, 1.1920928955078125e-07, R4 
 FFMA R7, R6, R7, R6 
 @P1 MOV32I R6, 0x7f800000 
 FFMA R7, R4, c[0x2][0x24], R7 
 @P1 FFMA R7, R0, +INF , R6 
 FCMP.NEU R7, R7, -INF , R0 
 STG.E [R2], R7 
 EXIT 
 ```
 
 一直到了sm90，都没有太大变化：

 ```sass
 testLog(float&):
 LDC R1, c[0x0][0x28] 
 LDC.64 R2, c[0x0][0x210] 
 ULDC.64 UR4, c[0x0][0x208] 
 LDG.E R0, desc[UR4][R2.64] 
 HFMA2.MMA R7, -RZ, RZ, 1.5048828125, 33.21875 
 FSETP.GEU.AND P0, PT, R0, 1.175494350822287508e-38, PT 
 @!P0 FMUL R0, R0, 8388608 
 IADD3 R4, R0.reuse, -0x3f2aaaab, RZ 
 ISETP.GE.U32.AND P1, PT, R0, 0x7f800000, PT 
 LOP3.LUT R5, R4, 0xff800000, RZ, 0xc0, !PT 
 IADD3 R4, R0, -R5, RZ 
 I2FP.F32.S32 R5, R5 
 FADD R6, R4, -1 
 FSEL R4, RZ, -23, P0 
 FSETP.NEU.AND P0, PT, R0, RZ, PT 
 FFMA R7, R6.reuse, -R7, 0.14084610342979431152 
 FFMA R4, R5, 1.1920928955078125e-07, R4 
 @P1 MOV R5, 0x7f800000 
 FFMA R7, R6, R7, -0.12148627638816833496 
 FFMA R7, R6, R7, 0.13980610668659210205 
 FFMA R7, R6, R7, -0.16684235632419586182 
 FFMA R7, R6, R7, 0.20012299716472625732 
 FFMA R7, R6, R7, -0.24999669194221496582 
 FFMA R7, R6, R7, 0.33333182334899902344 
 FFMA R7, R6, R7, -0.5 
 FMUL R7, R6, R7 
 FFMA R7, R6, R7, R6 
 FFMA R4, R4, 0.69314718246459960938, R7 
 @P1 FFMA R4, R0, R5, +INF  
 FSEL R5, R4, -INF , P0 
 STG.E desc[UR4][R2.64], R5 
 EXIT 
 ```

好，我们再看一个求正弦值的，我们这次换成双精度的计算：

```cpp
__global__ void testSin(double& d){
    d = sin(d);
}
```

我们看看PTX代码：

```ptx
.visible .entry testSin(double&)(
        .param .u64 testSin(double&)_param_0
)
{

        mov.u64         %SPL, __local_depot4;
        cvta.local.u64  %SP, %SPL;
        ld.param.u64    %rd3, [testSin(double&)_param_0];
        cvta.to.global.u64      %rd1, %rd3;
        add.u64         %rd4, %SP, 0;
        add.u64         %rd2, %SPL, 0;
        ld.global.f64   %fd1, [%rd1];
        {
        mov.b64         {%r4, %temp}, %fd1;
        }
        {
        mov.b64         {%temp, %r5}, %fd1;
        }
        and.b32         %r6, %r5, 2147483647;
        setp.eq.s32     %p1, %r6, 2146435072;
        setp.eq.s32     %p2, %r4, 0;
        and.pred        %p3, %p2, %p1;
        @%p3 bra        $L__BB4_3;
        bra.uni         $L__BB4_1;

$L__BB4_3:
        mov.f64         %fd22, 0d0000000000000000;
        mul.rn.f64      %fd38, %fd1, %fd22;
        mov.u32         %r12, 0;
        bra.uni         $L__BB4_4;

$L__BB4_1:
        mul.f64         %fd13, %fd1, 0d3FE45F306DC9C883;
        cvt.rni.s32.f64         %r12, %fd13;
        st.local.u32    [%rd2], %r12;
        cvt.rn.f64.s32  %fd14, %r12;
        neg.f64         %fd15, %fd14;
        mov.f64         %fd16, 0d3FF921FB54442D18;
        fma.rn.f64      %fd17, %fd15, %fd16, %fd1;
        mov.f64         %fd18, 0d3C91A62633145C00;
        fma.rn.f64      %fd19, %fd15, %fd18, %fd17;
        mov.f64         %fd20, 0d397B839A252049C0;
        fma.rn.f64      %fd38, %fd15, %fd20, %fd19;
        abs.f64         %fd21, %fd1;
        setp.ltu.f64    %p4, %fd21, 0d41E0000000000000;
        @%p4 bra        $L__BB4_4;

        { // callseq 0, 0
        st.param.f64    [param0+0], %fd1;
        st.param.b64    [param1+0], %rd4;
        call.uni (retval0), 
        __internal_trig_reduction_slowpathd, 
        (
        param0, 
        param1
        );
        ld.param.f64    %fd38, [retval0+0];
        } // callseq 0
        ld.local.u32    %r12, [%rd2];

$L__BB4_4:
        and.b32         %r8, %r12, 1;
        shl.b32         %r9, %r12, 3;
        and.b32         %r10, %r9, 8;
        setp.eq.s32     %p5, %r8, 0;
        selp.f64        %fd23, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p5;
        mul.wide.s32    %rd6, %r10, 8;
        mov.u64         %rd7, __cudart_sin_cos_coeffs;
        add.s64         %rd8, %rd7, %rd6;
        ld.global.nc.f64        %fd24, [%rd8+8];
        mul.rn.f64      %fd6, %fd38, %fd38;
        fma.rn.f64      %fd25, %fd23, %fd6, %fd24;
        ld.global.nc.f64        %fd26, [%rd8+16];
        fma.rn.f64      %fd27, %fd25, %fd6, %fd26;
        ld.global.nc.f64        %fd28, [%rd8+24];
        fma.rn.f64      %fd29, %fd27, %fd6, %fd28;
        ld.global.nc.f64        %fd30, [%rd8+32];
        fma.rn.f64      %fd31, %fd29, %fd6, %fd30;
        ld.global.nc.f64        %fd32, [%rd8+40];
        fma.rn.f64      %fd33, %fd31, %fd6, %fd32;
        ld.global.nc.f64        %fd34, [%rd8+48];
        fma.rn.f64      %fd7, %fd33, %fd6, %fd34;
        fma.rn.f64      %fd40, %fd7, %fd38, %fd38;
        @%p5 bra        $L__BB4_6;

        mov.f64         %fd35, 0d3FF0000000000000;
        fma.rn.f64      %fd40, %fd7, %fd6, %fd35;

$L__BB4_6:
        and.b32         %r11, %r12, 2;
        setp.eq.s32     %p6, %r11, 0;
        @%p6 bra        $L__BB4_8;

        mov.f64         %fd36, 0d0000000000000000;
        mov.f64         %fd37, 0dBFF0000000000000;
        fma.rn.f64      %fd40, %fd40, %fd37, %fd36;

$L__BB4_8:
        st.global.f64   [%rd1], %fd40;
        ret;

}
```

而sass实现不负重望地又搞出来一个slowpath函数，代码太长，这里从略。

### 8.5 剪枝和量化

#### 8.5.1 剪枝

以全连接网络为例，网络都是节点和连接节点的边组成的。我们想要压缩网络的大小，就可以通过计算，将一些不重要的节点从图中删除掉，如下图所示：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/prune2.png)

这个算法出自名门，是神经网络获得图灵奖的三巨头之一的Yann LeCun于1989年就研究出来了。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/yann-lecun.jpg)

剪枝虽然看起来简单粗暴，跟企业裁员似的。但是操作起来需要小火慢慢来的，一般在一个训练好的大网络上，一次一次迭代地将最低显著性分数的节点去掉，这样可以让损失变得最小化。
剪枝完成后，还要用剪完的网络进行微调，使得性能更好。
如果一次剪枝之后还达不到要求，这个过程可以重复多次，直到满足对于小模型的需求为止。

比如我们可以取让损失函数变化最大的节点作为被剪掉的节点。也可以采用随机策略随机删掉一个节点。也可以根据网络的结构取中间层进行剪枝，以减少对节点较小的输入输出层的影响。随机剪枝我们也称之为非结构化剪枝，而按模块进行剪枝的称为结构化剪枝。

从剪掉的节点数量上考虑，既可以每一轮被剪掉均匀的数量，也可以开始的时候多剪一些，后面慢慢变少。

最后，如果剪过头影响性能了，我们还可以让部分节点重新生长出来。然后可以再次尝试下一轮剪枝。

在主要框架中，早已经集成好了剪枝功能，比如在PyTorch中，剪枝功能是在torch.nn.utils.prune中定义的。

我们先看L1Unstructured，它是取将最小的L1-Norm值的节点剪掉为策略的剪枝方法：

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune

# 定义一个简单的神经网络
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 3)

    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

# 实例化网络
model = SimpleNN()

# 使用 L1Unstructured 对第一个全连接层进行剪枝
# 剪枝前，查看权重
print("Before pruning:")
print(model.fc1.weight)

# 应用 L1Unstructured 剪枝方法，保留 50% 的权重
prune.l1_unstructured(model.fc1, name='weight', amount=0.5)

# 剪枝后，查看权重
print("After pruning:")
print(model.fc1.weight)
```

我们来看看运行结果。剪枝之前的：
```
Before pruning:
Parameter containing:
tensor([[ 0.1743, -0.1874, -0.1400,  0.1085,  0.0037,  0.2902, -0.0728,  0.2963,
         -0.1599, -0.1496],
        [-0.0496, -0.0954,  0.0030, -0.1801,  0.1881,  0.0244,  0.0629, -0.2639,
         -0.0755, -0.2218],
        [-0.2467, -0.1869,  0.0836,  0.0503,  0.2446, -0.2809,  0.1273,  0.0471,
         -0.1552,  0.0118],
        [-0.2023, -0.2786, -0.2742,  0.0381, -0.0608,  0.0737, -0.1440, -0.0835,
         -0.0172,  0.1741],
        [-0.1663, -0.1361,  0.2251, -0.1459,  0.1826, -0.1802,  0.2597,  0.2781,
          0.1729, -0.1752]], requires_grad=True)
```

剪枝之后的：
```
After pruning:
tensor([[ 0.1743, -0.1874, -0.0000,  0.0000,  0.0000,  0.2902, -0.0000,  0.2963,
         -0.1599, -0.0000],
        [-0.0000, -0.0000,  0.0000, -0.1801,  0.1881,  0.0000,  0.0000, -0.2639,
         -0.0000, -0.2218],
        [-0.2467, -0.1869,  0.0000,  0.0000,  0.2446, -0.2809,  0.0000,  0.0000,
         -0.0000,  0.0000],
        [-0.2023, -0.2786, -0.2742,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,
         -0.0000,  0.1741],
        [-0.1663, -0.0000,  0.2251, -0.0000,  0.1826, -0.1802,  0.2597,  0.2781,
          0.1729, -0.1752]], grad_fn=<MulBackward0>)
```

我们可以看到，一半的权重值已经被剪成0了。

我们也可以使用torch.nn.utils.prune.random_unstructured函数来实现随机剪枝：

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune

# 定义一个简单的神经网络
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 3)

    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

# 实例化网络
model = SimpleNN()

# 使用 random_unstructured 对第一个全连接层进行剪枝
# 剪枝前，查看权重
print("Before pruning:")
print(model.fc1.weight)

# 应用 random_unstructured 剪枝方法，保留 50% 的权重
prune.random_unstructured(model.fc1, name='weight', amount=0.5)

# 剪枝后，查看权重
print("After pruning:")
print(model.fc1.weight)
```

随机剪枝的结果与上面的L1不同在于，每一次运行的结果是不相同的。

说完非结构化的，我们再来看结构化的。
结构化可以定义维，比如将第一维的都剪掉，我们看例子：

```python
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune


# 定义一个简单的神经网络
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 10)
        self.fc2 = nn.Linear(10, 5)
        self.fc3 = nn.Linear(5, 3)

    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        x = self.fc3(x)

        return x


# 实例化网络
model = SimpleNN()

# 使用 L1Unstructured 对第一个全连接层进行剪枝
# 剪枝前，查看权重
print("Before pruning:")
print(model.fc1.weight)

# 应用 random_structured 剪枝方法，保留 50% 的权重
prune.random_structured(model.fc1, name='weight',amount=0.5, dim=1)

# 剪枝后，查看权重
print("After pruning:")
print(model.fc1.weight)
```

我们来看运行结果：
```
Before pruning:
Parameter containing:
tensor([[ 1.8203e-01, -2.4652e-02, -1.8870e-01, -2.0959e-01, -1.4791e-01,
          1.7911e-01,  2.1782e-01,  2.0245e-01, -7.1234e-02, -2.4723e-01],
        [ 2.0795e-01, -2.4798e-01, -6.2147e-03, -2.7634e-01, -3.6599e-02,
         -1.2186e-01, -9.3189e-02,  1.0184e-01,  9.8952e-02, -1.6860e-01],
        [ 8.2882e-03, -9.2586e-02,  1.1309e-01,  1.3828e-01,  1.5534e-01,
         -6.5238e-02, -2.4512e-01, -1.8104e-01, -1.7913e-01, -6.7663e-02],
        [ 1.6401e-01,  1.5702e-01, -2.7113e-01, -1.1145e-01, -3.8372e-02,
          1.9320e-01, -1.1800e-01, -1.6497e-03, -2.7625e-01,  2.4986e-01],
        [ 9.3429e-02, -1.9261e-01,  1.1799e-02, -3.1452e-01,  3.8984e-02,
          2.5882e-01,  1.7893e-01, -3.0125e-01,  2.1812e-01,  3.0290e-01],
        [-9.5934e-05, -8.3178e-02,  1.2058e-01, -2.8590e-01,  2.9342e-01,
         -1.3845e-01, -2.2089e-01, -9.1614e-02,  2.7203e-01, -1.7542e-01],
        [ 1.5185e-02, -2.5059e-01,  2.8496e-01,  2.6329e-01,  8.1400e-02,
          2.1947e-01, -2.0651e-01,  2.3151e-01,  2.5052e-01,  7.7183e-02],
        [-4.8820e-02, -7.7806e-02, -2.2073e-01,  5.1517e-03, -2.3736e-01,
         -1.4963e-01, -2.0640e-01, -1.7726e-01, -2.6281e-01, -6.7827e-02],
        [-6.8090e-02,  3.0740e-01,  3.0408e-01,  1.8012e-01,  8.3739e-02,
         -2.3268e-01,  2.1999e-02,  1.3235e-01,  4.1730e-03,  2.9417e-01],
        [-3.3793e-02,  2.4021e-01, -6.9832e-02, -2.7820e-01, -1.7553e-01,
          9.3053e-02, -2.2394e-01, -2.2041e-01,  1.6536e-01, -6.8046e-02]],
       requires_grad=True)
After pruning:
tensor([[ 1.8203e-01, -0.0000e+00, -1.8870e-01, -2.0959e-01, -1.4791e-01,
          1.7911e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 2.0795e-01, -0.0000e+00, -6.2147e-03, -2.7634e-01, -3.6599e-02,
         -1.2186e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],
        [ 8.2882e-03, -0.0000e+00,  1.1309e-01,  1.3828e-01,  1.5534e-01,
         -6.5238e-02, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [ 1.6401e-01,  0.0000e+00, -2.7113e-01, -1.1145e-01, -3.8372e-02,
          1.9320e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00],
        [ 9.3429e-02, -0.0000e+00,  1.1799e-02, -3.1452e-01,  3.8984e-02,
          2.5882e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-9.5934e-05, -0.0000e+00,  1.2058e-01, -2.8590e-01,  2.9342e-01,
         -1.3845e-01, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],
        [ 1.5185e-02, -0.0000e+00,  2.8496e-01,  2.6329e-01,  8.1400e-02,
          2.1947e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-4.8820e-02, -0.0000e+00, -2.2073e-01,  5.1517e-03, -2.3736e-01,
         -1.4963e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],
        [-6.8090e-02,  0.0000e+00,  3.0408e-01,  1.8012e-01,  8.3739e-02,
         -2.3268e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
        [-3.3793e-02,  0.0000e+00, -6.9832e-02, -2.7820e-01, -1.7553e-01,
          9.3053e-02, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00]],
       grad_fn=<MulBackward0>)
```

大家看到那一列整齐的正0和负0了么。当然，这一维全0了，仍然不够50%，其他维还是要再出一些名额的。

#### 8.6 量化

在ARM处理器大核都要把32位计算模块砍掉的情况下，64位计算已经成为了哪怕是手机上的主流。最不济也可以使用32位的指令。在深度学习的计算中，我们主要使用也是32位精度的浮点计算。

当模型变大后，如果我们可以将32位浮点运算变成8位整数运算，甚至极端情况下搞成4位整数运算，则不管是在存储还是计算上都节省大量的资源。

量化的算法很容易想到，压缩时就是把一个区间的值都映射到一个离散值上。还原时就想办法恢复成之前的值。
最极端的情况下就是二值量化，这就退化成符号函数或者是激活函数了。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/quantized_value.png)

对照上图，量化要做的事情，就是尽可能有效地利用有限的量化后的存储空间，让原始数据的损失最小。
如果这么说比较抽象的话，我们边写代码，边举例子说明。

##### 8.6.1 固定大小量化方法

在PyTorch中，量化函数quantize_per_tensor主要需要三个参数：缩放因子，零点和量化类型。
类型我们取8位无符号数。

缩放因子的公式：scale = (max_val - min_val) / (qmax - qmin)
零点的计算公式：zero_point = qmin - round(min_val / scale)

对于8位无符号数的话，qmax = 256, qmin = 0。

我们先随便写一个找找感觉：

```python
import torch
x = torch.rand(2, 3, dtype=torch.float32)
print(x)

xq = torch.quantize_per_tensor(x, scale=0.5, zero_point=0, dtype=torch.quint8)
print(xq)
```

如果想看到量化之后的整数表示，我们可以通过int_repr方法来查看。
```python
xq.int_repr()
```

最后，我们可以用dequantize来解量化：
```python
xd = xq.dequantize()
print(xd)
```

torch.rand是取0到1之间的浮点数，那么max_val为1.0，min_val为0.0. 
scale就是1/256. 

我们把上面的串在一起：
```python
import torch
x = torch.rand(2, 3, dtype=torch.float32)
print(x)

xq = torch.quantize_per_tensor(x, scale=1/256, zero_point=0, dtype=torch.quint8)
print(xq)

# 看整数的表示：
print(xq.int_repr())

# 解量化
xd = xq.dequantize()
print(xd)
```

随机生成的值是这样的：
```
tensor([[0.8779, 0.2919, 0.6965],
        [0.8018, 0.2809, 0.0910]])
```

量化之后的值为整数值为：
```
tensor([[225,  75, 178],
        [205,  72,  23]], dtype=torch.uint8)
```

解量化之后的结果为：
```
tensor([[0.8789, 0.2930, 0.6953],
        [0.8008, 0.2812, 0.0898]])
```
基本上还是可以保证小数点之后两位左右的准确率。

如果我们还想省得更多，采用4位做量化会是什么样的结果呢？

4位的话，scale就变成1/16了：

```python
import torch

x = torch.tensor([[0.8779, 0.2919, 0.6965],
        [0.8018, 0.2809, 0.0910]])
print(x)

xq = torch.quantize_per_tensor(x, scale=1/16, zero_point=0, dtype=torch.quint8)
print(xq)

# 看整数的表示：
print(xq.int_repr())

# 解量化
xd = xq.dequantize()
print(xd)
```

输出结果如下：
```
tensor([[0.8779, 0.2919, 0.6965],
        [0.8018, 0.2809, 0.0910]])
tensor([[0.8750, 0.3125, 0.6875],
        [0.8125, 0.2500, 0.0625]], size=(2, 3), dtype=torch.quint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.0625, zero_point=0)
tensor([[14,  5, 11],
        [13,  4,  1]], dtype=torch.uint8)
tensor([[0.8750, 0.3125, 0.6875],
        [0.8125, 0.2500, 0.0625]])
```

可以看到，当只有4位量化的时候，只能做到1位小数差不多了。

##### 8.6.2 自动调整区间的量化

不过，观察上面的量化结果，我们发现，我们取的max_val和min_val都偏保守。

以上面4位量化为例：
```
tensor([[14,  5, 11],
        [13,  4,  1]], dtype=torch.uint8)
```
我们上没有用到15，下没有用到0，明显是浪费了一点精度。

为了更充分发挥潜力，我们可以计算更精确一些。
当然，这事情不需要手工搞，PyTorch为我们准备好了torch.quantization.MinMaxObserver，我们只要设定好范围，就可以调用calculate_qparams方法来自动计算缩放因子和零点位置：

```python
observer = torch.quantization.MinMaxObserver(quant_min=0,quant_max=15)
observer(x)

scale, zero_point = observer.calculate_qparams()
print(scale, zero_point)
```

跟上面的例子组合一下：
```
import torch

x = torch.tensor([[0.8779, 0.2919, 0.6965],
        [0.8018, 0.2809, 0.0910]])
print(x)

observer = torch.quantization.MinMaxObserver(quant_min=0,quant_max=15)
observer(x)

scale, zero_point = observer.calculate_qparams()
print(scale, zero_point)

xq = torch.quantize_per_tensor(x, scale=scale, zero_point=zero_point, dtype=torch.quint8)
print(xq)

# 看整数的表示：
print(xq.int_repr())

# 解量化
xd = xq.dequantize()
print(xd)
```

我们看一下结果：
```
tensor([[0.8779, 0.2919, 0.6965],
        [0.8018, 0.2809, 0.0910]])
tensor([0.0585]) tensor([0], dtype=torch.int32)
tensor([[0.8779, 0.2926, 0.7023],
        [0.8194, 0.2926, 0.1171]], size=(2, 3), dtype=torch.quint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.058526668697595596,
       zero_point=0)
tensor([[15,  5, 12],
        [14,  5,  2]], dtype=torch.uint8)
tensor([[0.8779, 0.2926, 0.7023],
        [0.8194, 0.2926, 0.1171]])
```

scale从0.0625降低到了0.058526668697595596，能提升6%吧。

更主要的是，随着可以使用动态监控，我们以后不管针对什么样的数据分布，都可以用更加符合大小的值来进行量化。

对了，在2023年4月20日这个时间点，PyTorch的量化功能还处于beta阶段。后面正式发布了我再更新。

##### 8.6.3 量化的硬件支持

经过上面的学习，我们对量化的原理和编程已经有了一个比较清晰的了解。

不过，在实际应用中并没有这么简单。在实际硬件中，如果是只有CPU的情况下，我们使用FBGEMM库来实现加速。

```python
import torch

# define a floating point model where some layers could be statically quantized
class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # QuantStub converts tensors from floating point to quantized
        self.quant = torch.ao.quantization.QuantStub()
        self.conv = torch.nn.Conv2d(1, 1, 1)
        self.relu = torch.nn.ReLU()
        # DeQuantStub converts tensors from quantized to floating point
        self.dequant = torch.ao.quantization.DeQuantStub()

    def forward(self, x):
        # manually specify where tensors will be converted from floating
        # point to quantized in the quantized model
        x = self.quant(x)
        x = self.conv(x)
        x = self.relu(x)
        # manually specify where tensors will be converted from quantized
        # to floating point in the quantized model
        x = self.dequant(x)
        return x

# create a model instance
model_fp32 = M()

# model must be set to eval mode for static quantization logic to work
model_fp32.eval()

# attach a global qconfig, which contains information about what kind
# of observers to attach. Use 'x86' for server inference and 'qnnpack'
# for mobile inference. Other quantization configurations such as selecting
# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques
# can be specified here.
# Note: the old 'fbgemm' is still available but 'x86' is the recommended default
# for server inference.
# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')
model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')

# Fuse the activations to preceding layers, where applicable.
# This needs to be done manually depending on the model architecture.
# Common fusions include `conv + relu` and `conv + batchnorm + relu`
model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])

# Prepare the model for static quantization. This inserts observers in
# the model that will observe activation tensors during calibration.
model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)

# calibrate the prepared model to determine quantization parameters for activations
# in a real world setting, the calibration would be done with a representative dataset
input_fp32 = torch.randn(4, 1, 4, 4)
model_fp32_prepared(input_fp32)

# Convert the observed model to a quantized model. This does several things:
# quantizes the weights, computes and stores the scale and bias value to be
# used with each activation tensor, and replaces key operators with quantized
# implementations.
model_int8 = torch.ao.quantization.convert(model_fp32_prepared)

# run the model, relevant calculations will happen in int8
res = model_int8(input_fp32)
print(res)
```

如果是在手机上运行，就要使用qnnpack库来替换掉x86或者fbgemm：
```python
model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')
```

FBGEMM和qnnpack都是矩阵计算的加速库。

##### 8.6.4 fbgemm库

FBGEMM (Facebook's Gemm Library) 是一个高性能、低精度矩阵乘法库，适用于服务器端的 x86 架构。它广泛应用于深度学习、推荐系统等领域。

我们来写个例子理解FBGEMM是什么。
先下载FBGEMM的代码：

```
git clone --recursive https://github.com/pytorch/FBGEMM.git
cd FBGEMM
mkdir build && cd build
cmake ..
make
make install
```

为了可以编译成功，我们还需要一个BLAS库，比如使用Intel的MKL库，或者是OpenBLAS库。后面我们会详细介绍这些支持并行开发的库。

编译成功之后，我们写个调用fbgemm进行矩阵计算的例子：

```cpp
#include <iostream>
#include "fbgemm/Fbgemm.h"

int main() {
  // 定义矩阵维度
  int M = 3;
  int N = 2;
  int K = 4;

  // 定义矩阵 A 和 B
  float A[M * K] = {1, 2, 3, 4,
                    5, 6, 7, 8,
                    9, 10, 11, 12};

  float B[K * N] = {1, 2,
                    3, 4,
                    5, 6,
                    7, 8};

  // 初始化 C 矩阵
  float C[M * N] = {0};

  // 定义 FBGEMM 参数
  fbgemm::matrix_op_t A_op = fbgemm::matrix_op_t::NoTranspose;
  fbgemm::matrix_op_t B_op = fbgemm::matrix_op_t::NoTranspose;

  // 执行矩阵乘法运算
  fbgemm::cblas_sgemm_ref(M, N, K, A, K, A_op, B, N, B_op, C, N);

  // 打印结果矩阵 C
  std::cout << "矩阵 C: " << std::endl;
  for (int i = 0; i < M; ++i) {
    for (int j = 0; j < N; ++j) {
      std::cout << C[i * N + j] << " ";
    }
    std::cout << std::endl;
  }

  return 0;
}
```

编译运行：

```
g++ -std=c++11 -I/path/to/FBGEMM/include -L/path/to/FBGEMM/lib fbgemm_example.cpp -o fbgemm_example -lfbgemm
```

qnnpack库现在已经是PyTorch的一部分，我们就不多做介绍了。


## 第十五章 机器学习应用初步

### 15.1 推荐算法

推荐算法是机器学习中的一个重要应用领域。推荐算法的目标是根据用户的历史行为和偏好，向用户推荐可能感兴趣的物品。推荐算法在电商、社交网络、音乐、视频等领域都有广泛的应用。

推荐算法的核心是协同过滤（Collaborative Filtering）技术。协同过滤是一种基于用户行为数据的推荐算法，它通过分析用户的历史行为，发现用户之间的相似性，从而向用户推荐可能感兴趣的物品。

协同过滤算法主要有两种类型：基于用户的协同过滤和基于物品的协同过滤。基于用户的协同过滤是通过分析用户之间的相似性，向用户推荐和他们相似的用户喜欢的物品。基于物品的协同过滤是通过分析物品之间的相似性，向用户推荐和他们喜欢的物品相似的物品。

本节我们以MovieLens数据集为例，介绍如何使用Python实现基于用户的协同过滤推荐算法。

MovieLens 数据集是一系列由GroupLens Research实验室收集和维护的电影评分数据集，主要用于研究和开发推荐系统。这些数据集包含了用户对电影的评分、用户的观影记录以及电影的元数据信息，如电影类型、导演、演员等。

MovieLens 数据集有多个版本，不同的版本包含不同数量的用户、电影和评分数据。以下是一些常见的MovieLens数据集版本及其特点：

ml-100k：包含100,000个评分，涉及1,000名用户和1,700部电影。
ml-1m：包含1,000,000个评分，涉及6,000名用户和4,000部电影。
ml-10m：包含10,000,000个评分，涉及72,000名用户和10,000部电影。
ml-20m：包含20,000,000个评分，涉及138,000名用户和27,000部电影。
ml-25m：包含25,000,000个评分，涉及的用户和电影数量更多。
MovieLens 数据集通常用于评估和比较不同的推荐算法，因为它们提供了丰富的用户行为数据和电影特征信息。这些数据集对于研究个性化推荐、协同过滤、矩阵分解等技术非常有价值。

我们以ml-100k数据集为例，介绍如何使用Python实现基于用户的协同过滤推荐算法。

ml-100k的数据的格式如下：

```
user_id	item_id	rating	timestamp
0	0	0	3	881250949
1	1	1	3	891717742
2	2	2	1	878887116
3	3	3	2	880606923
4	4	4	1	886397596
5	5	5	4	884182806
6	6	6	2	881171488
7	7	7	5	891628467
8	8	8	3	886324817
9	9	9	3	883603013
```

其中，user_id是用户的ID，item_id是电影的ID，rating是用户对电影的评分，timestamp是评分的时间戳。

首先，我们加载数据集：

```python
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

# 下载和加载 MovieLens 数据集
url = 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'
df = pd.read_csv(url, sep='\t', names=['user_id', 'item_id', 'rating', 'timestamp'])

# 将用户ID和物品ID转换为从0开始的索引
user_ids = df['user_id'].unique()
item_ids = df['item_id'].unique()
user2idx = {user: idx for idx, user in enumerate(user_ids)}
item2idx = {item: idx for idx, item in enumerate(item_ids)}

df['user_id'] = df['user_id'].apply(lambda x: user2idx[x])
df['item_id'] = df['item_id'].apply(lambda x: item2idx[x])

# 创建一个自定义数据集
class MovieLensDataset(Dataset):
    def __init__(self, dataframe):
        self.df = dataframe

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        user = self.df.iloc[idx, 0]
        item = self.df.iloc[idx, 1]
        rating = self.df.iloc[idx, 2]
        return torch.tensor(user, dtype=torch.long), torch.tensor(item, dtype=torch.long), torch.tensor(rating, dtype=torch.float)

dataset = MovieLensDataset(df)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
```

然后我们定义一个简单的矩阵分解模型，其中包括用户和物品的嵌入（embedding）。矩阵分解（Matrix Factorization）算法，这是一种常用于推荐系统的技术。矩阵分解的目标是将用户-物品评分矩阵分解为两个低秩矩阵：用户因子矩阵和物品因子矩阵，从而可以预测缺失的评分。

代码讲解我写在注释里了：

```python
import torch.nn as nn
import torch.nn.functional as F

class MatrixFactorization(nn.Module):
    def __init__(self, n_users, n_items, n_factors=20):
        super(MatrixFactorization, self).__init__()
        # 这是一个嵌入层，用于将用户ID映射到一个低维度的向量（即用户因子）。n_users是用户的总数，n_factors是每个用户因子的维度。
        self.user_factors = nn.Embedding(n_users, n_factors)   
        # 这也是一个嵌入层，用于将物品ID映射到一个低维度的向量（即物品因子）。n_items是物品的总数，n_factors是每个物品因子的维度。
        self.item_factors = nn.Embedding(n_items, n_factors)  

    def forward(self, user, item):
        user_embedding = self.user_factors(user)
        item_embedding = self.item_factors(item)
        # 计算用户因子向量和物品因子向量的逐元素乘积，并对结果向量沿第一个维度（即每个样本的评分）求和，得到最终的评分预测
        return (user_embedding * item_embedding).sum(1)
```

这种矩阵分解方法的基本思想是，如果某个用户对一个物品的评分较高，那么这个用户的因子向量和这个物品的因子向量在某种度量下应该是相似的。通过这种方式，我们可以预测用户对未评分的物品的喜好程度。

然后我们训练这个模型：
    
```python
def train(dataloader, model, loss_fn, optimizer, device):
    model.train()
    total_loss = 0
    for user, item, rating in dataloader:
        user, item, rating = user.to(device), item.to(device), rating.to(device)
        optimizer.zero_grad()
        prediction = model(user, item)
        loss = loss_fn(prediction, rating)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)

# 初始化模型
n_users = len(user_ids)
n_items = len(item_ids)
n_factors = 20
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = MatrixFactorization(n_users, n_items, n_factors).to(device)
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# 训练模型
n_epochs = 10
for epoch in range(n_epochs):
    train_loss = train(dataloader, model, loss_fn, optimizer, device)
    print(f'Epoch {epoch+1}/{n_epochs}, Loss: {train_loss:.4f}')
```

最后我们可以使用训练好的模型进行预测：

```python
from sklearn.metrics import mean_squared_error

def evaluate(dataloader, model, device):
    model.eval()
    actuals = []
    predictions = []
    with torch.no_grad():
        for user, item, rating in dataloader:
            user, item, rating = user.to(device), item.to(device), rating.to(device)
            prediction = model(user, item)
            actuals.extend(rating.cpu().numpy())
            predictions.extend(prediction.cpu().numpy())
    mse = mean_squared_error(actuals, predictions)
    return mse

test_mse = evaluate(dataloader, model, device)
print(f'Test MSE: {test_mse:.4f}')
```

运行结果如下：

```
Epoch 1/10, Loss: 18.9122
Epoch 2/10, Loss: 2.2563
Epoch 3/10, Loss: 1.1144
Epoch 4/10, Loss: 0.9866
Epoch 5/10, Loss: 0.9637
Epoch 6/10, Loss: 0.9208
Epoch 7/10, Loss: 0.8711
Epoch 8/10, Loss: 0.8352
Epoch 9/10, Loss: 0.8078
Epoch 10/10, Loss: 0.7798
Test MSE: 0.6653
```

这里我们使用均方误差（Mean Squared Error，MSE）作为评估指标，MSE 是预测值和真实值之间差值的平方和的均值。MSE 越小，模型的预测效果越好。


### 15.2 语音识别

我们首先来看看语音识别模型如使用，我们以Open AI的Whisper模型为例，这是一个基于Transformer的语音识别模型。

#### 15.2.1 语音识别模型Whisper的用法

Whisper是OpenAI开源的模型。它的用法非常简单，只要安装好相关的库，就可以直接用命令行来调用了。

安装就一个库：

```bash
pip install -U openai-whisper
```

然后就可以直接用命令行来调用了：

```bash
whisper va1.mp3 --language Chinese
```

我们还可以用model参数来选择模型，比如有10GB以上显存就可以选择使用large模型：

```bash
whisper va2.mp3 --model large --language Chinese
```

默认是small模型。还可以选择tiny, base, medium, large-v1和large-v2. 

如果是遇到视频的话，那么就用ffmpeg工具将视频中的音频部分提取出来。

#### 15.2.2 Whisper的原理

虽然从表象上，声音和文本还是非常不同的。但是到了模型这一层，一切又回到了我们熟悉的样子。

首先是层归一化：

```python
class LayerNorm(nn.LayerNorm):
    def forward(self, x: Tensor) -> Tensor:
        return super().forward(x.float()).type(x.dtype)
```

只做了一件事情，就是将泛型的x转成浮点数再前向计算。

再看它的全连接网络，就是PyTorch的线性网络的一个马甲：

```python
class Linear(nn.Linear):
    def forward(self, x: Tensor) -> Tensor:
        return F.linear(
            x,
            self.weight.to(x.dtype),
            None if self.bias is None else self.bias.to(x.dtype),
        )

```

这段代码定义了一个名为 Linear 的类，它继承自 nn.Linear 类。这个类重写了父类的 forward 方法，该方法接受一个张量 x 作为输入，并返回一个张量作为输出。
在 forward 方法中，首先调用了 F.linear 函数，该函数接受三个参数：输入张量 x，权重矩阵 self.weight.to(x.dtype) 和偏置向量 self.bias.to(x.dtype)。其中，权重矩阵和偏置向量都被转换为与输入张量相同的数据类型。
如果偏置向量为 None，则第三个参数传递的是 None。否则，传递转换后的偏置向量。

然后是对卷积的封装：

```python
class Conv1d(nn.Conv1d):
    def _conv_forward(
        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]
    ) -> Tensor:
        return super()._conv_forward(
            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)
        )
```

跟上面是一样复刻的，就不多解释了。

接着，熟悉的东西来了，位置嵌入：

```python
def sinusoids(length, channels, max_timescale=10000):
    """Returns sinusoids for positional embedding"""
    assert channels % 2 == 0
    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)
    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))
    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]
    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)
```

代码中使用了一个断言语句来确保 channels 是偶数。然后计算出 log_timescale_increment，它表示对数时间尺度的增量。接下来，使用 torch.exp 函数和 torch.arange 函数计算出逆时间尺度 inv_timescales。
然后，代码计算出缩放后的时间 scaled_time，它是一个二维张量，其中每一行都是一个时间序列。最后，使用 torch.cat 函数将缩放后的时间的正弦值和余弦值拼接在一起，并返回结果。

再然后，多头注意力果然就登场了：

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, n_state: int, n_head: int):
        super().__init__()
        self.n_head = n_head
        self.query = Linear(n_state, n_state)
        self.key = Linear(n_state, n_state, bias=False)
        self.value = Linear(n_state, n_state)
        self.out = Linear(n_state, n_state)

    def forward(
        self,
        x: Tensor,
        xa: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
        kv_cache: Optional[dict] = None,
    ):
        q = self.query(x)

        if kv_cache is None or xa is None or self.key not in kv_cache:
            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;
            # otherwise, perform key/value projections for self- or cross-attention as usual.
            k = self.key(x if xa is None else xa)
            v = self.value(x if xa is None else xa)
        else:
            # for cross-attention, calculate keys and values once and reuse in subsequent calls.
            k = kv_cache[self.key]
            v = kv_cache[self.value]

        wv, qk = self.qkv_attention(q, k, v, mask)
        return self.out(wv), qk

    def qkv_attention(
        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None
    ):
        n_batch, n_ctx, n_state = q.shape
        scale = (n_state // self.n_head) ** -0.25
        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale
        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale
        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)

        qk = q @ k
        if mask is not None:
            qk = qk + mask[:n_ctx, :n_ctx]
        qk = qk.float()

        w = F.softmax(qk, dim=-1).to(q.dtype)
        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()
```

MultiHeadAttention类的构造函数接受两个参数：n_state和n_head。n_state是每个头的维度，而n_head是头的数量。在初始化过程中，该类创建了四个线性层，分别用于查询（query）、键（key）、值（value）和输出（out）。

forward方法是该类的核心，它接受输入张量x、可选的附加输入xa、掩码mask和键值缓存kv_cache。这个方法首先通过查询线性层处理输入x，得到查询张量q。然后，根据kv_cache和xa的存在与否，决定是重新计算键和值张量，还是从缓存中获取它们。

如果kv_cache为空或者xa为空，或者self.key不在kv_cache中，那么就会计算新的键和值张量。否则，会从kv_cache中获取预先计算的键和值张量。

接下来，调用qkv_attention方法来计算注意力权重和加权后的值张量。这个方法首先对查询、键和值张量进行重塑和转置，以便在每个头上并行计算注意力。然后，计算查询和键的点积，应用掩码（如果提供），并对结果应用softmax函数以获得注意力权重。最后，使用权重对值张量进行加权求和，得到加权后的值张量。

forward方法返回加权后的值张量经过输出线性层的处理结果，以及查询和键的点积（作为额外的输出）。

这个多头注意力机制允许模型在不同的表示子空间中同时关注输入的不同部分，从而提高了模型的表达能力和性能。

然后是将多头注意力封装为残差块。如果不记得什么是残差块的，我们复习一下结构图：

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/transformers.png)

```python
class ResidualAttentionBlock(nn.Module):
    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):
        super().__init__()

        self.attn = MultiHeadAttention(n_state, n_head)
        self.attn_ln = LayerNorm(n_state)

        self.cross_attn = (
            MultiHeadAttention(n_state, n_head) if cross_attention else None
        )
        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None

        n_mlp = n_state * 4
        self.mlp = nn.Sequential(
            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)
        )
        self.mlp_ln = LayerNorm(n_state)

    def forward(
        self,
        x: Tensor,
        xa: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
        kv_cache: Optional[dict] = None,
    ):
        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]
        if self.cross_attn:
            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]
        x = x + self.mlp(self.mlp_ln(x))
        return x

```

ResidualAttentionBlock类的构造函数接受三个参数：n_state、n_head和cross_attention。n_state是每个头的维度，n_head是头的数量，cross_attention是一个布尔值，指示是否包含交叉注意力机制。

在初始化过程中，该类创建了一个多头注意力模块self.attn，一个层归一化模块self.attn_ln，一个可选的交叉注意力模块self.cross_attn（如果cross_attention为真），一个可选的层归一化模块self.cross_attn_ln（如果cross_attention为真），以及一个前馈神经网络模块self.mlp。前馈神经网络由两个线性层和一个GELU激活函数组成。

forward方法是该类的核心，它接受输入张量x、可选的附加输入xa、掩码mask和键值缓存kv_cache。这个方法首先通过自注意力模块和层归一化处理输入x，然后将结果与原始输入相加，实现残差连接。

如果存在交叉注意力模块，那么还会对x应用交叉注意力模块和层归一化，并将结果与之前的输出相加。

最后，将输出通过前馈神经网络和层归一化，并将结果与之前的输出相加，再次实现残差连接。

这个ResidualAttentionBlock模块通过残差连接和层归一化增强了模型的训练稳定性，并通过多头注意力和前馈神经网络提供了丰富的表达能力。

Whisper的编码器，编进来的是语音：

```python
class AudioEncoder(nn.Module):
    def __init__(
        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int
    ):
        super().__init__()
        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)
        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)
        self.register_buffer("positional_embedding", sinusoids(n_ctx, n_state))

        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(
            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]
        )
        self.ln_post = LayerNorm(n_state)

    def forward(self, x: Tensor):
        """
        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)
            the mel spectrogram of the audio
        """
        x = F.gelu(self.conv1(x))
        x = F.gelu(self.conv2(x))
        x = x.permute(0, 2, 1)

        assert x.shape[1:] == self.positional_embedding.shape, "incorrect audio shape"
        x = (x + self.positional_embedding).to(x.dtype)

        for block in self.blocks:
            x = block(x)

        x = self.ln_post(x)
        return x
```

编码器这边，初始化了2个卷积层conv1和conv2,用于降维和下采样语音数据。
然后初始化了一个positional_embedding,这是个位置编码,用来表示时间步信息。
再初始化了多个残差自注意力模块ResidualAttentionBlock,把编码通过自注意力块传递。

forward过程:

- 将语音数据传入conv1、conv2提取特征
- 加上positional_embedding表示时间步
- 传入自注意力ResidualAttentionBlock
- LayerNorm归一化
- 输出编码结果

而解码器是输出的文本，就没有卷积网络什么事儿了，就是残差多头注意力块：

```python
class TextDecoder(nn.Module):
    def __init__(
        self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int
    ):
        super().__init__()

        self.token_embedding = nn.Embedding(n_vocab, n_state)
        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))

        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(
            [
                ResidualAttentionBlock(n_state, n_head, cross_attention=True)
                for _ in range(n_layer)
            ]
        )
        self.ln = LayerNorm(n_state)

        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)
        self.register_buffer("mask", mask, persistent=False)

    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):
        """
        x : torch.LongTensor, shape = (batch_size, <= n_ctx)
            the text tokens
        xa : torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)
            the encoded audio features to be attended on
        """
        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0
        x = (
            self.token_embedding(x)
            + self.positional_embedding[offset : offset + x.shape[-1]]
        )
        x = x.to(xa.dtype)

        for block in self.blocks:
            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)

        x = self.ln(x)
        logits = (
            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)
        ).float()

        return logits
```

简而言之，文本解码器由下面几层网络组成：

- 一个将 token 转换为隐藏状态的词嵌入层
- 一个添加位置信息的 positional embedding 层
- 一个由 residual attention blocks 组成的堆栈
- 一个对隐藏状态进行归一化的层 normalization 层
- 一个计算输出 logits 的线性层

最后，将音频编码器与文本解码器组合在一起，就是一个Whipser:

```python
class Whisper(nn.Module):
    def __init__(self, dims: ModelDimensions):
        super().__init__()
        self.dims = dims
        self.encoder = AudioEncoder(
            self.dims.n_mels,
            self.dims.n_audio_ctx,
            self.dims.n_audio_state,
            self.dims.n_audio_head,
            self.dims.n_audio_layer,
        )
        self.decoder = TextDecoder(
            self.dims.n_vocab,
            self.dims.n_text_ctx,
            self.dims.n_text_state,
            self.dims.n_text_head,
            self.dims.n_text_layer,
        )
        # use the last half layers for alignment by default; see `set_alignment_heads()` below
        all_heads = torch.zeros(
            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool
        )
        all_heads[self.dims.n_text_layer // 2 :] = True
        self.register_buffer("alignment_heads", all_heads.to_sparse(), persistent=False)

    def set_alignment_heads(self, dump: bytes):
        array = np.frombuffer(
            gzip.decompress(base64.b85decode(dump)), dtype=bool
        ).copy()
        mask = torch.from_numpy(array).reshape(
            self.dims.n_text_layer, self.dims.n_text_head
        )
        self.register_buffer("alignment_heads", mask.to_sparse(), persistent=False)

    def embed_audio(self, mel: torch.Tensor):
        return self.encoder(mel)

    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):
        return self.decoder(tokens, audio_features)

    def forward(
        self, mel: torch.Tensor, tokens: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        return self.decoder(tokens, self.encoder(mel))

    @property
    def device(self):
        return next(self.parameters()).device

    @property
    def is_multilingual(self):
        return self.dims.n_vocab == 51865

    def install_kv_cache_hooks(self, cache: Optional[dict] = None):
        cache = {**cache} if cache is not None else {}
        hooks = []

        def save_to_cache(module, _, output):
            if module not in cache or output.shape[1] > self.dims.n_text_ctx:
                # save as-is, for the first token or cross attention
                cache[module] = output
            else:
                cache[module] = torch.cat([cache[module], output], dim=1).detach()
            return cache[module]

        def install_hooks(layer: nn.Module):
            if isinstance(layer, MultiHeadAttention):
                hooks.append(layer.key.register_forward_hook(save_to_cache))
                hooks.append(layer.value.register_forward_hook(save_to_cache))

        self.decoder.apply(install_hooks)
        return cache, hooks

    detect_language = detect_language_function
    transcribe = transcribe_function
    decode = decode_function
```

在 __init__ 方法中，首先初始化了一个 AudioEncoder 对象作为音频编码器，并初始化了一个 TextDecoder 对象作为文本解码器。然后创建了一个全零张量，表示所有的注意力头都不用于对齐。接下来，代码将张量的后一半设置为 True，表示默认使用后一半的注意力头进行对齐。最后，将张量注册为稀疏缓冲区。

接下来是 set_alignment_heads 方法，它接受一个字节串作为输入。这个方法用于设置用于对齐的多头注意力。首先使用 base85 解码和 gzip 解压缩对输入字节串进行处理，然后将其转换为布尔型数组。接下来，使用 torch.from_numpy 函数将数组转换为张量，并调整其形状。最后，将张量注册为稀疏缓冲区。

接下来是 embed_audio 方法，它接受一个声音频谱作为输入，并返回音频编码器的输出。然后是 logits 方法，它接受两个张量作为输入：文本令牌和音频特征。这个方法返回文本解码器的输出。

接下来是 forward 方法，它接受两个张量作为输入：声音频谱和文本令牌。这个方法首先使用音频编码器对声音频谱进行编码，然后将结果传递给文本解码器，并返回结果。

最后是一些属性和方法。其中 device 属性返回模型所在的设备；is_multilingual 属性返回模型是否支持多语言；install_kv_cache_hooks 方法用于安装键值缓存钩子；detect_language、transcribe 和 decode 分别是检测语言、转录和解码的函数。

### 15.3 可解释的机器学习

可解释的机器学习（Interpretable Machine Learning，或 Explainable Machine Learning）是指开发和使用能够提供透明且可理解输出的机器学习模型和方法。其主要目标是使模型的预测结果和决策过程对人类用户更加清晰和可理解，从而增加对模型的信任和依赖。这对于许多应用场景尤为重要，例如医疗诊断、金融决策和法律裁决等领域。

深度学习的可解释性一直是一个热门话题。深度学习模型通常被认为是“黑盒”模型，因为它们的内部结构和参数通常非常复杂，难以理解。这使得深度学习模型在某些领域（如医疗和金融）的应用受到了一定的限制。因此，研究人员一直在努力提高深度学习模型的可解释性，以便更好地理解模型的预测结果和决策过程。

为什么需要可解释的机器学习？

- 信任和透明度：用户和决策者更容易信任那些能够解释其行为的模型。
- 合规性和法律要求：某些领域（如金融和医疗）对模型的透明度有严格的法律要求。
- 模型调试：通过理解模型的内部机制，可以更容易地发现和修复模型中的错误。
- 公平性和偏见检测：通过解释模型，可以识别和纠正模型中的偏见和歧视。

可解释性方法可以分为两类：内生可解释模型和后处理解释方法。

内生可解释模型本身具有透明的结构和易于理解的决策规则。例如：

- 线性回归：模型的系数直接表示特征对预测的影响。
- 决策树：通过树状结构展示决策路径。
- 逻辑回归：类似于线性回归，但用于分类问题。

这也是这本书里我们花了这么多时间反复讲解这些模型的用法以及原理的原因。如果可以用简单可解释方法解决问题，那么就不需要用复杂的模型。

后处理解释方法用于解释复杂的、通常是“黑箱”模型（如深度神经网络、随机森林等）的预测结果。例如：

| 方法名称                        | 英文名称                                              | 描述                                                               |
|--------------------------------|-------------------------------------------------------|--------------------------------------------------------------------|
| SHAP                           | SHapley Additive exPlanations                         | 基于博弈论的解释方法，分配每个特征对预测的贡献。                   |
| LIME                           | Local Interpretable Model-agnostic Explanations       | 通过构建局部线性模型来解释模型在某个特定点的预测。                 |
| 特征重要性             | Feature Importance                                    | 衡量特征对模型预测的重要性，常用于随机森林和梯度提升树等模型。     |
| 部分依赖图                     | Partial Dependence Plots (PDPs)                       | 展示单个或多特征的变化如何影响预测结果。                           |
| 对抗性示例                     | Adversarial Examples                                  | 通过对输入进行微小的扰动来研究模型的敏感性和稳健性。               |

下面我们看一个用lime库解释机器学习方法的例子：

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import lime
import lime.lime_tabular

# 加载数据
data = load_iris()
X = data.data
y = data.target

# 拆分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练模型
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 使用 LIME 解释模型
explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=data.feature_names, class_names=data.target_names, discretize_continuous=True)
i = 0 # 选择一个样本进行解释
exp = explainer.explain_instance(X_test[i], model.predict_proba, num_features=2)
exp.show_in_notebook(show_all=False)
```

在这个例子中，我们使用了鸢尾花数据集（iris）来训练一个随机森林分类器，并使用 LIME 来解释模型的预测结果。

首先，我们加载数据集并拆分数据集为训练集和测试集。

然后，我们训练一个随机森林分类器，并使用 LIME 的 LimeTabularExplainer 类来解释模型的预测结果。

最后，我们选择一个样本进行解释，并显示解释结果。

### 15.4 演化学习

演化学习（Evolutionary Learning）是一类受生物演化过程启发的机器学习和优化方法。这些方法通过模拟自然选择和遗传进化的过程来寻找最佳解决方案。演化学习的核心思想是通过选择、交叉和变异等操作逐步改进候选解决方案群体，从而在复杂的搜索空间中找到全局最优解。

演化学习的基本概念

| 概念            | 英文名称            | 描述                                                                 |
|-----------------|---------------------|----------------------------------------------------------------------|
| 个体            | Individual          | 每个个体表示一个候选解决方案，通常用编码（如二进制串、实数向量等）表示。 |
| 种群            | Population          | 一组个体构成一个种群。                                                 |
| 适应度函数      | Fitness Function    | 用于评估每个个体的优劣程度。                                           |
| 选择            | Selection           | 按照适应度函数的值从种群中选择优秀的个体，通常采用轮盘赌选择、锦标赛选择等方法。 |
| 交叉            | Crossover           | 通过组合两个或多个个体的部分基因来生成新的个体，模拟生物的生殖过程。       |
| 变异            | Mutation            | 对个体的基因进行随机改变，以增加种群的多样性，防止陷入局部最优。           |
| 进化            | Evolution           | 经过多个世代的选择、交叉和变异，逐步优化种群中的个体。                     |

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/genetic.png)

演化学习的主要方法

| 演化算法名称 | 英文名称 | 简称 | 描述 |
|--------------|----------|------|------|
| 遗传算法 | Genetic Algorithm | GA | 一种经典的演化学习方法，通过选择、交叉和变异等操作来优化个体。 |
| 遗传编程 | Genetic Programming | GP | 将个体表示为程序或表达式，通过演化过程优化程序结构和参数。 |
| 差分进化 | Differential Evolution | DE | 一种基于向量差分的演化方法，适用于连续优化问题。 |
| 进化策略 | Evolution Strategies | ES | 另一种演化学习方法，主要通过变异和选择来优化个体。 |
| 粒子群优化 | Particle Swarm Optimization | PSO | 虽然不严格属于演化算法，但受群体行为启发，通过个体间的协作和信息共享进行优化。 |

遗传算法的流程

1. 初始化种群：随机生成一定数量的个体，组成初始种群。
2. 评估适应度：计算每个个体的适应度值。
3. 选择父母：根据适应度值从种群中选择父母个体，常用的方法有轮盘赌选择、锦标赛选择等。
4. 交叉：选择若干对父母进行交叉操作，生成新的子代个体。
5. 变异：对新生成的子代个体进行变异操作。
6. 生成新种群：将子代个体组成新的种群，替换旧种群。
7. 重复步骤2-6：迭代进行，直到达到终止条件（如达到最大迭代次数或适应度达到某个阈值）。

下面我们用PyTorch实现一个简单的遗传算法的示例：

```python
import torch
import random

# 参数设置
POPULATION_SIZE = 100
GENOME_LENGTH = 10
NUM_GENERATIONS = 50
MUTATION_RATE = 0.01

# 初始化种群
def initialize_population(size, genome_length):
    return torch.randint(0, 2, (size, genome_length), dtype=torch.float32)

# 适应度函数（示例：最大化1的数量）
def fitness(individual):
    return individual.sum().item()

# 选择机制（轮盘赌选择）
def selection(population, fitnesses):
    total_fitness = sum(fitnesses)
    probs = [f / total_fitness for f in fitnesses]
    selected_indices = random.choices(range(len(population)), probs, k=len(population))
    return population[selected_indices]

# 交叉（单点交叉）
def crossover(parent1, parent2):
    point = random.randint(1, len(parent1) - 1)
    child1 = torch.cat((parent1[:point], parent2[point:]))
    child2 = torch.cat((parent2[:point], parent1[point:]))
    return child1, child2

# 变异（逐位变异）
def mutate(individual, mutation_rate):
    for i in range(len(individual)):
        if random.random() < mutation_rate:
            individual[i] = 1 - individual[i]
    return individual

# 主进化过程
def genetic_algorithm():
    population = initialize_population(POPULATION_SIZE, GENOME_LENGTH)
    
    for generation in range(NUM_GENERATIONS):
        fitnesses = [fitness(ind) for ind in population]
        
        # 选择
        selected_population = selection(population, fitnesses)
        
        # 交叉和变异
        next_population = []
        for i in range(0, POPULATION_SIZE, 2):
            parent1, parent2 = selected_population[i], selected_population[i+1]
            child1, child2 = crossover(parent1, parent2)
            next_population.extend([mutate(child1, MUTATION_RATE), mutate(child2, MUTATION_RATE)])
        
        population = torch.stack(next_population)
        
        best_fitness = max(fitnesses)
        print(f'Generation {generation}: Best Fitness = {best_fitness}')
    
    best_individual = population[torch.argmax(torch.tensor(fitnesses))]
    return best_individual

# 运行遗传算法
best_solution = genetic_algorithm()
print(f'Best Solution: {best_solution.numpy()}')
```

运行结果如下，中间过程有省略：

```
Generation 0: Best Fitness = 9.0
Generation 1: Best Fitness = 9.0
...
Generation 48: Best Fitness = 10.0
Generation 49: Best Fitness = 10.0
Best Solution: [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]
```

我们也可以用封装好的库来实现遗传算法，比如DEAP库。

首先我们安装库：

```
pip install deap
```

然后导入模块：

```python
import random  
from deap import base, creator, tools, algorithms  
```
这里导入了必要的模块，包括random用于生成随机数，以及deap库中的几个组件用于创建遗传算法。

定义优化问题：

```python
creator.create("FitnessMax", base.Fitness, weights=(1.0,))  
creator.create("Individual", list, fitness=creator.FitnessMax)  
```

定义遗传算法的两个基本概念：适应度（FitnessMax）和个体（Individual）。适应度是衡量个体好坏的标准，在这里我们希望最大化某个值，因此权重设置为正数（1.0）。个体则是解决方案的表示，这里简单地使用列表来存储一个浮点数。

初始化工具箱：

```python
toolbox = base.Toolbox()  
toolbox.register("attr_float", random.uniform, -10, 10)  
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, 1)  
toolbox.register("population", tools.initRepeat, list, toolbox.individual)  
```


工具箱（toolbox）用于注册各种遗传算法的操作。这里注册了属性生成器（产生一个在-10到10之间的随机浮点数），个体生成器（创建一个包含单个属性的个体），以及种群生成器（创建一个由多个个体组成的种群）。

定义评估函数：

```python
def eval_func(individual):  
    return individual[0]**2,  
```

这个函数计算个体的适应度，即个体中数值的平方。返回的是一个元组，因为deap要求适应度是一个元组。

注册遗传操作：

```python
toolbox.register("mate", tools.cxBlend, alpha=0.5)  
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)  
toolbox.register("select", tools.selTournament, tournsize=3)  
toolbox.register("evaluate", eval_func)  
```

这里注册了遗传算法中的交叉（mate）、变异（mutate）和选择（select）操作，以及评估函数（evaluate）。交叉操作用cxBlend实现，变异操作用高斯变异实现，选择操作用锦标赛选择实现。

运行遗传算法：

```python
population = toolbox.population(n=100)  
NGEN = 40  
CXPB, MUTPB = 0.5, 0.2  

for gen in range(NGEN):  
    offspring = algorithms.varAnd(population, toolbox, cxpb=CXPB, mutpb=MUTPB)  
    fits = map(toolbox.evaluate, offspring)  
    for fit, ind in zip(fits, offspring):  
        ind.fitness.values = fit  
    population = toolbox.select(offspring, k=len(population))  
```

首先初始化一个大小为100的种群。然后进行40代进化，每一代中进行交叉、变异和选择操作，并重新评估个体的适应度。

输出结果：

```python
top_ind = tools.selBest(population, k=1)[0]  
print(f"Best individual: {top_ind}, Fitness: {top_ind.fitness.values[0]}")  
```

最后，从种群中选择最佳个体并打印出来，显示其值和适应度。

这段代码实现了一个简单的遗传算法，用于找到使函数f(x) = x^2最大化的x值。通过多代进化，算法逐渐向最优解靠拢。

我们看下完整的代码：

```python
import random
from deap import base, creator, tools, algorithms

# 定义优化问题：最大化目标函数 f(x) = x^2
creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_float", random.uniform, -10, 10)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, 1)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

def eval_func(individual):
    return individual[0]**2,

toolbox.register("mate", tools.cxBlend, alpha=0.5)
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)
toolbox.register("select", tools.selTournament, tournsize=3)
toolbox.register("evaluate", eval_func)

population = toolbox.population(n=100)
NGEN = 40
CXPB, MUTPB = 0.5, 0.2

for gen in range(NGEN):
    offspring = algorithms.varAnd(population, toolbox, cxpb=CXPB, mutpb=MUTPB)
    fits = map(toolbox.evaluate, offspring)
    for fit, ind in zip(fits, offspring):
        ind.fitness.values = fit
    population = toolbox.select(offspring, k=len(population))
    
top_ind = tools.selBest(population, k=1)[0]
print(f"Best individual: {top_ind}, Fitness: {top_ind.fitness.values[0]}")
```

运行结果如下：

```
Best individual: [-1479201.2340890465], Fitness: 2188036290930.558
```

### 2.3 PAC学习理论

PAC（Probably Approximately Correct）学习理论是计算学习理论中的一部分，由 Leslie Valiant 在 1984 年提出。它为机器学习提供了一个框架，用于分析学习算法的性能，特别是从有限样本中学习的有效性和可靠性。

概念（Concept）是指需要学习的目标函数，通常表示为$c$。

假设空间（Hypothesis Space）是候选假设的集合，表示为$H$，其中每个假设$h$都是一个可能的目标函数。

可分布性（Distribution）是指数据的样本遵循某个未知的概率分布$D$. 

误差（Error）可分为两种：实际误差和经验误差。

实际误差（True Error）是指假设$h$与目标概念$c$之间的误差，表示为$\text{error}_D(h)$，即在分布$D$下$h$和$c$不一致的概率。

经验误差（Empirical Error）是指在训练样本上 $h$ 和 $c$ 不一致的比例。

如果对于任何$\epsilon > 0$和$\delta > 0$，存在一个算法，能够在多项式时间内找到一个假设$h $，使得 $\text{error}_D(h) \leq \epsilon$ 并且这种情况发生的概率至少为 $1 - \delta$，我们就称假设空间$h$为 $(\epsilon, \delta)$-PAC 可学习的。

假设目标概念 $c$ 是从某个假设空间 $H$ 中学习的，那么样本复杂度通常可以表示为：$m = O\left(\frac{1}{\epsilon} \log \frac{|H|}{\delta}\right)$

这里的$m$是样本数量，$\epsilon$是误差容忍度， $\delta$是置信度参数， $|H|$ 是假设空间的大小。

在很多实际情况中，假设空间的大小 $|H|$ 可能非常大甚至是无限的，这时用假设空间的 VC 维度（Vapnik-Chervonenkis Dimension）来代替 $|H|$ 更为合适。VC 维度 $d_{VC}$是衡量假设空间复杂性的一个参数。

在这种情况下，样本复杂度可以表示为：$m = O\left(\frac{d_{VC}}{\epsilon} \log \frac{1}{\epsilon} + \frac{1}{\epsilon} \log \frac{1}{\delta}\right)$

VC 维度定义如下：假设空间 $H$的 VC 维度 $d_{VC}$ 是能够被 $H$ 完全分割（shatter）的一组点的最大数量。如果存在一组点 $\{x_1, x_2, \ldots, x_d\} $，对于这组点的任意标签组合，假设空间 $H$ 中总能找到一个假设使其完全正确分类这组点，那么 $H$ 就 shatter 这组点，并且 $d$ 是 VC 维度。

我们来看两个VC维度的例子来加深理解：

- 线性分类器（二维空间中的线性分割）：假设空间 $H$ 是二维平面上的所有直线。VC 维度为 3，因为最多可以找到三个点，使得通过不同直线能够对这三个点的所有可能的标记组合（即 $2^3 = 8$ 种组合）进行正确分类。但无法找到四个点使直线对其所有可能的标记组合进行正确分类。
- 一维空间上的阈值：在一维空间中，假设空间 $H$ 是所有可能的阈值函数。VC 维度为 1，因为任意两个点的所有可能标记组合不能被一个阈值函数完全正确分类。

## 参考文献

1. 周志华. 机器学习. 北京：清华大学出版社, 2016
2. 李航. 机器学习方法。北京：清华大学出版社, 2022
3. Tom M. Mitchell 著 曾华军 张银奎 等译. 机器学习. 北京：机械工业出版社, 2011
4. Richard S. Sutton, Andrew G. Barto著，俞凯等译. 强化学习（第2版），北京：电子工业出版社，2019
5. 诸葛越 主编，葫芦娃 著，百面机器学习，北京：人民邮电出版社，2018
6. 王贺、刘鹏、钱乾，机器学习算法竞赛实战，北京：人民邮电出版社，2021
7. Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N., Kaiser Łukasz, and Polosukhin Illia. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. 5998–6008.
8. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 4302-4310, 2017.
9. Schulman J, Wolski F, Dhariwal P, Radford A, Klimov O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. 2017
10. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540):529–533.
11. Mercer James 1909, XVI. Functions of positive and negative type, and their connection the theory of integral equationsPhilosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 209 415–446
