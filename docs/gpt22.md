# 2023å¹´çš„æ·±åº¦å­¦ä¹ å…¥é—¨æŒ‡å—(22) - ç™¾å·å¤§æ¨¡å‹13Bçš„è¿è¡ŒåŠé‡åŒ–

ä¸çŸ¥é“ä¸Šä¸€è®²çš„å¤§æ®µä»£ç å¤§å®¶çœ‹æ™•äº†æ²¡æœ‰ã€‚ä½†æ˜¯å¦‚æœä½ ä»”ç»†çœ‹äº†ä¼šå‘ç°ï¼Œå…¶å®ä»£ç è¿˜æ˜¯ä¸å…¨çš„ã€‚æ¯”å¦‚åˆ†è¯å™¨æˆ‘ä»¬å°±æ²¡è®²ã€‚
å¦å¤–ï¼Œ13Bæ¯”7Bçš„æ”¹è¿›ç‚¹ä¹Ÿæ²¡æœ‰è®²ã€‚

å†æœ‰ï¼Œå¯¹äº13Béœ€è¦å¤šå°‘æ˜¾å­˜æˆ‘ä»¬ä¹Ÿæ²¡è¯´ã€‚13Bå…‰æ˜¯æ¨¡å‹åŠ è½½å°±éœ€è¦26GBçš„æ˜¾å­˜ï¼ŒåŠ ä¸Šæ¨ç†éœ€è¦çš„æ¶ˆiè€—ï¼Œæ²¡æœ‰ä¸ª28GBä»¥ä¸Šçš„æ˜¾å­˜æ˜¯æ¯”è¾ƒæ‚¬çš„ã€‚æ°å¥½24GBçš„3090å’Œ4090å•å¡ä¸å¤Ÿç”¨ã€‚

æˆ‘ä»¬å…ˆä»åº”ç”¨è®²èµ·ã€‚

## ç™¾å·13bçš„å‘½ä»¤è¡Œäº¤äº’

ç™¾å·å®˜æ–¹åœ¨13bçš„å¼€æºä»£ç ä¸­ç»™æˆ‘ä»¬æä¾›äº†å‘½ä»¤è¡Œäº¤äº’å¼çš„åº”ç”¨å’ŒWebæœåŠ¡çš„åŸºæœ¬æ¡†æ¶ã€‚

æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å‘½ä»¤è¡Œäº¤äº’å¼çš„åº”ç”¨ã€‚

```python
import os
import torch
import platform
from colorama import Fore, Style
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig


def init_model():
    print("init model ...")
    model = AutoModelForCausalLM.from_pretrained(
        "baichuan-inc/Baichuan-13B-Chat",
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.generation_config = GenerationConfig.from_pretrained(
        "baichuan-inc/Baichuan-13B-Chat"
    )
    tokenizer = AutoTokenizer.from_pretrained(
        "baichuan-inc/Baichuan-13B-Chat",
        use_fast=False,
        trust_remote_code=True
    )
    return model, tokenizer


def clear_screen():
    if platform.system() == "Windows":
        os.system("cls")
    else:
        os.system("clear")
    print(Fore.YELLOW + Style.BRIGHT + "æ¬¢è¿ä½¿ç”¨ç™¾å·å¤§æ¨¡å‹ï¼Œè¾“å…¥è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå†å²ï¼ŒCTRL+C ä¸­æ–­ç”Ÿæˆï¼Œstream å¼€å…³æµå¼ç”Ÿæˆï¼Œexit ç»“æŸã€‚")
    return []


def main(stream=True):
    model, tokenizer = init_model()

    messages = clear_screen()
    while True:
        prompt = input(Fore.GREEN + Style.BRIGHT + "\nç”¨æˆ·ï¼š" + Style.NORMAL)
        if prompt.strip() == "exit":
            break
        if prompt.strip() == "clear":
            messages = clear_screen()
            continue
        print(Fore.CYAN + Style.BRIGHT + "\nBaichuanï¼š" + Style.NORMAL, end='')
        if prompt.strip() == "stream":
            stream = not stream
            print(Fore.YELLOW + "({}æµå¼ç”Ÿæˆ)\n".format("å¼€å¯" if stream else "å…³é—­"), end='')
            continue
        messages.append({"role": "user", "content": prompt})
        if stream:
            position = 0
            try:
                for response in model.chat(tokenizer, messages, stream=True):
                    print(response[position:], end='', flush=True)
                    position = len(response)
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
            except KeyboardInterrupt:
                pass
            print()
        else:
            response = model.chat(tokenizer, messages)
            print(response)
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
        messages.append({"role": "assistant", "content": response})

    print(Style.RESET_ALL)


if __name__ == "__main__":
    main()
```

è°ƒç”¨æ¨¡å‹çš„éƒ¨åˆ†å¤§å®¶éƒ½æ¯”è¾ƒç†Ÿæ‚‰äº†ï¼Œè¿™é‡Œå”¯ä¸€å€¼å¾—è¯´ä¸€è¯´çš„åè€Œæ˜¯æ˜¾ç¤ºæ ¼å¼ç›¸å…³çš„coloramaåº“ã€‚

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/baichuan1.png)

```python
    print(Fore.YELLOW + Style.BRIGHT + "æ¬¢è¿ä½¿ç”¨ç™¾å·å¤§æ¨¡å‹ï¼Œè¾“å…¥è¿›è¡Œå¯¹è¯ï¼Œclear æ¸…ç©ºå†å²ï¼ŒCTRL+C ä¸­æ–­ç”Ÿæˆï¼Œstream å¼€å…³æµå¼ç”Ÿæˆï¼Œexit ç»“æŸã€‚")
...
    prompt = input(Fore.GREEN + Style.BRIGHT + "\nç”¨æˆ·ï¼š" + Style.NORMAL)
```

ç³»ç»Ÿæç¤ºä¸ºé»„è‰²ï¼Œè€Œç”¨æˆ·è¾“å…¥ä¸ºç»¿è‰²ï¼Œç™¾å·çš„å›å¤ä¸ºé’è‰²ã€‚

çœ‹èµ·æ¥ç™¾å·çš„åŒå­¦æ˜¯å†™è¿‡å‰ç«¯çš„ï¼Œéƒ½ç”¨ä¸€ä¸ªé¢œè‰²å¤ªä¹±å¿ä¸äº†ã€‚ï¼šï¼‰

å®‰è£…æ—¶åˆ«å¿˜äº†å®‰è£…coloramaåº“ã€‚æˆ–è€…æŒ‰ä¸‹é¢çš„åˆ—è¡¨è£…å…¨äº†å§ï¼š

```bash
pip install transformers
pip install sentencepiece
pip install accelerate
pip install transformers_stream_generator
pip install colorama
pip install cpm_kernels
pip install streamlit
```

## ç™¾å·13bçš„WebæœåŠ¡demo

ç™¾å·çš„Web demoé‡Œï¼Œå…³äºæ¨¡å‹çš„è°ƒç”¨éƒ¨åˆ†è¿˜æ˜¯æ²¡å•¥å¯è®²çš„ã€‚
ä½†æ˜¯ï¼ŒStreamlitçš„å‰ç«¯æœ‰å¿…è¦ç®€å•è¯´ä¸€ä¸‹ã€‚
Streamlitå°è£…äº†å¾ˆå¤šå¸¸ç”¨çš„å‰ç«¯ç»„ä»¶ï¼Œæ¯”å¦‚å¯¹è¯è¿™æ ·çš„é«˜çº§ç»„ä»¶ï¼Œå°±æ˜¯ç”¨st.chat_message()æ¥å®ç°çš„ã€‚

æˆ‘ä»¬æ¥çœ‹ä¸ªä¾‹å­ï¼š

```python
import streamlit as st

with st.chat_message("assistant", avatar='ğŸ¤–'):
    st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾å·å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")
```

æˆ‘ä»¬æŠŠä¸Šé¢çš„æ–‡ä»¶å­˜ä¸ºtest1.pyï¼Œç„¶ååœ¨å‘½ä»¤è¡Œè¿è¡Œï¼š

```bash
streamlit run test1.py
```

è¿è¡Œä¹‹åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼Œçœ‹åˆ°å¦‚ä¸‹ç•Œé¢ï¼š
![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/streamlit.png)

```python
with st.chat_message("assistant", avatar='ğŸ¤–'):
```
è¿™ä¸€è¡Œåˆ›å»ºäº†ä¸€ä¸ªèŠå¤©æ¶ˆæ¯çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ¶ˆæ¯çš„å‘é€è€…æ˜¯ "assistant"ï¼Œå¹¶ä¸”ä½¿ç”¨äº†ä¸€ä¸ªæœºå™¨äººè¡¨æƒ…ä½œä¸ºå¤´åƒï¼ˆ'ğŸ¤–'ï¼‰ã€‚

```python
    st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾å·å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")
```
è¿™è¡Œä»£ç åœ¨ä¸Šè¿°çš„ "assistant" èŠå¤©æ¶ˆæ¯ä¸­æ·»åŠ äº†ä¸€æ®µ Markdown æ ¼å¼çš„æ–‡æœ¬ã€‚

å¥½ï¼Œä¸‹é¢æˆ‘ä»¬æŠŠç”¨æˆ·è¾“å…¥çš„åŠŸèƒ½åŠ è¿›æ¥ï¼Œä½¿ç”¨st.chat_input()å°±å¯ä»¥å®ç°ï¼Œä¸éœ€è¦å†™javascriptä»£ç ï¼š

```python
import streamlit as st

with st.chat_message("assistant", avatar='ğŸ¤–'):
    st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾å·å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
    with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
        st.markdown(prompt)
```

è¿è¡Œæ•ˆæœå¦‚ä¸‹ï¼š

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/st2.png)

æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç»™é¡µé¢åŠ ä¸Šæ ‡é¢˜å’Œå±æ€§ï¼š

```python
import streamlit as st

st.set_page_config(page_title="Baichuan-13B-Chat")
st.title("Baichuan-13B-Chat")

with st.chat_message("assistant", avatar='ğŸ¤–'):
    st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾å·å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
    with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
        st.markdown(prompt)
```

ç†è§£äº†ä¸Šé¢çš„åŸºç¡€çŸ¥è¯†ä¹‹åï¼Œæˆ‘ä»¬å°±ç›´æ¥çœ‹ç™¾å·çš„ä»£ç å§ï¼š

```python
import json
import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig


st.set_page_config(page_title="Baichuan-13B-Chat")
st.title("Baichuan-13B-Chat")


@st.cache_resource
def init_model():
    model = AutoModelForCausalLM.from_pretrained(
        "baichuan-inc/Baichuan-13B-Chat",
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.generation_config = GenerationConfig.from_pretrained(
        "baichuan-inc/Baichuan-13B-Chat"
    )
    tokenizer = AutoTokenizer.from_pretrained(
        "baichuan-inc/Baichuan-13B-Chat",
        use_fast=False,
        trust_remote_code=True
    )
    return model, tokenizer


def clear_chat_history():
    del st.session_state.messages


def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾å·å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages


def main():
    model, tokenizer = init_model()
    messages = init_chat_history()

    if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
        with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
            st.markdown(prompt)
        messages.append({"role": "user", "content": prompt})
        print(f"[user] {prompt}", flush=True)
        with st.chat_message("assistant", avatar='ğŸ¤–'):
            placeholder = st.empty()
            for response in model.chat(tokenizer, messages, stream=True):
                placeholder.markdown(response)
                if torch.backends.mps.is_available():
                    torch.mps.empty_cache()
        messages.append({"role": "assistant", "content": response})
        print(json.dumps(messages, ensure_ascii=False), flush=True)

        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()
```

## é‡åŒ–

å¦‚æœæƒ³è¦åœ¨æ¶ˆè´¹çº§çš„å•å¡ä¸Šè¿è¡Œç™¾å·13bçš„æ¨ç†ï¼Œéœ€è¦å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚

ç™¾å·13bæ”¯æŒ8ä½å’Œ4ä½çš„é‡åŒ–ã€‚8ä½é‡åŒ–ä¹‹åéœ€è¦18.6Gä»¥ä¸Šçš„æ˜¾å­˜ã€‚4ä½é‡åŒ–ä¹‹åéœ€è¦11.5GBä»¥ä¸Šçš„æ˜¾å­˜ã€‚åŒæ—¶ï¼ŒCPUåœ¨å®ç°é‡åŒ–çš„æ—¶å€™éœ€è¦36.1Gçš„å†…å­˜ï¼Œ32Gçš„ä¸å¤ªå¤Ÿç”¨ã€‚

æˆ‘ä»¬å…ˆçœ‹ä¸‹8ä½é‡åŒ–çš„ä¾‹å­ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig
tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan-13B-Chat", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-13B-Chat", torch_dtype=torch.float16, trust_remote_code=True)
model.generation_config = GenerationConfig.from_pretrained("baichuan-inc/Baichuan-13B-Chat")
model = model.quantize(8).cuda()
messages = []
messages.append({"role": "user", "content":"äºšå†å±±å¤§çš„éª‘å…µä¸ºä»€ä¹ˆå¼ºå¤§ï¼Ÿ"})
response = model.chat(tokenizer, messages)
print(response)
```

è¾“å‡ºå¦‚ä¸‹ï¼š
```
äºšå†å±±å¤§å¤§å¸çš„éª‘å…µä¹‹æ‰€ä»¥å¼ºå¤§ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªåŸå› ï¼š

1. é©¬åŒ¹è´¨é‡é«˜ï¼šäºšå†å±±å¤§æ‰€å¤„çš„é©¬å…¶é¡¿åœ°åŒºç››äº§ä¼˜è´¨æˆ˜é©¬ï¼Œè¿™äº›é©¬åŒ¹ä½“å‹é«˜å¤§ã€é€Ÿåº¦å¿«ã€è€åŠ›å¼ºï¼Œéå¸¸é€‚åˆè¿›è¡Œæˆ˜æ–—ã€‚è¿™ä½¿å¾—ä»–çš„éª‘å…µåœ¨æˆ˜åœºä¸Šå…·æœ‰å¾ˆé«˜çš„æœºåŠ¨æ€§å’Œå†²å‡»åŠ›ã€‚

2. è®­ç»ƒæœ‰ç´ ï¼šäºšå†å±±å¤§çš„éª‘å…µç»è¿‡ä¸¥æ ¼çš„è®­ç»ƒï¼Œèƒ½å¤Ÿç†Ÿç»ƒåœ°ä½¿ç”¨æ­¦å™¨å’Œæˆ˜æœ¯ã€‚ä»–ä»¬ä¸ä»…æ“…é•¿å†²é”‹é™·é˜µï¼Œè¿˜èƒ½å¤Ÿåœ¨æˆ˜åœºä¸Šçµæ´»åœ°è¿›è¡Œè¿‚å›ã€åŒ…æŠ„ç­‰è¡ŒåŠ¨ï¼Œå¯¹æ•Œå†›é€ æˆä¸¥é‡æ‰“å‡»ã€‚

3. è£…å¤‡ç²¾è‰¯ï¼šäºšå†å±±å¤§çš„éª‘å…µè£…å¤‡äº†å½“æ—¶æœ€å…ˆè¿›çš„æ­¦å™¨å’ŒæŠ¤å…·ï¼Œå¦‚é•¿çŸ›ã€å¼“ç®­ã€ç›¾ç‰Œç­‰ã€‚è¿™äº›æ­¦å™¨æ—¢èƒ½æœ‰æ•ˆä¿æŠ¤å£«å…µï¼Œåˆèƒ½ç»™äºˆæ•Œäººæ²‰é‡çš„æ‰“å‡»ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜é…å¤‡äº†é©¬é•«ï¼Œä½¿éª‘å£«åœ¨é©¬èƒŒä¸Šæ›´åŠ ç¨³å®šï¼Œæé«˜äº†æˆ˜æ–—æ•ˆç‡ã€‚

4. ä¸¥å¯†çš„ç»„ç»‡å’ŒæŒ‡æŒ¥ï¼šäºšå†å±±å¤§çš„éª‘å…µåœ¨æˆ˜åœºä¸Šæœ‰ä¸¥å¯†çš„ç»„ç»‡å’ŒæŒ‡æŒ¥ä½“ç³»ã€‚ä»–ä»¬é€šè¿‡æ——å¸œã€å·è§’ç­‰æ–¹å¼è¿›è¡Œé€šä¿¡ï¼Œç¡®ä¿éƒ¨é˜Ÿä¹‹é—´çš„ååŒä½œæˆ˜ã€‚åŒæ—¶ï¼Œäºšå†å±±å¤§æœ¬äººä½œä¸ºç»Ÿå¸…ï¼Œå¯¹éª‘å…µæˆ˜æœ¯æœ‰ç€æ·±åˆ»çš„ç†è§£ï¼Œèƒ½å¤Ÿæ ¹æ®æˆ˜åœºæƒ…å†µåˆ¶å®šåˆé€‚çš„æˆ˜ç•¥ã€‚

5. å¼ºå¤§çš„å¿ƒç†ç´ è´¨ï¼šäºšå†å±±å¤§çš„éª‘å…µæ‹¥æœ‰æé«˜çš„å¿ƒç†ç´ è´¨ï¼Œä»–ä»¬åœ¨æˆ˜åœºä¸Šå‹‡æ•¢æ— ç•ï¼Œæ•¢äºé¢å¯¹ä»»ä½•å›°éš¾ã€‚è¿™ç§ç²¾ç¥åŠ›é‡ä½¿å¾—ä»–ä»¬åœ¨æˆ˜æ–—ä¸­å§‹ç»ˆä¿æŒæ—ºç››çš„æ–—å¿—ï¼Œæˆä¸ºä¸€æ”¯ä¸å¯å°è§‘çš„åŠ›é‡ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œäºšå†å±±å¤§çš„éª‘å…µä¹‹æ‰€ä»¥å¼ºå¤§ï¼Œæ˜¯å› ä¸ºä»–ä»¬æ‹¥æœ‰é«˜è´¨é‡çš„é©¬åŒ¹ã€è®­ç»ƒæœ‰ç´ çš„å£«å…µã€ç²¾è‰¯çš„è£…å¤‡ã€ä¸¥å¯†çš„ç»„ç»‡å’Œå“è¶Šçš„é¢†å¯¼ã€‚è¿™äº›å› ç´ å…±åŒé“¸å°±äº†ä¸€æ”¯å¼ºå¤§çš„éª‘å…µéƒ¨é˜Ÿï¼Œä½¿å¾—äºšå†å±±å¤§å¤§å¸èƒ½å¤Ÿå¾æœæ•´ä¸ªå·²çŸ¥ä¸–ç•Œã€‚
```

æ•ˆæœçœ‹æ¥ä»ç„¶ä¸é”™å“ˆã€‚

å¦‚æœæƒ³è¦ä½¿ç”¨4ä½é‡åŒ–ï¼Œå°†`model = model.quantize(8).cuda()`æ”¹ä¸º`model = model.quantize(4).cuda()`å³å¯:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig
tokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan-13B-Chat", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-13B-Chat", torch_dtype=torch.float16, trust_remote_code=True)
model.generation_config = GenerationConfig.from_pretrained("baichuan-inc/Baichuan-13B-Chat")
model = model.quantize(4).cuda()
messages = []
messages.append({"role": "user", "content":"äºšå†å±±å¤§å¤§å¸çš„éª‘å…µä¸ºä»€ä¹ˆå¼ºå¤§ï¼Ÿ"})
response = model.chat(tokenizer, messages)
print(response)
```

è¾“å‡ºå¦‚ä¸‹ï¼š
```
äºšå†å±±å¤§(Alexander the Great)çš„éª‘å…µä¹‹æ‰€ä»¥å¼ºå¤§ï¼Œä¸»è¦åŸå› æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š

1. è®­ç»ƒå’Œçºªå¾‹ï¼šäºšå†å±±å¤§çš„å†›é˜Ÿä»¥ä¸¥æ ¼çš„è®­ç»ƒå’Œé«˜æ°´å¹³çš„çºªå¾‹è‘—ç§°ã€‚ä»–çš„å£«å…µæ¥å—äº†é«˜åº¦ä¸“ä¸šçš„å†›äº‹è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯åœ¨é©¬æœ¯ã€å°„å‡»æŠ€å·§å’Œæˆ˜åœºæˆ˜æœ¯æ–¹é¢ã€‚è¿™ä½¿å¾—ä»–ä»¬åœ¨æˆ˜åœºä¸Šå…·æœ‰å¾ˆé«˜çš„æœºåŠ¨æ€§å’Œæˆ˜æ–—åŠ›ã€‚

2. é©¬åŒ¹è´¨é‡ï¼šäºšå†å±±å¤§çš„éª‘å…µä½¿ç”¨çš„æ˜¯é«˜å“è´¨çš„æˆ˜é©¬ï¼Œè¿™äº›é©¬åŒ¹ç»è¿‡ç²¾æŒ‘ç»†é€‰ï¼Œå…·å¤‡å‡ºè‰²çš„é€Ÿåº¦ã€è€åŠ›å’ŒåŠ›é‡ã€‚è¿™äº›é©¬åŒ¹åœ¨æˆ˜åœºä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å›½å®¶çš„é©¬åŒ¹ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿå¿«é€Ÿç§»åŠ¨å¹¶æœ‰æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ã€‚

3. è£…å¤‡ç²¾è‰¯ï¼šäºšå†å±±å¤§çš„éª‘å…µé…å¤‡äº†å…ˆè¿›çš„æ­¦å™¨å’Œç›”ç”²ï¼Œå¦‚é•¿çŸ›ã€å¼“ç®­å’ŒæŠ¤èƒ¸ç”²ç­‰ã€‚è¿™äº›è£…å¤‡ä¸ä»…æé«˜äº†ä»–ä»¬çš„æˆ˜æ–—åŠ›ï¼Œè¿˜é™ä½äº†ä¼¤äº¡ç‡ã€‚

4. æˆ˜ç•¥ä¼˜åŠ¿ï¼šäºšå†å±±å¤§çš„éª‘å…µåœ¨æˆ˜äº‰ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹ä»˜æ•Œå†›æ­¥å…µæ—¶ã€‚ä»–ä»¬çš„é«˜é€Ÿåº¦å’ŒæœºåŠ¨æ€§ä½¿ä»–ä»¬èƒ½å¤Ÿè¿…é€Ÿçªç ´æ•Œäººçš„é˜²çº¿ï¼Œä¸ºæ­¥å…µæä¾›æ”¯æ´ã€‚æ­¤å¤–ï¼Œéª‘å…µè¿˜å¯ä»¥ç”¨äºä¾¦æŸ¥æ•Œæƒ…ã€åˆ‡æ–­è¡¥ç»™çº¿ä»¥åŠè¿›è¡Œéªšæ‰°ä½œæˆ˜ã€‚

5. é¢†å¯¼åŠ›ï¼šäºšå†å±±å¤§çš„é¢†å¯¼æ‰èƒ½å’Œå“è¶ŠæŒ‡æŒ¥ä½¿ä»–çš„å†›é˜Ÿå£«æ°”é«˜æ¶¨ã€‚ä»–çš„å£«å…µä»¬å¯¹ä»–å……æ»¡ä¿¡å¿ƒï¼Œæ„¿æ„ä¸ºä»–å‡ºç”Ÿå…¥æ­»ã€‚è¿™ç§ç´§å¯†çš„å›¢é˜Ÿç²¾ç¥å’Œå¿ è¯šä½¿å¾—äºšå†å±±å¤§çš„éª‘å…µåœ¨æˆ˜åœºä¸Šå…·æœ‰å¼ºå¤§çš„å‡èšåŠ›å’Œæˆ˜æ–—åŠ›ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œäºšå†å±±å¤§çš„éª‘å…µä¹‹æ‰€ä»¥å¼ºå¤§ï¼Œæ˜¯å› ä¸ºä»–ä»¬æ‹¥æœ‰é«˜ç´ è´¨çš„å£«å…µã€ä¼˜è‰¯çš„é©¬åŒ¹ã€ç²¾è‰¯çš„è£…å¤‡ã€æœ‰æ•ˆçš„æˆ˜ç•¥ä»¥åŠå“è¶Šçš„é¢†å¯¼åŠ›ã€‚è¿™äº›å› ç´ å…±åŒé“¸å°±äº†ä»–ä»¬æ— ä¸ä¼¦æ¯”çš„æˆ˜æ–—åŠ›ï¼Œä½¿ä»–ä»¬åœ¨å†å²ä¸Šç•™ä¸‹äº†æ·±åˆ»çš„å°è®°ã€‚
```

çœ‹èµ·æ¥ä¹Ÿè¿˜ä¸é”™å“ˆã€‚

## é‡åŒ–çš„å®ç°

æˆ‘ä»¬æ¥çœ‹ä¸‹é‡åŒ–çš„å®ç°ï¼Œåœ¨modeling_baichuan.pyä¸­çš„quantizeå…¶å®å°±æ˜¯æŠŠW,oå’Œmlpçš„æ¯ä¸€å±‚éƒ½é‡åŒ–æ‰ã€‚

```python
    def quantize(self, bits: int):
        try:
            from .quantizer import QLinear
        except ImportError:
            raise ImportError(
                f"Needs QLinear to run quantize."
            )

        for layer in self.model.layers:
            layer.self_attn.W_pack = QLinear(
                bits=bits,
                weight=layer.self_attn.W_pack.weight,
                bias = None,
            )
            layer.self_attn.o_proj = QLinear(
                bits=bits,
                weight=layer.self_attn.o_proj.weight,
                bias = None,
            )
            layer.mlp.gate_proj = QLinear(
                bits=bits,
                weight=layer.mlp.gate_proj.weight,
                bias = None,
            )
            layer.mlp.down_proj = QLinear(
                bits=bits,
                weight=layer.mlp.down_proj.weight,
                bias = None,
            )
            layer.mlp.up_proj = QLinear(
                bits=bits,
                weight=layer.mlp.up_proj.weight,
                bias = None,
            )
        return self
```

æˆ‘ä»¬ç»§ç»­çœ‹ä¸‹QLinearçš„å®ç°ï¼Œå…¶å®å°±æ˜¯æŠŠæƒé‡å’Œåç½®é‡åŒ–æ‰ï¼Œç„¶ååœ¨forwardçš„æ—¶å€™ï¼ŒæŠŠè¾“å…¥ä¹Ÿé‡åŒ–æ‰ï¼Œç„¶åå†åšçŸ©é˜µä¹˜æ³•ï¼Œæœ€åå†åé‡åŒ–å›å»ã€‚

åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œé¦–å…ˆå°† bits å‚æ•°ä¿å­˜åˆ° self.quant_bits å±æ€§ä¸­ã€‚ç„¶åè®¡ç®—é‡åŒ–æ‰€éœ€çš„ç¼©æ”¾å› å­ self.scaleã€‚è¿™ä¸ªç¼©æ”¾å› å­æ˜¯é€šè¿‡å°†æƒé‡çŸ©é˜µçš„ç»å¯¹å€¼å–æœ€å¤§å€¼ï¼Œç„¶åé™¤ä»¥ (2 ** (bits - 1)) - 1) æ¥è®¡ç®—çš„ã€‚æ¥ä¸‹æ¥ï¼Œæ ¹æ®é‡åŒ–ä½æ•°çš„ä¸åŒï¼Œä½¿ç”¨ä¸åŒçš„æ–¹æ³•å¯¹æƒé‡çŸ©é˜µè¿›è¡Œé‡åŒ–ã€‚å¦‚æœé‡åŒ–ä½æ•°ä¸º 4ï¼Œåˆ™è°ƒç”¨ quant4 å‡½æ•°è¿›è¡Œé‡åŒ–ï¼›å¦‚æœé‡åŒ–ä½æ•°ä¸º 8ï¼Œåˆ™ä½¿ç”¨å››èˆäº”å…¥æ–¹æ³•è¿›è¡Œé‡åŒ–ã€‚æœ€åï¼Œå°†åç½®é¡¹è®¾ç½®ä¸º Noneã€‚

```python
class QLinear(torch.nn.Module):
    def __init__(self, bits: int, weight: torch.Tensor, bias=None):
        super().__init__()
        self.quant_bits = bits
        self.scale = weight.abs().max(dim=-1).values / ((2 ** (bits - 1)) - 1)
        self.scale = self.scale.to(torch.float32)
        if self.quant_bits == 4:
            self.weight = quant4(weight, self.scale)
        elif self.quant_bits == 8:
            self.weight = torch.round(weight.to(self.scale.dtype) / self.scale[:, None]).to(torch.int8)
        if self.quant_bits == 8:
            self.weight = self.weight.T
        self.bias = None
```

è¿™ä¸ªç±»è¿˜å®šä¹‰äº†ä¸€ä¸ªåä¸º forward çš„æ–¹æ³•ï¼Œå®ƒæ¥å—ä¸€ä¸ªåä¸º input çš„å‚æ•°ã€‚è¿™ä¸ªæ–¹æ³•é¦–å…ˆæ£€æŸ¥è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œå¹¶å°†æƒé‡çŸ©é˜µå’Œç¼©æ”¾å› å­è½¬ç§»åˆ°è¾“å…¥å¼ é‡æ‰€åœ¨çš„è®¾å¤‡ä¸Šã€‚ç„¶åæ ¹æ®é‡åŒ–ä½æ•°çš„ä¸åŒï¼Œä½¿ç”¨ä¸åŒçš„æ–¹æ³•å¯¹æƒé‡çŸ©é˜µè¿›è¡Œåé‡åŒ–ï¼Œå¹¶ä¸è¾“å…¥å¼ é‡è¿›è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—ã€‚å¦‚æœåç½®é¡¹ä¸ä¸º Noneï¼Œåˆ™å°†å…¶åŠ åˆ°è¾“å‡ºå¼ é‡ä¸Šã€‚æœ€åè¿”å›è¾“å‡ºå¼ é‡ã€‚

```python
    def forward(self, input):
        if self.quant_bits == 4:
            assert(input.dtype == torch.bfloat16 or input.dtype == torch.float16)            

        if self.weight.device != input.device:
            self.weight = self.weight.to(input.device)
            self.scale = self.scale.to(input.device)
        
        if self.quant_bits == 4:
            self.scale = self.scale.to(input.dtype)
            rweight = dequant4(self.weight, self.scale, input).T
            output = torch.matmul(input, rweight)
        elif self.quant_bits == 8:
            rweight = self.weight.to(input.dtype) * self.scale.to(input.dtype)
            output = torch.matmul(input, rweight)
        if self.bias is not None:
            output = output + self.bias
        return output
```

é‡åŒ–çš„åŸç†æˆ‘ä»¬ä¹‹å‰å·²ç»è®²è¿‡äº†ï¼Œæˆ‘ä»¬æ¥çœ‹4ä½é‡åŒ–çš„å®ç°ï¼Œæˆ‘è¿˜æ˜¯æŠŠæ³¨é‡Šå†™åœ¨ä»£ç è¡Œé‡Œï¼š

```python
def quant4(weight: torch.Tensor, scale: torch.Tensor):
    stream = torch.cuda.current_stream()
    num_row = weight.size(0)
    num_chan_fp16 = weight.size(1)
    # 4bit
    num_chan_int = num_chan_fp16 // 8
    qweight = torch.zeros((num_row, num_chan_int), dtype=torch.int32, device=weight.device)
    intweight = torch.empty(num_row, num_chan_fp16, dtype = torch.int32)
    # å°†æƒé‡å¼ é‡é™¤ä»¥æ¯”ä¾‹å› å­ã€å››èˆäº”å…¥ã€è£å‰ªåœ¨ [-16, 15] èŒƒå›´å†…ï¼Œç„¶åè½¬æ¢ä¸º 32 ä½æ•´æ•°
    intweight = torch.clip(torch.round(weight.to(scale.dtype) / scale[:, None]),-16, 15).to(dtype=torch.int32) 

    # ä½¿ç”¨ä½æ“ä½œï¼ˆä½ç§»å’Œä½ä¸ï¼‰å°† 8 ä¸ª 4 ä½æ•´æ•°æ‰“åŒ…åˆ°ä¸€ä¸ª 32 ä½æ•´æ•°ä¸­
    for j in range(num_chan_int):
        qweight[:, j] = ((intweight[:, j*8+7] & 0x0f) << 28) \
            | ((intweight[:, j*8+6] & 0x0f) << 24) \
            | ((intweight[:, j*8+5] & 0x0f) << 20) \
            | ((intweight[:, j*8+4] & 0x0f) << 16) \
            | ((intweight[:, j*8+3] & 0x0f) << 12) \
            | ((intweight[:, j*8+2] & 0x0f) << 8) \
            | ((intweight[:, j*8+1] & 0x0f) << 4) \
            | ((intweight[:, j*8] & 0x0f))
    return qweight
```

## å°ç»“

è¿™ä¸€èŠ‚æˆ‘ä»¬è¿›ä¸€æ­¥äº†è§£äº†ç™¾å·13bå¤§æ¨¡å‹è¿è¡Œå’Œé‡åŒ–çš„æ–¹æ³•ï¼Œä»¥åŠç®€è¦ä»‹ç»äº†é‡åŒ–çš„åŸç†ã€‚
