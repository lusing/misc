# 2023年的深度学习入门指南(12) - 参数高效微调PEFT

大家都知道，大模型的训练需要海量的算力。其实，即使是只对大模型做微调训练，也是需要大量的计算资源的。

有没有用更少的计算资源来进行微调的方法呢？研究者研发出了几种被Hugging Face统称为参数高效微调PEFT(Parameter-Efficient Fine-Tuning)的技术。

这其中常用的几个大家应该已经耳熟能详了，比如广泛应用的LoRA技术，Prefix Tuning技术，Prompt Tuning技术等等。

我们先学习如何使用，然后我们再学习其背后的原理。

## 