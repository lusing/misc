# 2023å¹´çš„æ·±åº¦å­¦ä¹ å…¥é—¨æŒ‡å—(25) - é€šä¹‰åƒé—®7b

æœ€è¿‘å‘ç”Ÿçš„ä¸¤ä»¶äº‹æƒ…éƒ½æ¯”è¾ƒæœ‰æ„æ€ï¼Œä¸€ä¸ªæ˜¯è¿ç»­å¼€æºäº†7bå’Œ13bæ¨¡å‹çš„ç™¾å·ï¼Œå¯¹å…¶53bé—­æºäº†ï¼›å¦ä¸€ä¸ªæ˜¯é—­æºé¡¹ç›®é€šä¹‰åƒé—®å¼€æºäº†è‡ªå·±çš„7bæ¨¡å‹ã€‚

ä¸‹é¢æˆ‘ä»¬å°±æ¥ç ”ç©¶ä¸‹é€šä¹‰åƒé—®7b.

## ä½¿ç”¨é€šä¹‰åƒé—®7b

é¦–å…ˆå®‰è£…ä¾èµ–åº“ï¼š

```bash
pip install transformers==4.31.0 accelerate tiktoken einops transformers_stream_generator bitsandbytes
```

é€šä¹‰åƒé—®7bçš„å¼€æºåšå¾—è¿˜æ˜¯ä¸é”™çš„ï¼Œä¸å…‰åœ¨è‡ªå®¶çš„é­”æ­å¹³å°ä¸Šå¯ä»¥ç”¨ï¼Œè€Œä¸”ä¹Ÿå¼€æ”¾åœ¨äº†huggingfaceä¸Šï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨huggingfaceçš„APIæ¥è°ƒç”¨ã€‚

æˆ‘ä»¬æŒ‰ç…§å®˜æ–¹çš„ä¸‰è½®å¯¹è¯çš„ä¾‹å­ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

# ç¬¬ä¸€è½®å¯¹è¯ 1st dialogue turn
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)

# ç¬¬äºŒè½®å¯¹è¯ 2nd dialogue turn
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)

# ç¬¬ä¸‰è½®å¯¹è¯ 3rd dialogue turn
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
```

è¿è¡Œç»“æœå¦‚ä¸‹ï¼š
```
ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ã€‚
---
å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ï¼š

è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå«åšæ°å…‹çš„æ•…äº‹ã€‚æ°å…‹æ˜¯ä¸€ä¸ªéå¸¸æœ‰æŠ±è´Ÿçš„å¹´è½»äººï¼Œä»–ä¸€ç›´æ¢¦æƒ³ç€è‡ªå·±èƒ½å¤Ÿæˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚ä»–ä»å°å°±å¯¹å•†ä¸šå’Œåˆ›ä¸šæœ‰ç€æµ“åšçš„å…´è¶£ï¼Œè€Œä¸”éå¸¸å‹¤å¥‹ï¼Œæ€»æ˜¯åŠªåŠ›å­¦ä¹ å’Œæ¢ç´¢æ–°çš„çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚

åœ¨å¤§å­¦é‡Œï¼Œæ°å…‹å­¦ä¹ äº†å•†ä¸šç®¡ç†å’Œåˆ›ä¸šè¯¾ç¨‹ï¼Œå¹¶ä¸”ç§¯æå‚åŠ å„ç§å•†ä¸šç«èµ›å’Œå®ä¹ é¡¹ç›®ã€‚ä»–é€šè¿‡è‡ªå·±çš„åŠªåŠ›å’Œèªæ˜æ‰æ™ºï¼Œèµ¢å¾—äº†å¾ˆå¤šå¥–é¡¹å’Œæœºä¼šï¼Œå¾—åˆ°äº†å¾ˆå¤šå®è´µçš„ç»éªŒå’ŒçŸ¥è¯†ã€‚

æ¯•ä¸šåï¼Œæ°å…‹å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹åœ¨å¸‚åœºä¸Šå¯»æ‰¾æœºä¼šï¼Œå‘ç°äº†ä¸€ä¸ªéå¸¸æœ‰æ½œåŠ›çš„è¡Œä¸šï¼Œå¹¶ä¸”å†³å®šåœ¨è¿™ä¸ªè¡Œä¸šé‡Œåˆ›ä¸šã€‚ä»–é¢ä¸´ç€å¾ˆå¤šæŒ‘æˆ˜å’Œå›°éš¾ï¼Œä½†æ˜¯ä»–éå¸¸åšéŸ§å’Œæœ‰å†³å¿ƒï¼Œä¸æ–­åŠªåŠ›å’Œæ¢ç´¢æ–°çš„æ–¹æ³•å’Œæ€è·¯ï¼Œä¸æ–­åœ°å­¦ä¹ å’Œè¿›æ­¥ã€‚

æ°å…‹å’Œä»–çš„å›¢é˜Ÿç»å†äº†è®¸å¤šå›°éš¾å’Œå¤±è´¥ï¼Œä½†æ˜¯ä»–ä»¬ä¸€ç›´ä¿æŒç€ä¹è§‚å’Œç§¯æçš„æ€åº¦ï¼Œå¹¶ä¸”ä¸æ–­åœ°å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±çš„æ–¹æ³•å’Œç­–ç•¥ã€‚æœ€ç»ˆï¼Œä»–ä»¬ç»ˆäºæˆåŠŸåœ°æ¨å‡ºäº†ä¸€æ¬¾éå¸¸å—æ¬¢è¿çš„äº§å“ï¼Œå¹¶ä¸”åœ¨å¸‚åœºä¸Šè·å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚

æ°å…‹çš„æˆåŠŸä¸ä»…ä»…æ˜¯å› ä¸ºä»–çš„èªæ˜æ‰æ™ºå’Œå‹¤å¥‹åŠªåŠ›ï¼Œæ›´é‡è¦çš„æ˜¯å› ä¸ºä»–å…·æœ‰åšå®šçš„ä¿¡å¿µå’Œä¸å±ˆä¸æŒ çš„ç²¾ç¥ã€‚ä»–ä¸æ–­åœ°å­¦ä¹ å’Œè¿›æ­¥ï¼Œä¸æ–­åœ°å°è¯•æ–°çš„æ–¹æ³•å’Œæ€è·¯ï¼Œä¸æ–­åœ°å…‹æœå›°éš¾å’ŒæŒ‘æˆ˜ï¼Œæœ€ç»ˆå–å¾—äº†æˆåŠŸã€‚ä»–çš„æ•…äº‹å‘Šè¯‰æˆ‘ä»¬ï¼Œåªè¦æˆ‘ä»¬å…·æœ‰å‹‡æ°”å’Œå†³å¿ƒï¼Œå°±å¯ä»¥åœ¨åˆ›ä¸šçš„é“è·¯ä¸Šå–å¾—æˆåŠŸã€‚
---
è¿™ä¸ªæ•…äº‹çš„æ ‡é¢˜å¯ä»¥æ˜¯ï¼šã€Šæ°å…‹çš„åˆ›ä¸šä¹‹è·¯ã€‹ã€‚
```

ä¸çŸ¥é“åƒé—®7bæ‰€è¯´çš„æ°å…‹ï¼Œæ˜¯ä¸æ˜¯å§“é©¬ï¼Ÿï¼šï¼‰

## gradio

åƒé—®7bçš„Web demoç”¨çš„æ˜¯Gradioæ¥å®ç°çš„ã€‚ä¸Streamlitç±»ä¼¼ï¼ŒGradioä¹Ÿæ˜¯åŒ…å«äº†ç®€å•çš„Webå°è£…ï¼ŒåŠ ä¸Šå‰ç«¯çš„å°è£…ã€‚

æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸ªæœ€ç®€å•çš„ä¾‹å­ï¼š
```python
import gradio as gr

def greet(name):
    return "Hello " + name + "!"

demo = gr.Interface(fn=greet, inputs="text", outputs="text")
    
if __name__ == "__main__":
    demo.launch() 
```

Gradioå¯¹Jupyter Notebookçš„æ”¯æŒç›¸å½“å¥½ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨Jupyter Notebookä¸­è¿è¡Œï¼Œæ—¢å¯ä»¥å¯åŠ¨åç«¯ï¼Œä¹Ÿèƒ½å±•ç¤ºå‰ç«¯ã€‚

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gradio.png)

Gradioé€šè¿‡Markdownæ–¹æ³•æ¥ä¹¦å†™markdownæ–‡æœ¬ï¼Œå½“ç„¶ä¹Ÿæ”¯æŒhtmlæ ‡ç­¾ï¼š

```python
    gr.Markdown("""<p align="center"><img src="https://modelscope.cn/api/v1/models/qwen/Qwen-7B-Chat/repo?Revision=master&FilePath=assets/logo.jpeg&View=true" style="height: 80px"/><p>""")
    gr.Markdown("""<center><font size=8>Qwen-7B-Chat Bot</center>""")
    gr.Markdown(
        """<center><font size=3>This WebUI is based on Qwen-7B-Chat, developed by Alibaba Cloud. (æœ¬WebUIåŸºäºQwen-7B-Chatæ‰“é€ ï¼Œå®ç°èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚)</center>"""
    )
    gr.Markdown(
        """<center><font size=4>Qwen-7B <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>&nbsp ï½œ Qwen-7B-Chat <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>&nbsp ï½œ &nbsp<a href="https://github.com/QwenLM/Qwen-7B">Github</a></center>"""
    )
```

æˆ‘ä»¬æ¥çœ‹ä¸‹æ•ˆæœï¼š

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gradio2.png)

Gradioæ”¯æŒTextBoxç”¨äºè¾“å…¥ï¼ŒButtonç”¨äºç‚¹å‡»äº‹ä»¶ï¼Œè€Œä¸”æ”¯æŒChatBotè¿™æ ·çš„å¤æ‚æ§ä»¶ã€‚è¿˜å¯ä»¥ç”¨Rowæ¥æ¨ªå‘å¸ƒå±€ï¼š

```python
    chatbot = gr.Chatbot(lines=10, label='Qwen-7B-Chat', elem_classes="control-height")
    query = gr.Textbox(lines=2, label='Input')

    with gr.Row():
        emptyBtn = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
        submitBtn = gr.Button("ğŸš€ Submit (å‘é€)")
        regenBtn = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")
```

æ•ˆæœå¦‚ä¸‹ï¼š

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/gradio3.png)

å®Œæ•´ä»£ç å¦‚ä¸‹ï¼Œå¤§å®¶å¯ä»¥è‡ªå·±è¿è¡Œä¸€ä¸‹ï¼š

```python
import gradio as gr
with gr.Blocks() as demo:
    gr.Markdown("""<p align="center"><img src="https://modelscope.cn/api/v1/models/qwen/Qwen-7B-Chat/repo?Revision=master&FilePath=assets/logo.jpeg&View=true" style="height: 80px"/><p>""")
    gr.Markdown("""<center><font size=8>Qwen-7B-Chat Bot</center>""")
    gr.Markdown(
        """<center><font size=3>This WebUI is based on Qwen-7B-Chat, developed by Alibaba Cloud. (æœ¬WebUIåŸºäºQwen-7B-Chatæ‰“é€ ï¼Œå®ç°èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚)</center>"""
    )
    gr.Markdown(
        """<center><font size=4>Qwen-7B <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>&nbsp ï½œ Qwen-7B-Chat <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>&nbsp ï½œ &nbsp<a href="https://github.com/QwenLM/Qwen-7B">Github</a></center>"""
    )

    chatbot = gr.Chatbot(lines=10, label='Qwen-7B-Chat', elem_classes="control-height")
    query = gr.Textbox(lines=2, label='Input')

    with gr.Row():
        emptyBtn = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
        submitBtn = gr.Button("ğŸš€ Submit (å‘é€)")
        regenBtn = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")

    gr.Markdown(
        """<font size=2>Note: This demo is governed by the original license of Qwen-7B. We strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, including hate speech, violence, pornography, deception, etc. (æ³¨ï¼šæœ¬æ¼”ç¤ºå—Qwen-7Bçš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)"""
    )

if __name__ == "__main__":
    demo.launch()
```

å†ç»™ä¸‰ä¸ªButtoné…ä¸Šå“åº”å‡½æ•°ï¼Œå°±å¯ä»¥å“åº”åŠŸèƒ½äº†ï¼š
```python
    submitBtn.click(predict, [query, chatbot], [chatbot], show_progress=True)
    submitBtn.click(reset_user_input, [], [query])
    emptyBtn.click(reset_state, outputs=[chatbot], show_progress=True)
    regenBtn.click(regenerate, [chatbot], [chatbot], show_progress=True)
```

å…¶ä¸­reset_stateåªæ›´æ–°ä¸‹å†…éƒ¨çŠ¶æ€å°±å¥½ï¼š

```python
def reset_state():
    task_history.clear()
    return []
```

reset_user_inputéœ€è¦é€šè¿‡updateå‡½æ•°æ¥åˆ·æ–°ä¸‹çŠ¶æ€ï¼Œå†™è¿‡Reactçš„åŒå­¦åº”è¯¥å¾ˆç†Ÿæ‚‰ï¼Œè¿™å…¶å®æ˜¯ä¸ªå¼‚æ­¥æ“ä½œå“ˆï¼š

```python
def reset_user_input():
    return gr.update(value="")
```

ç„¶åæ˜¯éœ€è¦å¤„ç†ä¸‹æµçŠ¶æ€çš„predictå‡½æ•°ï¼š

```python
def predict(query, chatbot):
    print("User: " + parse_text(query))
    chatbot.append((parse_text(query), ""))
    fullResponse = ""

    for response in model.chat_stream(tokenizer, query, history=task_history):
        chatbot[-1] = (parse_text(query), parse_text(response))

        yield chatbot
        fullResponse = parse_text(response)

    task_history.append((query, fullResponse))
    print("Qwen-7B-Chat: " + parse_text(fullResponse))
```

æ³¨æ„yieldçš„ç”¨æ³•ï¼Œchatbotå°±æ˜¯æˆ‘ä»¬ç”¨gr.ChatBotç”Ÿæˆçš„å¯¹è¯æ¡†æ§ä»¶ã€‚

regenerateä»ç„¶è¦æ³¨æ„ä¸‹yieldï¼š

```python
def regenerate(chatbot):
    if not task_history:
        yield chatbot
        return
    item = task_history.pop(-1)
    chatbot.pop(-1)
    yield from predict(item[0], chatbot)
```

## ä»£ç è¶…å‚æ•°

ä¸‹é¢æˆ‘ä»¬æ¥çœ‹ä¸‹Qwen-7B-Chatçš„ä»£ç ã€‚

é¦–å…ˆæ˜¯æ”¯æŒäº†å“ªäº›é…ç½®é¡¹å’Œè¶…å‚æ•°ï¼š

```python
from transformers import PretrainedConfig


class QWenConfig(PretrainedConfig):
    model_type = "qwen"
    keys_to_ignore_at_inference = ["past_key_values"]
    attribute_map = {
        "hidden_size": "n_embd",
        "num_attention_heads": "n_head",
        "max_position_embeddings": "n_positions",
        "num_hidden_layers": "n_layer",
    }

    def __init__(
        self,
        vocab_size=151851,
        n_embd=4096,
        n_layer=32,
        n_head=32,
        n_inner=None,
        embd_pdrop=0.0,
        attn_pdrop=0.0,
        layer_norm_epsilon=1e-5,
        initializer_range=0.02,
        scale_attn_weights=True,
        use_cache=True,
        eos_token_id=151643,
        apply_residual_connection_post_layernorm=False,
        bf16=False,
        fp16=False,
        fp32=False,
        kv_channels=128,
        rotary_pct=1.0,
        rotary_emb_base=10000,
        use_dynamic_ntk=False,
        use_logn_attn=False,
        use_flash_attn=True,
        ffn_hidden_size=22016,
        no_bias=True,
        tie_word_embeddings=False,
        **kwargs,
    ):
        self.eos_token_id = eos_token_id
        super().__init__(
            eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs
        )

        self.vocab_size = vocab_size
        self.n_embd = n_embd
        self.n_layer = n_layer
        self.n_head = n_head
        self.n_inner = n_inner
        self.embd_pdrop = embd_pdrop
        self.attn_pdrop = attn_pdrop
        self.layer_norm_epsilon = layer_norm_epsilon
        self.initializer_range = initializer_range
        self.scale_attn_weights = scale_attn_weights
        self.use_cache = use_cache
        self.apply_residual_connection_post_layernorm = (
            apply_residual_connection_post_layernorm
        )
        self.bf16 = bf16
        self.fp16 = fp16
        self.fp32 = fp32
        self.kv_channels = kv_channels
        self.rotary_pct = rotary_pct
        self.rotary_emb_base = rotary_emb_base
        self.use_dynamic_ntk = use_dynamic_ntk
        self.use_logn_attn = use_logn_attn
        self.use_flash_attn = use_flash_attn
        self.ffn_hidden_size = ffn_hidden_size
        self.no_bias = no_bias
        self.tie_word_embeddings = tie_word_embeddings
```

æˆ‘ä»¬æ¥è§£é‡Šä¸‹è¿™äº›å‚æ•°ï¼š

- vocab_sizeï¼šè¯æ±‡è¡¨å¤§å°ï¼Œå³æ¨¡å‹å¯ä»¥å¤„ç†çš„ä¸åŒå•è¯çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 151851
- n_embdï¼š åµŒå…¥å±‚çš„ç»´åº¦ï¼Œå³æ¯ä¸ªå•è¯æˆ–ä½ç½®çš„å‘é‡è¡¨ç¤ºçš„é•¿åº¦ï¼Œé»˜è®¤ä¸º 4096
- n_layerï¼š ç¼–ç å™¨å±‚çš„æ•°é‡ï¼Œå³æ¨¡å‹ä¸­é‡å¤å †å çš„è‡ªæ³¨æ„åŠ›å±‚å’Œå‰é¦ˆå±‚çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 32
- n_head=32ï¼š æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œå³æ¯ä¸ªç¼–ç å™¨å±‚ä¸­åˆ†å‰²åçš„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ•°é‡ï¼Œé»˜è®¤ä¸º 32
- n_innerï¼š å‰é¦ˆå±‚çš„å†…éƒ¨ç»´åº¦ï¼Œå³æ¯ä¸ªç¼–ç å™¨å±‚ä¸­å…¨è¿æ¥å±‚çš„éšè—å•å…ƒæ•°ï¼Œé»˜è®¤ä¸º Noneï¼Œè¡¨ç¤ºä¸åµŒå…¥å±‚ç»´åº¦ç›¸åŒ
- embd_pdropï¼š åµŒå…¥å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ï¼Œå³åœ¨åµŒå…¥å±‚ååº”ç”¨ä¸¢å¼ƒæ­£åˆ™åŒ–æ—¶éšæœºç½®é›¶å•å…ƒçš„æ¦‚ç‡ï¼Œé»˜è®¤ä¸º 0.0ï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ä¸¢å¼ƒæ­£åˆ™åŒ–
- attn_pdropï¼š æ³¨æ„åŠ›å±‚çš„ä¸¢å¼ƒæ¦‚ç‡ï¼Œå³åœ¨æ³¨æ„åŠ›å±‚ååº”ç”¨ä¸¢å¼ƒæ­£åˆ™åŒ–æ—¶éšæœºç½®é›¶å•å…ƒçš„æ¦‚ç‡ï¼Œé»˜è®¤ä¸º 0.0ï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ä¸¢å¼ƒæ­£åˆ™åŒ–
- layer_norm_epsilonï¼š å±‚å½’ä¸€åŒ–çš„ epsilon å€¼ï¼Œå³åœ¨è®¡ç®—å±‚å½’ä¸€åŒ–æ—¶åŠ åˆ°åˆ†æ¯ä¸Šçš„å°é‡ï¼Œé˜²æ­¢é™¤ä»¥é›¶ï¼Œé»˜è®¤ä¸º 1e-5
- initializer_rangeï¼š åˆå§‹åŒ–èŒƒå›´ï¼Œå³åœ¨åˆå§‹åŒ–æ¨¡å‹å‚æ•°æ—¶ä½¿ç”¨çš„å‡åŒ€åˆ†å¸ƒçš„ä¸Šä¸‹ç•Œï¼Œé»˜è®¤ä¸º 0.02
- scale_attn_weightsï¼š æ˜¯å¦ç¼©æ”¾æ³¨æ„åŠ›æƒé‡ï¼Œå³åœ¨è®¡ç®—å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶æ—¶æ˜¯å¦é™¤ä»¥æ³¨æ„åŠ›å¤´æ•°çš„å¹³æ–¹æ ¹ï¼Œé»˜è®¤ä¸º True
- use_cacheï¼š æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œå³åœ¨è§£ç æ—¶æ˜¯å¦ä¿å­˜å‰é¢è®¡ç®—è¿‡çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›é”®å€¼å¯¹ï¼Œé»˜è®¤ä¸º True
- eos_token_idï¼šç»“æŸç¬¦å·çš„ IDï¼Œå³è¡¨ç¤ºåºåˆ—ç»“æŸçš„ç‰¹æ®Šå•è¯å¯¹åº”çš„æ•´æ•°ç¼–å·ï¼Œé»˜è®¤ä¸º 151643
- apply_residual_connection_post_layernormï¼šæ˜¯å¦åœ¨å±‚å½’ä¸€åŒ–ååº”ç”¨æ®‹å·®è¿æ¥ï¼Œå³åœ¨æ¯ä¸ªç¼–ç å™¨å±‚ä¸­æ˜¯å¦å…ˆè¿›è¡Œå±‚å½’ä¸€åŒ–å†åŠ ä¸Šè¾“å…¥ï¼Œé»˜è®¤ä¸º False
- bf16ï¼šæ˜¯å¦ä½¿ç”¨ bf16 æ ¼å¼ï¼Œå³æ˜¯å¦ä½¿ç”¨ 16 ä½æµ®ç‚¹æ•°æ¥å­˜å‚¨æ¨¡å‹å‚æ•°å’Œè®¡ç®—æ¢¯åº¦ï¼Œé»˜è®¤ä¸º False
- fp16ï¼šæ˜¯å¦ä½¿ç”¨ fp16 æ ¼å¼ï¼Œå³æ˜¯å¦ä½¿ç”¨ 16 ä½æµ®ç‚¹æ•°æ¥å­˜å‚¨æ¨¡å‹å‚æ•°å’Œè®¡ç®—æ¢¯åº¦ï¼Œé»˜è®¤ä¸º False
- fp32ï¼šæ˜¯å¦ä½¿ç”¨ fp32 æ ¼å¼ï¼Œå³æ˜¯å¦ä½¿ç”¨ 32 ä½æµ®ç‚¹æ•°æ¥å­˜å‚¨æ¨¡å‹å‚æ•°å’Œè®¡ç®—æ¢¯åº¦ï¼Œé»˜è®¤ä¸º False
- kv_channelsï¼š é”®å€¼é€šé“æ•°ï¼Œå³åœ¨è®¡ç®—æ³¨æ„åŠ›é”®å€¼å¯¹æ—¶ä½¿ç”¨çš„çº¿æ€§å˜æ¢çš„è¾“å‡ºç»´åº¦ï¼Œé»˜è®¤ä¸º 128
- rotary_pctï¼š æ—‹è½¬ç™¾åˆ†æ¯”ï¼Œå³åœ¨åµŒå…¥å±‚ä¸­ä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç çš„æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º 1.0ï¼Œè¡¨ç¤ºå…¨éƒ¨ä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
- rotary_emb_baseï¼š æ—‹è½¬åµŒå…¥åŸºæ•°ï¼Œå³åœ¨è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç æ—¶ä½¿ç”¨çš„åŸºæ•°ï¼Œé»˜è®¤ä¸º 10000
- use_dynamic_ntkï¼šæ˜¯å¦ä½¿ç”¨åŠ¨æ€ NTKï¼Œå³æ˜¯å¦åœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶ä½¿ç”¨åŠ¨æ€ç¥ç»åˆ‡çº¿æ ¸æ–¹æ³•ï¼Œé»˜è®¤ä¸º False
- use_logn_attnï¼š æ˜¯å¦ä½¿ç”¨å¯¹æ•°æ³¨æ„åŠ›ï¼Œå³æ˜¯å¦åœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶ä½¿ç”¨å¯¹æ•°å‡½æ•°æ¥åŠ é€Ÿå’Œå‹ç¼©ï¼Œé»˜è®¤ä¸º False
- use_flash_attnï¼š æ˜¯å¦ä½¿ç”¨é—ªå­˜æ³¨æ„åŠ›ï¼Œå³æ˜¯å¦åœ¨è®¡ç®—æ³¨æ„åŠ›æƒé‡æ—¶ä½¿ç”¨é—ªå­˜å˜æ¢æ¥é™ä½å¤æ‚åº¦ï¼Œé»˜è®¤ä¸º True
- ffn_hidden_sizeï¼š å‰é¦ˆå±‚çš„éšè—å¤§å°ï¼Œå³æ¯ä¸ªç¼–ç å™¨å±‚ä¸­å…¨è¿æ¥å±‚çš„è¾“å‡ºç»´åº¦ï¼Œé»˜è®¤ä¸º 22016
- no_biasï¼š æ˜¯å¦ä¸ä½¿ç”¨åç½®ï¼Œå³åœ¨æ¨¡å‹ä¸­çš„æ‰€æœ‰çº¿æ€§å˜æ¢ä¸­æ˜¯å¦ä¸æ·»åŠ åç½®å‘é‡ï¼Œé»˜è®¤ä¸º True
- tie_word_embeddingsï¼š æ˜¯å¦ç»‘å®šè¯åµŒå…¥ï¼Œå³åœ¨æ¨¡å‹ä¸­æ˜¯å¦å…±äº«è¾“å…¥å’Œè¾“å‡ºçš„è¯åµŒå…¥çŸ©é˜µï¼Œé»˜è®¤ä¸º False
- kwargsï¼š å…¶ä»–å‚æ•°ï¼Œç”¨äºæ¥æ”¶é¢å¤–çš„é…ç½®ä¿¡æ¯æˆ–è¦†ç›–ä¸Šé¢çš„é»˜è®¤å€¼

## Flash Attention

åƒé—®7bå»ºè®®ä½¿ç”¨flash attentionæ¥è¿›è¡ŒåŠ é€Ÿã€‚
Flash Attention æ˜¯ä¸€ç§æ–°å‹çš„æ³¨æ„åŠ›ç®—æ³•ï¼Œå®ƒå¯ä»¥å¿«é€Ÿå’Œå†…å­˜é«˜æ•ˆåœ°è®¡ç®—ç²¾ç¡®çš„æ³¨æ„åŠ›æƒé‡ï¼Œè€Œä¸éœ€è¦è¿‘ä¼¼æˆ–å‹ç¼©ã€‚å®ƒçš„ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨ GPU çš„å±‚æ¬¡åŒ–å†…å­˜ç»“æ„ï¼Œé€šè¿‡åˆ†å—å’Œé‡ç”¨çš„æ–¹æ³•ï¼Œå‡å°‘ä»é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰åˆ°ç‰‡ä¸Šé™æ€éšæœºå­˜å‚¨å™¨ï¼ˆSRAMï¼‰çš„è¯»å†™æ¬¡æ•°ï¼Œä»è€Œæé«˜è®¡ç®—é€Ÿåº¦å’ŒèŠ‚çœå†…å­˜ç©ºé—´ã€‚Flash Attention è¿˜å¯ä»¥æ‰©å±•åˆ°å—ç¨€ç–æ³¨æ„åŠ›ï¼Œè¿›ä¸€æ­¥é™ä½è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜æ¶ˆè€—ã€‚

Flash Attention çš„ä¸»è¦ä¼˜åŠ¿æœ‰ï¼š
- å®ƒå¯ä»¥å®ç°ä¸æ ‡å‡†æ³¨æ„åŠ›ç›¸åŒçš„æ¨¡å‹è´¨é‡å’Œç²¾åº¦ï¼Œè€Œä¸ç‰ºç‰²ä»»ä½•ä¿¡æ¯æˆ–å¼•å…¥ä»»ä½•å™ªå£°ã€‚
- å®ƒå¯ä»¥åœ¨ä¸åŒçš„åºåˆ—é•¿åº¦ã€æ‰¹é‡å¤§å°ã€æ¨¡å‹å¤§å°å’Œç¡¬ä»¶é…ç½®ä¸‹ï¼Œéƒ½èƒ½è¾¾åˆ°æ˜¾è‘—çš„åŠ é€Ÿå’Œå†…å­˜èŠ‚çœæ•ˆæœã€‚
- å®ƒå¯ä»¥ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯å¦‚æ··åˆç²¾åº¦è®­ç»ƒã€æ¿€æ´»æ£€æŸ¥ç‚¹ç­‰å…¼å®¹ï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚
- å®ƒå¯ä»¥æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

å…·ä½“åŸç†æˆ‘ä»¬åé¢ä¼šåˆ†æåˆ°å…¶è®ºæ–‡å’Œä»£ç ã€‚
ä»£ç åœ¨ï¼šhttps://github.com/Dao-AILab/flash-attentionã€‚è®ºæ–‡åœ¨ï¼šhttps://arxiv.org/abs/2205.14135

è¿™é‡Œæˆ‘ä»¬å…ˆçœ‹åœ¨åƒé—®7bä¸­å¦‚ä½•ä½¿ç”¨flash attentionã€‚

é¦–å…ˆè¦æŠŠFlash attentionçš„åº“åŠ è½½è¿›æ¥ï¼š
```python
def _import_flash_attn():
    global apply_rotary_emb_func, rms_norm, flash_attn_unpadded_func
    try:
        from flash_attn.layers.rotary import apply_rotary_emb_func as __apply_rotary_emb_func
        apply_rotary_emb_func = __apply_rotary_emb_func
    except ImportError:
        logger.warn(
            "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency "
            "https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary"
        )

    try:
        from flash_attn.ops.rms_norm import rms_norm as __rms_norm
        rms_norm = __rms_norm
    except ImportError:
        logger.warn(
            "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency "
            "https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm"
        )

    try:
        import flash_attn
        if not hasattr(flash_attn, '__version__'):
            from flash_attn.flash_attn_interface import flash_attn_unpadded_func as __flash_attn_unpadded_func
        else:
            if int(flash_attn.__version__.split(".")[0]) >= 2:
                from flash_attn.flash_attn_interface import flash_attn_varlen_func as __flash_attn_unpadded_func
            else:
                from flash_attn.flash_attn_interface import flash_attn_unpadded_func as __flash_attn_unpadded_func
        flash_attn_unpadded_func = __flash_attn_unpadded_func
    except ImportError:
        logger.warn(
            "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency "
            "https://github.com/Dao-AILab/flash-attention"
        )
```

ç„¶åæˆ‘ä»¬å®ç°ä¸€ä¸ªä½¿ç”¨Flash Attentionçš„è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼š
```python
class FlashSelfAttention(torch.nn.Module):
    def __init__(
        self,
        causal=False,
        softmax_scale=None,
        attention_dropout=0.0,
    ):
        super().__init__()
        assert flash_attn_unpadded_func is not None, (
            "Please install FlashAttention first, " "e.g., with pip install flash-attn"
        )
        assert (
            rearrange is not None
        ), "Please install einops first, e.g., with pip install einops"
        self.causal = causal
        self.softmax_scale = softmax_scale
        self.dropout_p = attention_dropout

    def forward(self, q, k, v):
        assert all((i.dtype in [torch.float16, torch.bfloat16] for i in (q, k, v)))
        assert all((i.is_cuda for i in (q, k, v)))
        batch_size, seqlen_q = q.shape[0], q.shape[1]
        seqlen_k = k.shape[1]
        q, k, v = [rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v]]
        cu_seqlens_q = torch.arange(
            0,
            (batch_size + 1) * seqlen_q,
            step=seqlen_q,
            dtype=torch.int32,
            device=q.device,
        )

        if self.training:
            assert seqlen_k == seqlen_q

            is_causal = self.causal
            cu_seqlens_k = cu_seqlens_q
        else:
            is_causal = seqlen_q == seqlen_k
            cu_seqlens_k = torch.arange(
                0,
                (batch_size + 1) * seqlen_k,
                step=seqlen_k,
                dtype=torch.int32,
                device=q.device,
            )
            self.dropout_p = 0
        output = flash_attn_unpadded_func(
            q,
            k,
            v,
            cu_seqlens_q,
            cu_seqlens_k,
            seqlen_q,
            seqlen_k,
            self.dropout_p,
            softmax_scale=self.softmax_scale,
            causal=is_causal,
        )

        output = rearrange(output, "(b s) ... -> b s ...", b=batch_size)
        return output
```

å…¶ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š
- é¦–å…ˆï¼Œæ£€æŸ¥q, k, vçš„æ•°æ®ç±»å‹æ˜¯å¦ä¸ºtorch.float16æˆ–torch.bfloat16ï¼Œä»¥åŠæ˜¯å¦åœ¨CUDAè®¾å¤‡ä¸Šè¿è¡Œã€‚
- ç„¶åï¼Œä½¿ç”¨einopsåº“çš„rearrangeå‡½æ•°ï¼Œå°†q, k, vçš„å½¢çŠ¶ä»"b s â€¦â€œå˜ä¸ºâ€(b s) â€¦"ï¼Œå…¶ä¸­bæ˜¯æ‰¹æ¬¡å¤§å°ï¼Œsæ˜¯åºåˆ—é•¿åº¦ã€‚
- æ¥ç€ï¼Œæ ¹æ®qå’Œkçš„åºåˆ—é•¿åº¦ï¼Œç”Ÿæˆä¸¤ä¸ªæ•´æ•°å¼ é‡cu_seqlens_qå’Œcu_seqlens_kï¼Œå®ƒä»¬è¡¨ç¤ºæ¯ä¸ªæ‰¹æ¬¡ä¸­æ¯ä¸ªåºåˆ—çš„èµ·å§‹ä½ç½®ã€‚
- å†ç„¶åï¼Œæ ¹æ®æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼å’Œæ˜¯å¦ä½¿ç”¨å› æœæ©ç ï¼Œè®¾ç½®cu_seqlens_kå’Œis_causalçš„å€¼ï¼Œä»¥åŠæ³¨æ„åŠ›çš„dropoutæ¦‚ç‡ã€‚
- æ ¸å¿ƒçš„Flash Attentionæ¥äº†ï¼Œè°ƒç”¨flash_attn_unpadded_funcå‡½æ•°ï¼Œå®ƒæ˜¯FlashAttentionåº“æä¾›çš„ä¸€ä¸ªæ ¸å¿ƒå‡½æ•°ï¼Œå®ƒå¯ä»¥å¿«é€Ÿè®¡ç®—æœªå¡«å……çš„è‡ªæ³¨æ„åŠ›çŸ©é˜µï¼Œå¹¶è¿”å›è¾“å‡ºå¼ é‡ã€‚
- æœ€åï¼Œå°†è¾“å‡ºå¼ é‡çš„å½¢çŠ¶ä»"(b s) â€¦â€œå˜å›"b s â€¦â€ï¼Œå¹¶è¿”å›ã€‚

## RMSNormå±‚

é€šä¹‰åƒé—®çš„RMSNormè·Ÿä¹‹å‰è®²çš„åŸºæœ¬ä¸€æ ·ï¼Œè¿™é‡Œå°±ä¸å¤šè§£é‡Šäº†ï¼š

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        if rms_norm is not None and x.is_cuda:
            return rms_norm(x, self.weight, self.eps)
        else:
            output = self._norm(x.float()).type_as(x)
            return output * self.weight

```

## ä½ç½®ç¼–ç 

åƒé—®7bçš„ä½ç½®ç¼–ç æ˜¯æ ‡å‡†çš„Rotary Position Embeddingã€‚æ¥è‡ªè®ºæ–‡ã€ŠRoFormer: Enhanced Transformer with Rotary Position Embeddingã€‹ã€‚

```python
class RotaryEmbedding(torch.nn.Module):
    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        if importlib.util.find_spec("einops") is None:
            raise RuntimeError("einops is required for Rotary Embedding")

        self._rotary_pos_emb_cache = None
        self._seq_len_cached = 0
        self._ntk_alpha_cached = 1.0

    def update_rotary_pos_emb_cache(self, max_seq_len, offset=0, ntk_alpha=1.0):
        seqlen = max_seq_len + offset
        if seqlen > self._seq_len_cached or ntk_alpha != self._ntk_alpha_cached:
            base = self.base * ntk_alpha ** (self.dim / (self.dim - 2))
            self.inv_freq = 1.0 / (
                base
                ** (
                    torch.arange(0, self.dim, 2, device=self.inv_freq.device).float()
                    / self.dim
                )
            )
            self._seq_len_cached = max(2 * seqlen, 16)
            self._ntk_alpha_cached = ntk_alpha
            seq = torch.arange(self._seq_len_cached, device=self.inv_freq.device)
            freqs = torch.outer(seq.type_as(self.inv_freq), self.inv_freq)
            emb = torch.cat((freqs, freqs), dim=-1)
            from einops import rearrange

            self._rotary_pos_emb_cache = rearrange(emb, "n d -> 1 n 1 d")

    def forward(self, max_seq_len, offset=0, ntk_alpha=1.0):
        self.update_rotary_pos_emb_cache(max_seq_len, offset, ntk_alpha)
        return self._rotary_pos_emb_cache[:, offset : offset + max_seq_len]
```

åƒé—®7bçš„_rotate_halfä½¿ç”¨äº†einopsåº“æ¥åŠ é€Ÿï¼š

```python
def _rotate_half(x):
    from einops import rearrange

    x = rearrange(x, "... (j d) -> ... j d", j=2)
    x1, x2 = x.unbind(dim=-2)
    return torch.cat((-x2, x1), dim=-1)
```

æœ€åæ˜¯apply_rotary_pos_embçš„å®ç°ï¼Œä½¿ç”¨äº†apply_rotary_emb_funcæ¥è¿›è¡ŒåŠ é€Ÿã€‚

```python
def apply_rotary_pos_emb(t, freqs):
    if apply_rotary_emb_func is not None and t.is_cuda:
        t_ = t.float()
        freqs = freqs.squeeze(0).squeeze(1)
        cos = freqs[:, : freqs.shape[-1] // 2].cos()
        sin = freqs[:, : freqs.shape[-1] // 2].sin()
        output = apply_rotary_emb_func(t_, cos, sin).type_as(t)
        return output
    else:
        rot_dim = freqs.shape[-1]
        t_, t_pass_ = t[..., :rot_dim], t[..., rot_dim:]
        t_ = t_.float()
        t_pass_ = t_pass_.float()
        t_ = (t_ * freqs.cos()) + (_rotate_half(t_) * freqs.sin())
        return torch.cat((t_, t_pass_), dim=-1).type_as(t)
```

## åƒé—®7bçš„æ³¨æ„åŠ›ç»“æ„

é¦–å…ˆè¿˜æ˜¯ä¸€å †å˜é‡å®šä¹‰ï¼š

```python
class QWenAttention(nn.Module):
    def __init__(self, config, layer_number=None):
        super().__init__()

        max_positions = config.max_position_embeddings
        self.register_buffer(
            "bias",
            torch.tril(
                torch.ones((max_positions, max_positions), dtype=torch.bool)
            ).view(1, 1, max_positions, max_positions),
            persistent=False,
        )
        self.register_buffer("masked_bias", torch.tensor(-1e4), persistent=False)
        self.layer_number = max(1, layer_number)
        self.params_dtype = config.params_dtype
        self.seq_length = config.seq_length

        self.hidden_size = config.hidden_size
        self.split_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads

        self.use_flash_attn = config.use_flash_attn
        self.scale_attn_weights = True

        self.layer_idx = None

        self.projection_size = config.kv_channels * config.num_attention_heads

        assert self.projection_size % config.num_attention_heads == 0
        self.hidden_size_per_attention_head = (
            self.projection_size // config.num_attention_heads
        )

        self.c_attn = nn.Linear(config.hidden_size, 3 * self.projection_size)

        self.c_proj = nn.Linear(
            config.hidden_size, self.projection_size, bias=not config.no_bias
        )

        self.is_fp32 = not (config.bf16 or config.fp16)
        if (
            self.use_flash_attn
            and flash_attn_unpadded_func is not None
            and not self.is_fp32
        ):
            self.core_attention_flash = FlashSelfAttention(
                causal=True, attention_dropout=config.attn_pdrop
            )

        self.bf16 = config.bf16

        if config.rotary_pct == 1.0:
            self.rotary_ndims = None
        else:
            assert config.rotary_pct < 1
            self.rotary_ndims = int(
                self.hidden_size_per_attention_head * config.rotary_pct
            )
        dim = (
            self.rotary_ndims
            if self.rotary_ndims is not None
            else self.hidden_size_per_attention_head
        )
        self.rotary_emb = RotaryEmbedding(dim, base=config.rotary_emb_base)

        self.use_dynamic_ntk = config.use_dynamic_ntk
        self.use_logn_attn = config.use_logn_attn

        logn_list = [
            math.log(i, self.seq_length) if i > self.seq_length else 1
            for i in range(1, 32768)
        ]
        self.logn_tensor = torch.tensor(logn_list)[None, :, None, None]
        self._ntk_cached = 1.0

        self.attn_dropout = nn.Dropout(config.attn_pdrop)
```

å¤§è‡´ä»‹ç»ä¸€ä¸‹è¿™äº›å˜é‡ï¼Œå…·ä½“çš„å«ä¹‰æˆ‘ä»¬åœ¨åé¢ä»£ç å¯ä»¥è®²åˆ°ï¼š

- max_positions å®šä¹‰äº†æ¨¡å‹å¯ä»¥å¤„ç†çš„æœ€å¤§ä½ç½®æ•°ï¼Œå®ƒæ¥è‡ªé…ç½®å¯¹è±¡
- bias æ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼Œå¤§å°ä¸º (max_positions, max_positions)ï¼Œç”¨äºå®ç°è‡ªæ³¨æ„åŠ›çš„å±è”½ã€‚å®ƒè¢«æ³¨å†Œä¸ºä¸€ä¸ªä¸éœ€è¦æŒä¹…åŒ–çš„ç¼“å†²åŒº
- masked_bias æ˜¯ä¸€ä¸ªå…·æœ‰å¤§è´Ÿå€¼ï¼ˆ-1e4ï¼‰çš„å¼ é‡ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›å¾—åˆ†ä¸­å±è”½æŸäº›ä½ç½®
- layer_number æ˜¯å½“å‰å±‚çš„å±‚æ•°ï¼Œè‡³å°‘ä¸º1
- params_dtype æ˜¯æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹
- seq_length æ˜¯è¾“å…¥åºåˆ—çš„é•¿åº¦
- hidden_sizeã€split_sizeã€num_headsã€head_dim åˆ†åˆ«ä¸ºéšè—å±‚å¤§å°ï¼Œåˆ†å‰²å¤§å°ï¼Œæ³¨æ„åŠ›å¤´æ•°å’Œæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
- use_flash_attn æ˜¯ä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ Flash Attention
- scale_attn_weights æ˜¯ä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦å¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡Œç¼©æ”¾
- projection_size å®šä¹‰äº†æŠ•å½±çš„å¤§å°ï¼Œå®ƒç­‰äº kv_channels å’Œ num_attention_heads çš„ä¹˜ç§¯
- c_attn å’Œ c_proj æ˜¯ä¸¤ä¸ªçº¿æ€§å±‚ï¼Œç”¨äºè®¡ç®—æ³¨æ„åŠ›å¾—åˆ†
- core_attention_flash æ˜¯ä¸€ä¸ª FlashSelfAttention å¯¹è±¡ï¼Œåªæœ‰åœ¨ä½¿ç”¨ Flash Attention å¹¶ä¸”æ•°æ®ç±»å‹ä¸æ˜¯ fp32 æ—¶æ‰ä¼šåˆ›å»º
- bf16 æ˜¯ä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ bf16 æ•°æ®ç±»å‹
- rotary_emb æ˜¯ä¸€ä¸ª RotaryEmbedding å¯¹è±¡ï¼Œç”¨äºå®ç°æ—‹è½¬ä½ç½®ç¼–ç 
- use_dynamic_ntk æ˜¯ä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨åŠ¨æ€ NTK
- use_logn_attn æ˜¯ä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ logn æ³¨æ„åŠ›
- logn_tensor æ˜¯ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«äº†ä¸€äº›é¢„è®¡ç®—çš„ logn å€¼
- attn_dropout æ˜¯ä¸€ä¸ª Dropout å±‚ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­æ·»åŠ éšæœºæ€§

ä¸‹é¢æˆ‘ä»¬æ¥çœ‹æ³¨æ„åŠ›çš„è®¡ç®—ï¼š
```python
    def _attn(self, query, key, value, attention_mask=None, head_mask=None):
        attn_weights = torch.matmul(query, key.transpose(-1, -2))

        if self.scale_attn_weights:
            attn_weights = attn_weights / torch.full(
                [],
                value.size(-1) ** 0.5,
                dtype=attn_weights.dtype,
                device=attn_weights.device,
            )

        query_length, key_length = query.size(-2), key.size(-2)
        causal_mask = self.bias[
            :, :, key_length - query_length : key_length, :key_length
        ]
        mask_value = torch.finfo(attn_weights.dtype).min
        mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(
            attn_weights.device
        )
        attn_weights = torch.where(
            causal_mask, attn_weights.to(attn_weights.dtype), mask_value
        )

        attn_weights = nn.functional.softmax(attn_weights, dim=-1)

        attn_weights = attn_weights.type(value.dtype)
        attn_weights = self.attn_dropout(attn_weights)

        if head_mask is not None:
            attn_weights = attn_weights * head_mask

        attn_output = torch.matmul(attn_weights, value)
        attn_output = attn_output.transpose(1, 2)

        return attn_output, attn_weights
```

å…¶ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š
- ä½¿ç”¨ torch.matmul è®¡ç®—æŸ¥è¯¢ï¼ˆqueryï¼‰å’Œé”®ï¼ˆkeyï¼‰çš„ç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ attn_weightsã€‚
- å¦‚æœ self.scale_attn_weights ä¸º Trueï¼Œåˆ™å°†æ³¨æ„åŠ›æƒé‡é™¤ä»¥å€¼ï¼ˆvalueï¼‰çš„æœ€åä¸€ä¸ªç»´åº¦çš„å¹³æ–¹æ ¹ï¼Œè¿™æ˜¯ä¸€ç§å¸¸è§çš„ç¼©æ”¾æ“ä½œï¼Œç”¨äºæ§åˆ¶æ³¨æ„åŠ›æƒé‡çš„å¤§å°ã€‚
- åˆ›å»ºä¸€ä¸ªå› æœå±è”½ causal_maskï¼Œè¯¥å±è”½ç”¨äºç¡®ä¿åœ¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸­ï¼Œä»»ä½•ä½ç½®åªèƒ½æ³¨æ„åˆ°å…¶ä¹‹å‰çš„ä½ç½®ã€‚å…¶ä¸­ mask_value æ˜¯ä¸€ä¸ªéå¸¸å°çš„æ•°ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›å¾—åˆ†ä¸­å±è”½æŸäº›ä½ç½®ã€‚
- ä½¿ç”¨ torch.where åº”ç”¨å› æœå±è”½ã€‚å¦‚æœ causal_mask ä¸­çš„æŸä¸€ä½ç½®ä¸º Trueï¼Œé‚£ä¹ˆåœ¨å¯¹åº”çš„ attn_weights ä½ç½®ä¿æŒåŸå€¼ï¼Œå¦åˆ™ç”¨ mask_value æ›¿æ¢ã€‚
- å¯¹æ³¨æ„åŠ›æƒé‡åº”ç”¨ softmax å‡½æ•°ï¼Œä½¿å¾—æ‰€æœ‰æƒé‡ä¹‹å’Œä¸º1ï¼Œè¿™æ ·å¯ä»¥å°†å®ƒä»¬è§£é‡Šä¸ºæ¦‚ç‡ã€‚
- ä½¿ç”¨ attn_dropout å¯¹æ³¨æ„åŠ›æƒé‡åº”ç”¨ dropout æ“ä½œï¼Œä»¥å¢åŠ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
- å¦‚æœæä¾›äº† head_maskï¼Œåˆ™å°†å…¶åº”ç”¨åˆ°æ³¨æ„åŠ›æƒé‡ä¸Šï¼Œè¿™å¯ä»¥ç”¨äºå±è”½æŸäº›æ³¨æ„åŠ›å¤´ã€‚
- ä½¿ç”¨æ³¨æ„åŠ›æƒé‡å’Œå€¼ï¼ˆvalueï¼‰è®¡ç®—æ³¨æ„åŠ›è¾“å‡º attn_outputï¼Œå¹¶å°†å…¶å¼ é‡çš„ç¬¬1ç»´å’Œç¬¬2ç»´è¿›è¡Œè½¬ç½®ï¼Œä»¥æ»¡è¶³åç»­æ“ä½œçš„éœ€è¦ã€‚

ä¸ºäº†æé«˜è®¡ç®—ç²¾åº¦ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªAttentionçš„è®¡ç®—å‡½æ•°ï¼š

```python
    def _upcast_and_reordered_attn(
        self, query, key, value, attention_mask=None, head_mask=None
    ):
        bsz, num_heads, q_seq_len, dk = query.size()
        _, _, k_seq_len, _ = key.size()

        attn_weights = torch.empty(
            bsz * num_heads,
            q_seq_len,
            k_seq_len,
            dtype=torch.float32,
            device=query.device,
        )

        scale_factor = 1.0
        if self.scale_attn_weights:
            scale_factor /= float(value.size(-1)) ** 0.5

        with autocast(enabled=False):
            q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(
                -1, dk, k_seq_len
            )
            attn_weights = torch.baddbmm(
                attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor
            )
            attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)

        query_length, key_length = query.size(-2), key.size(-2)
        causal_mask = self.bias[
            :, :, key_length - query_length : key_length, :key_length
        ]
        mask_value = torch.finfo(attn_weights.dtype).min
        mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(
            attn_weights.device
        )
        attn_weights = torch.where(causal_mask, attn_weights, mask_value)

        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask

        attn_weights = nn.functional.softmax(attn_weights, dim=-1)

        if attn_weights.dtype != torch.float32:
            raise RuntimeError(
                "Error with upcasting, attn_weights does not have dtype torch.float32"
            )
        attn_weights = attn_weights.type(value.dtype)
        attn_weights = self.attn_dropout(attn_weights)

        if head_mask is not None:
            attn_weights = attn_weights * head_mask

        attn_output = torch.matmul(attn_weights, value)

        return attn_output, attn_weights
```

_upcast_and_reordered_attnæ³¨æ„åŠ›æƒé‡è®¡ç®—ä½¿ç”¨float32ç²¾åº¦ã€‚å°†queryå’Œkey reshapeæˆ2DçŸ©é˜µ,ç„¶åä½¿ç”¨torch.baddbmmè¿›è¡Œé«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•ã€‚è®¡ç®—å¾—åˆ°çš„attn_weightså†reshapeå›åŸå§‹çš„4Då½¢çŠ¶ã€‚åŒæ ·åº”ç”¨å› æœé®æ©çŸ©é˜µå’Œattention maskã€‚

åœ¨softmaxä¹‹å‰æ ¡éªŒattn_weightsæ˜¯å¦æ˜¯float32,å¦‚æœä¸æ˜¯ä¼šæŠ¥é”™ã€‚softmaxåå†å°†attn_weightsè½¬å›valueçš„dtypeã€‚

æœ€åå¾—åˆ°attentionè¾“å‡ºå’Œæƒé‡çŸ©é˜µã€‚

è¿˜æœ‰å¯¹å¤´çš„æ‹†åˆ†å’Œç»„è£…çš„ä¸¤ä¸ªè¾…åŠ©å‡½æ•°ï¼š

```python
    def _split_heads(self, tensor, num_heads, attn_head_size):
        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)
        tensor = tensor.view(new_shape)
        return tensor

    def _merge_heads(self, tensor, num_heads, attn_head_size):
        tensor = tensor.contiguous()
        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)
        return tensor.view(new_shape)
```

_split_heads å‡½æ•°çš„ä½œç”¨æ˜¯å°†è¾“å…¥å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦åˆ†å‰²æˆä¸¤ä¸ªç»´åº¦ï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼ˆnum_headsï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„å¤§å°ï¼ˆattn_head_sizeï¼‰ã€‚å‡½æ•°é¦–å…ˆåˆ›å»ºäº†æ–°çš„å½¢çŠ¶ new_shapeï¼Œç„¶åä½¿ç”¨ view å‡½æ•°å°†è¾“å…¥å¼ é‡å˜å½¢ä¸ºè¿™ä¸ªæ–°çš„å½¢çŠ¶ã€‚

_merge_heads å‡½æ•°çš„ä½œç”¨æ˜¯å°† _split_heads å‡½æ•°å¤„ç†åçš„å¼ é‡å›å½’åˆ°åŸå§‹çš„ç»´åº¦ã€‚é¦–å…ˆï¼Œå®ƒä¼šè°ƒç”¨ contiguous å‡½æ•°ç¡®ä¿å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„ï¼Œè¿™æ˜¯å› ä¸ºåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œview å‡½æ•°éœ€è¦è¾“å…¥å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„ã€‚ç„¶åï¼Œå®ƒåˆ›å»ºäº†æ–°çš„å½¢çŠ¶ new_shapeï¼Œå¹¶ä½¿ç”¨ view å‡½æ•°å°†è¾“å…¥å¼ é‡å˜å½¢ä¸ºè¿™ä¸ªæ–°çš„å½¢çŠ¶ã€‚

æœ€åæ˜¯å‰å‘è®¡ç®—ã€‚ä¸»è¦åˆ†ä¸ºåæ­¥ï¼š
- è¾“å…¥å‚æ•°ï¼šhidden_statesæ˜¯è¾“å…¥çš„éšè—çŠ¶æ€ï¼Œlayer_pastæ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œattention_maskå’Œhead_maskåˆ†åˆ«æ˜¯æ³¨æ„åŠ›æ©ç å’Œå¤´æ©ç ï¼Œencoder_hidden_stateså’Œencoder_attention_maskæ˜¯åœ¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­ä½¿ç”¨çš„ï¼Œoutput_attentionså†³å®šæ˜¯å¦è¾“å‡ºæ³¨æ„åŠ›æƒé‡ï¼Œuse_cacheå†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜ã€‚
- è®¡ç®— queryã€key å’Œ valueï¼šé€šè¿‡self.c_attn(hidden_states)è®¡ç®—æ··åˆå±‚ï¼Œç„¶åå°†å…¶æ‹†åˆ†ä¸ºæŸ¥è¯¢ã€é”®å’Œå€¼ã€‚æ‹†åˆ†åçš„å¤§å°æ˜¯self.split_sizeã€‚
- åˆ†å‰²å¤šå¤´æ³¨æ„åŠ›ï¼šä½¿ç”¨_split_heads()å‡½æ•°å¯¹ queryã€key å’Œ value è¿›è¡Œæ‹†åˆ†ï¼Œå°†æœ€åä¸€ä¸ªç»´åº¦æ‹†åˆ†ä¸ºself.num_headså’Œself.head_dimã€‚
- å¤„ç†æ—‹è½¬ä½ç½®åµŒå…¥ï¼šæ ¹æ®kv_seq_lenå’Œntk_alphaè®¡ç®—æ—‹è½¬ä½ç½®åµŒå…¥ã€‚ç„¶åï¼Œå¯¹ query å’Œ key åº”ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ã€‚
- å¤„ç† past layerï¼šå¦‚æœlayer_pastå­˜åœ¨ï¼Œå°†å…¶ä¸å½“å‰çš„ key å’Œ value è¿æ¥èµ·æ¥ã€‚
- å¤„ç†ç¼“å­˜ï¼šå¦‚æœuse_cacheä¸º Trueï¼Œåˆ™å°†å½“å‰çš„ key å’Œ value å­˜å‚¨åˆ°presentä¸­ã€‚
- åº”ç”¨å¯¹æ•°æ³¨æ„åŠ›ï¼šå¦‚æœuse_logn_attnä¸º Trueï¼Œå¹¶ä¸”å½“å‰ä¸å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œé‚£ä¹ˆå°†å¯¹ query åº”ç”¨å¯¹æ•°æ³¨æ„åŠ›ã€‚
- åº”ç”¨ Flash Attention æˆ–å¸¸è§„æ³¨æ„åŠ›ï¼šå¦‚æœuse_flash_attnä¸º Trueï¼Œå¹¶ä¸”æ»¡è¶³ä¸€äº›å…¶ä»–æ¡ä»¶ï¼Œé‚£ä¹ˆä½¿ç”¨ Flash Attention å¯¹ queryã€key å’Œ value è¿›è¡Œå¤„ç†ã€‚å¦åˆ™ï¼Œä½¿ç”¨å¸¸è§„çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ä¸”å°† queryã€key å’Œ value çš„ç»´åº¦é‡æ–°æ’åˆ—ä»¥ç¬¦åˆ_attn()å‡½æ•°çš„è¦æ±‚ã€‚
- è®¡ç®—æ³¨æ„åŠ›è¾“å‡ºå¹¶è¿›è¡ŒæŠ•å½±ï¼šä½¿ç”¨self.c_proj()å°†æ³¨æ„åŠ›è¾“å‡ºè¿›è¡ŒæŠ•å½±ã€‚
- ç”Ÿæˆè¾“å‡ºï¼šå¦‚æœoutput_attentionsä¸º Trueï¼Œé‚£ä¹ˆåœ¨è¾“å‡ºä¸­åŠ å…¥æ³¨æ„åŠ›æƒé‡ã€‚

```python
    def forward(
        self,
        hidden_states: Optional[Tuple[torch.FloatTensor]],
        layer_past: Optional[Tuple[torch.Tensor]] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        head_mask: Optional[torch.FloatTensor] = None,
        encoder_hidden_states: Optional[torch.Tensor] = None,
        encoder_attention_mask: Optional[torch.FloatTensor] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
    ):

        mixed_x_layer = self.c_attn(hidden_states)
        query, key, value = mixed_x_layer.split(self.split_size, dim=2)

        query = self._split_heads(query, self.num_heads, self.head_dim)
        key = self._split_heads(key, self.num_heads, self.head_dim)
        value = self._split_heads(value, self.num_heads, self.head_dim)

        kv_seq_len = hidden_states.size()[1]
        if layer_past:
            # layer past[0] shape: bs * seq_len * head_num * dim
            kv_seq_len += layer_past[0].shape[1]
        if (
            self.use_dynamic_ntk
            and kv_seq_len == hidden_states.size()[1]
            and not self.training
        ):
            context_value = math.log(kv_seq_len / self.seq_length, 2) + 1
            ntk_alpha = 2 ** math.ceil(context_value) - 1
            ntk_alpha = max(ntk_alpha, 1)
            self._ntk_cached = ntk_alpha
        else:
            ntk_alpha = self._ntk_cached
        rotary_pos_emb = self.rotary_emb(kv_seq_len, ntk_alpha=ntk_alpha).to(
            hidden_states.device
        )

        if rotary_pos_emb is not None:
            if isinstance(rotary_pos_emb, tuple):
                rotary_pos_emb = rotary_pos_emb
            else:
                rotary_pos_emb = (rotary_pos_emb,) * 2

        if rotary_pos_emb is not None:
            q_pos_emb, k_pos_emb = rotary_pos_emb
            # Slice the pos emb for current inference
            cur_len = query.shape[1]
            q_pos_emb = q_pos_emb[:, -cur_len:, :, :]
            k_pos_emb = k_pos_emb[:, -cur_len:, :, :]
            query = apply_rotary_pos_emb(query, q_pos_emb)
            key = apply_rotary_pos_emb(key, k_pos_emb)

        if layer_past is not None:
            past_key, past_value = layer_past[0], layer_past[1]
            key = torch.cat((past_key, key), dim=1)
            value = torch.cat((past_value, value), dim=1)

        if use_cache:
            present = (key, value)
        else:
            present = None

        if self.use_logn_attn and not self.training:
            if self.logn_tensor.device != query.device or self.logn_tensor.dtype != query.dtype:
                self.logn_tensor = self.logn_tensor.to(query.device).type_as(query)
            seq_start = key.size(1) - query.size(1)
            seq_end = key.size(1)
            logn_tensor = self.logn_tensor[:, seq_start:seq_end, :, :]
            query = query * logn_tensor.expand_as(query)

        if (
            self.use_flash_attn
            and flash_attn_unpadded_func is not None
            and not self.is_fp32
            and query.is_cuda
        ):
            q, k, v = query, key, value
            context_layer = self.core_attention_flash(q, k, v)

            context_layer = rearrange(
                context_layer, "b s h d -> b s (h d)"
            ).contiguous()
        else:
            query = query.permute(0, 2, 1, 3)
            key = key.permute(0, 2, 1, 3)
            value = value.permute(0, 2, 1, 3)
            attn_output, attn_weight = self._attn(
                query, key, value, attention_mask, head_mask
            )
            context_layer = self._merge_heads(
                attn_output, self.num_heads, self.head_dim
            )

        attn_output = self.c_proj(context_layer)
        outputs = (attn_output, present)
        if output_attentions:
            if (
                self.use_flash_attn
                and flash_attn_unpadded_func is not None
                and not self.is_fp32
            ):
                raise ValueError("Cannot output attentions while using flash-attn")
            else:
                outputs += (attn_weight,)

        return outputs
```

## å°ç»“

åƒé—®7bçš„ä»£ç æ¯”è¾ƒé•¿ï¼Œå®ç°çš„æ¥å£ä¹Ÿè¾ƒå¤šï¼Œä¸‹ä¸€èŠ‚æˆ‘ä»¬ç»§ç»­ä»‹ç»å°†è‡ªæ³¨æ„åŠ›æ¨¡å—å’Œç»„è£…æˆæ¨¡å‹çš„ä»£ç ã€‚
